{"meta":{"title":"Just do it","subtitle":null,"description":"平时工作中所遇到的一些问题的总结","author":"zhaoxunyong","url":"http://blog.gcalls.cn"},"pages":[{"title":"关于我","date":"2016-12-25T07:58:56.000Z","updated":"2024-08-02T05:39:00.967Z","comments":false,"path":"about/index.html","permalink":"http://blog.gcalls.cn/about/index.html","excerpt":"","text":"2000年左右参加工作，做过普工、技工、网络管理员、程序员、架构师、研发总监。比较喜欢搞技术，尤其是新技术，研究起来可以到废寝忘食的地步。公司中很多新技术都是通过我大胆引入的，并且效果也很好。技术是IT行业的基础，有好的技术才能有更好的发展，只有不断学习才能跟上时代的脚步。"},{"title":"分类","date":"2017-01-01T06:52:00.000Z","updated":"2024-08-02T05:39:00.967Z","comments":false,"path":"categories/index.html","permalink":"http://blog.gcalls.cn/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-01-01T06:50:50.000Z","updated":"2024-08-02T05:39:01.139Z","comments":false,"path":"tags/index.html","permalink":"http://blog.gcalls.cn/tags/index.html","excerpt":"","text":""},{"title":"","date":"2025-01-07T06:49:46.860Z","updated":"2024-08-02T05:39:00.995Z","comments":true,"path":"files/spring-cloud-docker-prometheus-grafana监控/prometheus.json","permalink":"http://blog.gcalls.cn/files/spring-cloud-docker-prometheus-grafana监控/prometheus.json","excerpt":"","text":"{\"__inputs\":[{\"name\":\"DS_HKCASH\",\"label\":\"hkcash\",\"description\":\"\",\"type\":\"datasource\",\"pluginId\":\"prometheus\",\"pluginName\":\"Prometheus\"}],\"__requires\":[{\"type\":\"grafana\",\"id\":\"grafana\",\"name\":\"Grafana\",\"version\":\"4.6.1\"},{\"type\":\"panel\",\"id\":\"graph\",\"name\":\"Graph\",\"version\":\"\"},{\"type\":\"datasource\",\"id\":\"prometheus\",\"name\":\"Prometheus\",\"version\":\"1.0.0\"}],\"annotations\":{\"list\":[{\"builtIn\":1,\"datasource\":\"-- Grafana --\",\"enable\":true,\"hide\":true,\"iconColor\":\"rgba(0, 211, 255, 1)\",\"name\":\"Annotations & Alerts\",\"type\":\"dashboard\"}]},\"editable\":true,\"gnetId\":null,\"graphTooltip\":0,\"hideControls\":false,\"id\":null,\"links\":[],\"refresh\":\"30s\",\"rows\":[{\"collapse\":false,\"height\":322,\"panels\":[{\"aliasColors\":{},\"bars\":false,\"dashLength\":10,\"dashes\":false,\"datasource\":\"${DS_HKCASH}\",\"fill\":1,\"id\":1,\"legend\":{\"alignAsTable\":true,\"avg\":true,\"current\":true,\"max\":true,\"min\":true,\"rightSide\":false,\"show\":true,\"total\":false,\"values\":true},\"lines\":true,\"linewidth\":1,\"links\":[],\"nullPointMode\":\"null\",\"percentage\":false,\"pointradius\":5,\"points\":false,\"renderer\":\"flot\",\"seriesOverrides\":[],\"spaceLength\":10,\"span\":6,\"stack\":false,\"steppedLine\":false,\"targets\":[{\"expr\":\"heap{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"A\"},{\"expr\":\"heap_committed{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"B\"},{\"expr\":\"heap_used{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"C\"},{\"expr\":\"nonheap{job=\\\"~[[job]]\\\",instance=\\\"~[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"D\"},{\"expr\":\"nonheap_committed{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"E\"},{\"expr\":\"nonheap_used{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"F\"}],\"thresholds\":[],\"timeFrom\":null,\"timeShift\":null,\"title\":\"Heap\",\"tooltip\":{\"shared\":true,\"sort\":2,\"value_type\":\"individual\"},\"type\":\"graph\",\"xaxis\":{\"buckets\":null,\"mode\":\"time\",\"name\":null,\"show\":true,\"values\":[]},\"yaxes\":[{\"format\":\"short\",\"label\":null,\"logBase\":1,\"max\":null,\"min\":null,\"show\":true},{\"format\":\"short\",\"label\":null,\"logBase\":1,\"max\":null,\"min\":null,\"show\":true}]},{\"aliasColors\":{},\"bars\":false,\"dashLength\":10,\"dashes\":false,\"datasource\":\"${DS_HKCASH}\",\"fill\":1,\"id\":4,\"legend\":{\"alignAsTable\":true,\"avg\":true,\"current\":true,\"max\":true,\"min\":true,\"show\":true,\"total\":false,\"values\":true},\"lines\":true,\"linewidth\":1,\"links\":[],\"nullPointMode\":\"null\",\"percentage\":false,\"pointradius\":5,\"points\":false,\"renderer\":\"flot\",\"seriesOverrides\":[],\"spaceLength\":10,\"span\":6,\"stack\":false,\"steppedLine\":false,\"targets\":[{\"expr\":\"mem{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"A\"},{\"expr\":\"mem_free{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"B\"}],\"thresholds\":[],\"timeFrom\":null,\"timeShift\":null,\"title\":\"Mem\",\"tooltip\":{\"shared\":true,\"sort\":2,\"value_type\":\"individual\"},\"type\":\"graph\",\"xaxis\":{\"buckets\":null,\"mode\":\"time\",\"name\":null,\"show\":true,\"values\":[]},\"yaxes\":[{\"format\":\"short\",\"label\":null,\"logBase\":1,\"max\":null,\"min\":null,\"show\":true},{\"format\":\"short\",\"label\":null,\"logBase\":1,\"max\":null,\"min\":null,\"show\":true}]}],\"repeat\":null,\"repeatIteration\":null,\"repeatRowId\":null,\"showTitle\":false,\"title\":\"Dashboard Row\",\"titleSize\":\"h6\"},{\"collapse\":false,\"height\":330,\"panels\":[{\"aliasColors\":{},\"bars\":false,\"dashLength\":10,\"dashes\":false,\"datasource\":\"${DS_HKCASH}\",\"fill\":1,\"id\":2,\"legend\":{\"alignAsTable\":true,\"avg\":true,\"current\":true,\"max\":true,\"min\":true,\"show\":true,\"total\":false,\"values\":true},\"lines\":true,\"linewidth\":1,\"links\":[],\"nullPointMode\":\"null\",\"percentage\":false,\"pointradius\":5,\"points\":false,\"renderer\":\"flot\",\"seriesOverrides\":[],\"spaceLength\":10,\"span\":6,\"stack\":false,\"steppedLine\":false,\"targets\":[{\"expr\":\"threads{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"A\"},{\"expr\":\"threads_peak{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"B\"},{\"expr\":\"threads_daemon{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"C\"}],\"thresholds\":[],\"timeFrom\":null,\"timeShift\":null,\"title\":\"Threads\",\"tooltip\":{\"shared\":true,\"sort\":2,\"value_type\":\"individual\"},\"type\":\"graph\",\"xaxis\":{\"buckets\":null,\"mode\":\"time\",\"name\":null,\"show\":true,\"values\":[]},\"yaxes\":[{\"format\":\"short\",\"label\":null,\"logBase\":1,\"max\":null,\"min\":null,\"show\":true},{\"format\":\"short\",\"label\":null,\"logBase\":1,\"max\":null,\"min\":null,\"show\":true}]},{\"aliasColors\":{},\"bars\":false,\"dashLength\":10,\"dashes\":false,\"datasource\":\"${DS_HKCASH}\",\"fill\":1,\"id\":3,\"legend\":{\"alignAsTable\":true,\"avg\":true,\"current\":true,\"max\":true,\"min\":true,\"show\":true,\"total\":false,\"values\":true},\"lines\":true,\"linewidth\":1,\"links\":[],\"nullPointMode\":\"null\",\"percentage\":false,\"pointradius\":5,\"points\":false,\"renderer\":\"flot\",\"seriesOverrides\":[],\"spaceLength\":10,\"span\":6,\"stack\":false,\"steppedLine\":false,\"targets\":[{\"expr\":\"systemload_average{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\"}\",\"format\":\"time_series\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"A\"}],\"thresholds\":[],\"timeFrom\":null,\"timeShift\":null,\"title\":\"SystemloadAverage\",\"tooltip\":{\"shared\":true,\"sort\":0,\"value_type\":\"individual\"},\"type\":\"graph\",\"xaxis\":{\"buckets\":null,\"mode\":\"time\",\"name\":null,\"show\":true,\"values\":[]},\"yaxes\":[{\"format\":\"short\",\"label\":null,\"logBase\":1,\"max\":null,\"min\":null,\"show\":true},{\"format\":\"short\",\"label\":null,\"logBase\":1,\"max\":null,\"min\":null,\"show\":true}]}],\"repeat\":null,\"repeatIteration\":null,\"repeatRowId\":null,\"showTitle\":false,\"title\":\"Dashboard Row\",\"titleSize\":\"h6\"},{\"collapse\":false,\"height\":383,\"panels\":[{\"aliasColors\":{},\"bars\":false,\"dashLength\":10,\"dashes\":false,\"datasource\":\"${DS_HKCASH}\",\"description\":\"gauge_servo_response_api\",\"fill\":1,\"height\":\"\",\"id\":5,\"legend\":{\"alignAsTable\":true,\"avg\":true,\"current\":true,\"max\":true,\"min\":true,\"rightSide\":false,\"show\":true,\"sideWidth\":null,\"sort\":\"current\",\"sortDesc\":true,\"total\":false,\"values\":true},\"lines\":true,\"linewidth\":1,\"links\":[],\"nullPointMode\":\"null\",\"percentage\":false,\"pointradius\":5,\"points\":false,\"renderer\":\"flot\",\"seriesOverrides\":[],\"spaceLength\":10,\"span\":12,\"stack\":false,\"steppedLine\":false,\"targets\":[{\"expr\":\"{job=~\\\"[[job]]\\\",instance=~\\\"[[instance]]\\\",__name__=~\\\"gauge_servo_response_api_.*\\\"}\",\"format\":\"time_series\",\"instant\":false,\"interval\":\"\",\"intervalFactor\":2,\"legendFormat\":\"-\",\"refId\":\"A\"}],\"thresholds\":[],\"timeFrom\":null,\"timeShift\":null,\"title\":\"Gauge_servo_response_api\",\"tooltip\":{\"shared\":true,\"sort\":2,\"value_type\":\"individual\"},\"type\":\"graph\",\"xaxis\":{\"buckets\":null,\"mode\":\"time\",\"name\":null,\"show\":true,\"values\":[]},\"yaxes\":[{\"format\":\"ms\",\"label\":null,\"logBase\":1,\"max\":null,\"min\":null,\"show\":true},{\"format\":\"short\",\"label\":null,\"logBase\":1,\"max\":null,\"min\":null,\"show\":true}]}],\"repeat\":null,\"repeatIteration\":null,\"repeatRowId\":null,\"showTitle\":false,\"title\":\"Dashboard Row\",\"titleSize\":\"h6\"}],\"schemaVersion\":14,\"style\":\"dark\",\"tags\":[],\"templating\":{\"list\":[{\"allValue\":null,\"current\":{},\"datasource\":\"${DS_HKCASH}\",\"hide\":0,\"includeAll\":false,\"label\":null,\"multi\":false,\"name\":\"job\",\"options\":[],\"query\":\"{job=~\\\".+\\\"}\",\"refresh\":2,\"regex\":\"/.*job=\\\\\\\"(.+?)\\\\\\\".*/\",\"sort\":0,\"tagValuesQuery\":\"\",\"tags\":[],\"tagsQuery\":\"\",\"type\":\"query\",\"useTags\":false},{\"allValue\":null,\"current\":{},\"datasource\":\"${DS_HKCASH}\",\"hide\":0,\"includeAll\":false,\"label\":null,\"multi\":true,\"name\":\"instance\",\"options\":[],\"query\":\"{job=~\\\"$job\\\",instance=~\\\".+\\\"}\",\"refresh\":2,\"regex\":\"/.*instance=\\\\\\\"(.+?)\\\\\\\".*/\",\"sort\":0,\"tagValuesQuery\":\"/.*instance=\\\\\\\"(.+?)\\\\\\\".*/\",\"tags\":[],\"tagsQuery\":\"{job=~\\\"$job\\\",instance=~\\\".+\\\"}\",\"type\":\"query\",\"useTags\":false}]},\"time\":{\"from\":\"now-1h\",\"to\":\"now\"},\"timepicker\":{\"refresh_intervals\":[\"5s\",\"10s\",\"30s\",\"1m\",\"5m\",\"15m\",\"30m\",\"1h\",\"2h\",\"1d\"],\"time_options\":[\"5m\",\"15m\",\"1h\",\"6h\",\"12h\",\"24h\",\"2d\",\"7d\",\"30d\"]},\"timezone\":\"\",\"title\":\"hkcash-dashboard\",\"version\":53}"}],"posts":[{"title":"Prometheus","slug":"Prometheus","date":"2023-10-13T03:38:15.000Z","updated":"2024-08-02T05:39:00.967Z","comments":true,"path":"/2023/10/Prometheus.html","link":"","permalink":"http://blog.gcalls.cn/2023/10/Prometheus.html","excerpt":"","text":"Installation123456789mkdir -p /works/config/prometheuschown -R 10001:10001 /data/prometheus/prometheus-data/cp -a prometheus.yml prometheus.rules.yml /works/config/prometheus/docker run -d --name prometheus \\ --user 10001 -p 9090:9090 \\ -v /works/config/prometheus:/etc/prometheus \\ -v /data/prometheus/prometheus-data:/prometheus \\ prom/prometheus:v2.47.1","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"},{"name":"Prometheus","slug":"Linux/Prometheus","permalink":"http://blog.gcalls.cn/categories/Linux/Prometheus/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"},{"name":"Prometheus","slug":"Prometheus","permalink":"http://blog.gcalls.cn/tags/Prometheus/"}]},{"title":"CD","slug":"CD","date":"2023-09-26T07:25:04.000Z","updated":"2024-08-02T05:39:00.967Z","comments":true,"path":"/2023/09/CD.html","link":"","permalink":"http://blog.gcalls.cn/2023/09/CD.html","excerpt":"This is a guide to CD and CI.","text":"This is a guide to CD and CI. Introduction123456789101112131. ArgoCD &lt;--- ok2. FluxCD &lt;--- Not Recommend3. JenkinsX &lt;--- Not Recommend4. aliyun5. GitlabCI &lt;--- ok6. tekton &lt;--- based on k8s, more complicated than others.Prometheuskustomize &lt;--- okrancher &lt;--- okk3s/AutoK3s &lt;--- ok K3sA lightweight kubernetes, like minikube. K3s Install1234567891011121314151617181920212223242526272829303132333435363738394041424344#https://docs.k3s.io/quick-start#https://github.com/k3s-io/k3s/issues/1160#Must have hostname can resolved by dns, or add them in /etc/hosts file of all machines:cat /etc/hosts:192.168.101.82 dave-PC192.168.109.50 dave-analysis-server192.168.109.52 peter-analysis-server192.168.109.53 eino-analysis-server#Install Master:curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --disable traefik\" sh#In China:curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_EXEC=\"server --disable traefik\" INSTALL_K3S_MIRROR=cn sh -cp -a /etc/rancher/k3s/k3s.yaml ~/.kube/configsed -i 's;127.0.0.1;192.168.101.82;g' ~/.kube/configkubectl get all -A -o wide#Install Node:#K3S_TOKEN is located in Master Server: /var/lib/rancher/k3s/server/node-tokencurl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \\INSTALL_K3S_EXEC=\"--node-ip=192.168.109.50 --flannel-iface=eth1\" \\INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.101.82:6443 K3S_NODE_NAME=dave-analysis-server \\K3S_TOKEN=K10944a3ff29886d4bc05ba79fa6f8a8504bca00e35231e397f9242d7af6724d16c::server:32d4961ba4b4ea871855e856d0ccef06 sh -curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \\INSTALL_K3S_EXEC=\"--node-ip=192.168.109.52 --flannel-iface=eth1\" \\INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.101.82:6443 K3S_NODE_NAME=peter-analysis-server \\K3S_TOKEN=K10944a3ff29886d4bc05ba79fa6f8a8504bca00e35231e397f9242d7af6724d16c::server:32d4961ba4b4ea871855e856d0ccef06 sh -curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \\INSTALL_K3S_EXEC=\"--node-ip=192.168.109.53 --flannel-iface=eth1\" \\INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.101.82:6443 K3S_NODE_NAME=eino-analysis-server \\K3S_TOKEN=K10944a3ff29886d4bc05ba79fa6f8a8504bca00e35231e397f9242d7af6724d16c::server:32d4961ba4b4ea871855e856d0ccef06 sh -#cloudeon(Just a memo, ignore this)#https://cloudeon.top/docker run -d -p 7700:7700 --name cloudeon \\ -v /data/cloudeon/application.properties:/usr/local/cloudeon/cloudeon-assembly/conf/application.properties \\ registry.cn-hangzhou.aliyuncs.com/udh/cloudeon:v1.3.0#https://blog.thenets.org/how-to-create-a-k3s-cluster-with-nginx-ingress-controller/#https://blog.csdn.net/weixin_45444133/article/details/116952250kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.5/deploy/static/provider/cloud/deploy.yaml Uninstall1234#Master/usr/local/bin/k3s-uninstall.sh#Node/usr/local/bin/k3s-agent-uninstall.sh Demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# Create a test Namespace, if not existkubectl create namespace test# Apply the example file#https://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/kubectl -n test apply -f my-example.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: test-nginx-app namespace: testspec: selector: matchLabels: name: test-nginx-backend template: metadata: labels: name: test-nginx-backend spec: containers: - name: backend image: docker.io/nginx:alpine imagePullPolicy: Always ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: test-nginx-service namespace: testspec: ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: name: test-nginx-backend---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: test-nginx-ingress namespace: testspec: rules: - host: test.w1.thenets.org http: paths: - path: / pathType: Prefix backend: service: name: test-nginx-service port: number: 80 ingressClassName: nginx KustomizeDeclarative Management of Kubernetes Objects Using Kustomize | Kubernetes 12345678910111213141516Kustomize/├── base│ ├── deployment.yaml│ ├── kustomization.yaml│ └── service.yaml└── overlays ├── dev │ ├── deployment.yaml │ ├── kustomization.yaml │ ├── password.txt │ └── service.yaml └── prod ├── deployment.yaml ├── kustomization.yaml ├── password.txt └── service.yaml base/kustomization.yaml 1234567commonLabels: type: democommonAnnotations: version: 1.1.0resources:- deployment.yaml- service.yaml base/deployment.yaml 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: labels: app: apache name: portfoliospec: replicas: 1 selector: matchLabels: app: apache strategy: &#123;&#125; template: metadata: labels: app: apache spec: containers: - image: singharunk/webserver:v3 name: portfolio resources: &#123;&#125; base/service.yaml 123456789101112apiVersion: v1kind: Servicemetadata: name: portfolio-servicespec: selector: app: apache ports: - protocol: TCP port: 80 targetPort: 80 name: porty Dev environment: overlays/dev/kustomization.yaml 123456789101112131415namePrefix: dev-commonLabels: env: devcommonAnnotations: typeofApp: htmlAppbases:- ../../basenamespace: kustomize-namespacepatchesStrategicMerge:- deployment.yaml- service.yamlsecretGenerator:- name: dummy files: - password.txt overlays/dev/deployment.yaml 123456789101112131415161718apiVersion: apps/v1kind: Deploymentmetadata: name: portfoliospec: replicas: 1 template: spec: containers: - image: singharunk/webserver:v3 name: portfolio volumeMounts: - name: dummy mountPath: /opt/password.txt volumes: - name: dummy secret: secretName: dummy overlays/dev/password.txt 1this is dummy secret but now I am changing it overlays/dev/service.yaml 123456789101112apiVersion: v1kind: Servicemetadata: name: portfolio-servicespec: selector: app: apache ports: - protocol: TCP port: 82 targetPort: 80 name: httpx Prod environment: overlays/prod/kustomization.yaml 1234567891011121314151617181920212223242526namePrefix: prd-commonLabels: env: prdcommonAnnotations: typeofApp: htmlApp rollout: value2bases:- ../../basepatchesStrategicMerge:- deployment.yaml- service.yamlsecretGenerator:- name: dummy files: - password.txtnamespace: kustomize-namespacegeneratorOptions: disableNameSuffixHash: true overlays/prod/deployment.yaml 123456789101112131415161718apiVersion: apps/v1kind: Deploymentmetadata: name: portfoliospec: replicas: 2 template: spec: containers: - image: singharunk/webserver:v3 name: portfolio volumeMounts: - name: dummy mountPath: /opt/password.txt volumes: - name: dummy secret: secretName: dummy overlays/prod/password.txt 1this is dummy secret overlays/prod/service.yaml 123456789101112apiVersion: v1kind: Servicemetadata: name: portfolio-servicespec: selector: app: apache ports: - protocol: TCP port: 81 targetPort: 80 name: portx Create 123456cd overlays/dev#Run kubectl kustomize ./ to check if any error occurred#kubectl kustomize &lt;kustomization_directory&gt;kubectl kustomize ./#To apply those Resources, run kubectl apply with --kustomize or -k flag:kubectl apply -k &lt;kustomization_directory&gt; ArgocdGetting Started - Argo CD - Declarative GitOps CD for Kubernetes (argo-cd.readthedocs.io) Argo CD 保姆级入门教程 (qq.com) Install123456789101112#argocd with kubernetes 1.18#https://argo-cd.readthedocs.io/en/stable/getting_started/#https://mp.weixin.qq.com/s?__biz=MzU1MzY4NzQ1OA==&amp;mid=2247512193&amp;idx=1&amp;sn=da41bb4072870e34bdf338c22bcbc8cc&amp;chksm=fbedf04ccc9a795a08f4b0deb5a8518aa901dc1e8678277d232fff0d05ba1613a3f8d8636ab9&amp;scene=178&amp;cur_album_id=2470838961377427457#rdkubectl create namespace argocd#kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yamlkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.1.2/manifests/install.yaml#Service Type Load Balancer#Change the argocd-server service type to LoadBalancer:kubectl patch svc argocd-server -n argocd -p '&#123;\"spec\": &#123;\"type\": \"LoadBalancer\"&#125;&#125;'kubectl -n argocd get svc Enable external web ui 12345678910111213#Change port to 8443 and 8080kubectl -n argocd edit svc argocd-serve ports: - name: http nodePort: 31291 port: 8080 protocol: TCP targetPort: 8080 - name: https nodePort: 31592 port: 8443 protocol: TCP targetPort: 8080 The URL is: https://192.168.64.6:8443/, login name is admin. Getting password from: 1kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;&#123;.data.password&#125;&quot; | base64 -d; echo DemoDemo112345fleet/├── application.yaml└── dev ├── deployment.yaml └── service.yaml application.yaml 12345678910111213141516171819202122232425# application.yamlapiVersion: argoproj.io/v1alpha1kind: Applicationmetadata: name: argo-demo namespace: argocdspec: project: default source: #repoURL: https://github.com/yangchuansheng/argocd-lab.git repoURL: http://gitlab.zerofinance.net/dave.zhao/fleet_demo.git targetRevision: master path: dev destination: server: https://kubernetes.default.svc namespace: myapp # syncPolicy: # syncOptions: # - CreateNamespace=true # automated: # selfHeal: true # prune: true dev/deployment.yaml 1234567891011121314151617181920# deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: myappspec: selector: matchLabels: app: myapp replicas: 2 template: metadata: labels: app: myapp spec: containers: - name: myapp image: registry.zerofinance.net/xpayappimage/am-webhook:1.0.x ports: - containerPort: 8088 dev/service.yaml 123456789101112# service.yamlapiVersion: v1kind: Servicemetadata: name: myapp-servicespec: selector: app: myapp ports: - port: 8088 protocol: TCP targetPort: 8088 Create Apps Via Command 1kubectl apply -f application.yaml Creating Apps Via UI Open a browser to the Argo CD external UI, and login by visiting the IP/hostname in a browser and use the credentials set in step 4. After logging in, click the + New App button: Notice: If you create apps via UI, you don’t need application.yaml located in root folder. More usage please visit: Getting Started - Argo CD - Declarative GitOps CD for Kubernetes (argo-cd.readthedocs.io) Demo2guestbook-ui-deployment.yaml 1234567891011121314151617181920apiVersion: apps/v1kind: Deploymentmetadata: name: guestbook-uispec: replicas: 1 revisionHistoryLimit: 3 selector: matchLabels: app: guestbook-ui template: metadata: labels: app: guestbook-ui spec: containers: - image: gcr.io/heptio-images/ks-guestbook-demo:0.2 name: guestbook-ui ports: - containerPort: 80 guestbook-ui-svc.yaml 123guestbook/├── guestbook-ui-deployment.yaml└── guestbook-ui-svc.yaml guestbook-ui-deployment.yaml 1234567891011121314151617181920apiVersion: apps/v1kind: Deploymentmetadata: name: guestbook-uispec: replicas: 1 revisionHistoryLimit: 3 selector: matchLabels: app: guestbook-ui template: metadata: labels: app: guestbook-ui spec: containers: - image: gcr.io/heptio-images/ks-guestbook-demo:0.2 name: guestbook-ui ports: - containerPort: 80 guestbook-ui-svc.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: v1kind: Servicemetadata: name: guestbook-uispec: ports: - port: 80 targetPort: 80 selector: app: guestbook-ui# ---# apiVersion: networking.k8s.io/v1# kind: Ingress# metadata:# name: rollouts-bluegreen-ingress# annotations:# kubernetes.io/ingress.class: nginx# spec:# rules:# - host: guestbook-ui.local# http:# paths:# - path: /# pathType: Prefix# backend:# # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field# service:# name: guestbook-ui# port:# number: 80---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: guestbook-ui-ingress annotations: kubernetes.io/ingress.class: nginxspec: rules: - host: guestbook-ui.local http: paths: - backend: serviceName: guestbook-ui servicePort: 80 argo-rolloutsInstall123456kubectl create namespace argo-rollouts#kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yamlkubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/download/v1.2.2/install.yamlwget https://github.com/argoproj/argo-rollouts/releases/download/v1.2.2/kubectl-argo-rollouts-linux-amd64wget https://github.com/argoproj/argo-rollouts/releases/download/v1.2.2/kubectl-argo-rollouts-windows-amd64 Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#demo#kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/rollout.yaml#kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/service.yamlkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/v1.2.2/docs/getting-started/basic/rollout.yamlkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/v1.2.2/docs/getting-started/basic/service.yaml#Watch#kubectl argo rollouts get rollout rollouts-demo -wkubectl argo rollouts -n testing get rollout rollouts-demo --watch#Updating a Rolloutkubectl argo rollouts -n testing set image rollouts-demo \\ rollouts-demo=argoproj/rollouts-demo:yellow#Promoting a Rolloutkubectl argo rollouts -n testing promote rollouts-demo#Updating a red Rolloutkubectl argo rollouts -n testing set image rollouts-demo \\ rollouts-demo=argoproj/rollouts-demo:red#Aborting a Rolloutkubectl argo rollouts -n testing abort rollouts-demo#In order to make Rollout considered Healthy again and not Degraded, it is necessary to change the desired state back to the previous, stable versiokubectl argo rollouts -n testing set image rollouts-demo \\ rollouts-demo=argoproj/rollouts-demo:yellow #ingress:#https://argoproj.github.io/argo-rollouts/getting-started/nginx/kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/rollout.yamlkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/services.yamlkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/ingress.yaml#cat ingress.yaml has to be changed from &gt;=1.19 clusterapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: rollouts-demo-stable annotations: kubernetes.io/ingress.class: nginxspec: rules: - host: rollouts-demo.local http: paths: - path: / pathType: Prefix backend: # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field service: name: rollouts-demo-stable port:#Perform an updatekubectl argo rollouts set image rollouts-demo rollouts-demo=argoproj/rollouts-demo:yellow Dashboard123#dashboardkubectl argo rollouts dashboard(url为启动这个命令的那台机器)http://192.168.102.82:3100/rollouts bluegreen12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#bluegreen#https://github.com/argoproj/argo-rollouts/blob/master/examples/rollout-bluegreen.yaml#cat rollout-bluegreen.yaml# This example demonstrates a Rollout using the blue-green update strategy, which contains a manual# gate before promoting the new stack.apiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata: name: rollout-bluegreenspec: replicas: 2 revisionHistoryLimit: 2 selector: matchLabels: app: rollout-bluegreen template: metadata: labels: app: rollout-bluegreen spec: containers: - name: rollouts-demo image: argoproj/rollouts-demo:blue imagePullPolicy: Always ports: - containerPort: 8080 strategy: blueGreen: # activeService specifies the service to update with the new template hash at time of promotion. # This field is mandatory for the blueGreen update strategy. activeService: rollout-bluegreen-active # previewService specifies the service to update with the new template hash before promotion. # This allows the preview stack to be reachable without serving production traffic. # This field is optional. previewService: rollout-bluegreen-preview # autoPromotionEnabled disables automated promotion of the new stack by pausing the rollout # immediately before the promotion. If omitted, the default behavior is to promote the new # stack as soon as the ReplicaSet are completely ready/available. # Rollouts can be resumed using: `kubectl argo rollouts promote ROLLOUT` autoPromotionEnabled: false---kind: ServiceapiVersion: v1metadata: name: rollout-bluegreen-activespec: selector: app: rollout-bluegreen ports: - protocol: TCP port: 80 targetPort: 8080---kind: ServiceapiVersion: v1metadata: name: rollout-bluegreen-previewspec: selector: app: rollout-bluegreen ports: - protocol: TCP port: 80 targetPort: 8080---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: rollouts-bluegreen-ingress annotations: kubernetes.io/ingress.class: nginxspec: rules: - host: rollouts-bluegreen.local http: paths: - path: / pathType: Prefix backend: # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field service: name: rollout-bluegreen-active port: number: 80 Set image: 1234kubectl argo rollouts set image rollout-bluegreen \\ rollouts-demo=argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollout-bluegreen -w Demobluegreenrollout-bluegreen.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# This example demonstrates a Rollout using the blue-green update strategy, which contains a manual# gate before promoting the new stack.apiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata: name: rollout-bluegreenspec: replicas: 2 revisionHistoryLimit: 2 selector: matchLabels: app: rollout-bluegreen template: metadata: labels: app: rollout-bluegreen spec: containers: - name: rollouts-demo image: argoproj/rollouts-demo:blue imagePullPolicy: Always ports: - containerPort: 8080 strategy: blueGreen: # activeService specifies the service to update with the new template hash at time of promotion. # This field is mandatory for the blueGreen update strategy. activeService: rollout-bluegreen-active # previewService specifies the service to update with the new template hash before promotion. # This allows the preview stack to be reachable without serving production traffic. # This field is optional. previewService: rollout-bluegreen-preview # autoPromotionEnabled disables automated promotion of the new stack by pausing the rollout # immediately before the promotion. If omitted, the default behavior is to promote the new # stack as soon as the ReplicaSet are completely ready/available. # Rollouts can be resumed using: `kubectl argo rollouts promote ROLLOUT` autoPromotionEnabled: false---kind: ServiceapiVersion: v1metadata: name: rollout-bluegreen-activespec: selector: app: rollout-bluegreen ports: - protocol: TCP port: 80 targetPort: 8080---kind: ServiceapiVersion: v1metadata: name: rollout-bluegreen-previewspec: selector: app: rollout-bluegreen ports: - protocol: TCP port: 80 targetPort: 8080# ---# apiVersion: networking.k8s.io/v1# kind: Ingress# metadata:# name: rollouts-bluegreen-ingress# annotations:# kubernetes.io/ingress.class: nginx# spec:# rules:# - host: rollouts-bluegreen.local# http:# paths:# - path: /# pathType: Prefix# backend:# # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field# service:# name: rollout-bluegreen-active# port:# number: 80---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: rollouts-bluegreen-ingress annotations: kubernetes.io/ingress.class: nginxspec: rules: - host: rollouts-bluegreen.local http: paths: - backend: serviceName: rollout-bluegreen-active servicePort: 80 Canaryingress.yaml 12345678910111213141516171819202122232425262728293031323334# apiVersion: networking.k8s.io/v1# kind: Ingress# metadata:# name: rollouts-canary-stable# annotations:# kubernetes.io/ingress.class: nginx# spec:# rules:# - host: rollouts-canary.local# http:# paths:# - path: /# pathType: Prefix# backend:# # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field# service:# name: rollouts-canary-stable# port:# number: 80apiVersion: extensions/v1beta1kind: Ingressmetadata: name: rollouts-canary-stable annotations: kubernetes.io/ingress.class: nginxspec: rules: - host: rollouts-canary.local http: paths: - backend: serviceName: rollouts-canary-stable servicePort: 80 rollout.yaml 123456789101112131415161718192021222324252627282930313233343536apiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata: name: rollouts-canaryspec: replicas: 2 strategy: canary: canaryService: rollouts-canary-canary stableService: rollouts-canary-stable trafficRouting: nginx: stableIngress: rollouts-canary-stable steps: - setWeight: 5 - pause: &#123;&#125; revisionHistoryLimit: 2 selector: matchLabels: app: rollouts-canary template: metadata: labels: app: rollouts-canary spec: containers: - name: rollouts-canary image: argoproj/rollouts-demo:blue ports: - name: http containerPort: 8080 protocol: TCP resources: requests: memory: 32Mi cpu: 5m services.yaml 123456789101112131415161718192021222324252627282930apiVersion: v1kind: Servicemetadata: name: rollouts-canary-canaryspec: ports: - port: 80 targetPort: http protocol: TCP name: http selector: app: rollouts-canary # This selector will be updated with the pod-template-hash of the canary ReplicaSet. e.g.: # rollouts-pod-template-hash: 7bf84f9696---apiVersion: v1kind: Servicemetadata: name: rollouts-canary-stablespec: ports: - port: 80 targetPort: http protocol: TCP name: http selector: app: rollouts-canary # This selector will be updated with the pod-template-hash of the stable ReplicaSet. e.g.: # rollouts-pod-template-hash: 789746c88d Arago-workflow1234567891011121314151617181920212223242526#arago-workflow:Controller and Serverkubectl create namespace argokubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/v3.3.9/install.yamlkubectl -n argo edit svc argo-serverchange argo-server to LoadBalancerhttps://192.168.64.6:2746/#https://argoproj.github.io/argo-workflows/quick-start/kubectl patch deployment \\ argo-server \\ --namespace argo \\ --type='json' \\ -p='[&#123;\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/args\", \"value\": [ \"server\", \"--auth-mode=server\"]&#125;]'Get login password:k get secret -n argoARGO_TOKEN=\"Bearer $(kubectl get -n argo secret argo-server-token-vw8xf -o=jsonpath='&#123;.data.token&#125;' | base64 --decode)\"echo $ARGO_TOKENargo-windows-amd64.exe submit -n argo --watch https://raw.githubusercontent.com/argoproj/argo-workflows/v3.3.9/examples/hello-world.yaml Gitlab-RunnerDocker12345678910111213141516171819202122232425262728293031323334353637#https://docs.gitlab.com/runner/install/docker.html#https://docs.gitlab.com/runner/register/index.html#docker#cat /etc/gitlab-runner/config.tomlconcurrent = 1check_interval = 0[session_server] session_timeout = 1800[[runners]] name = \"my-runner\" url = \"http://gitlab.zerofinance.net/\" token = \"111111\" executor = \"docker\" [runners.custom_build_dir] [runners.cache] [runners.cache.s3] [runners.cache.gcs] [runners.cache.azure] [runners.docker] tls_verify = false image = \"docker:latest\" privileged = true disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [\"/var/run/docker.sock:/var/run/docker.sock\",\"/cache\",\"/works/config/runner:/runner\"] shm_size = 0docker run -d --name gitlab-runner --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /works/config/runner:/etc/gitlab-runner \\ gitlab/gitlab-runner:latest#Register runnerdocker run --rm -it -v /works/config/runner:/etc/gitlab-runner gitlab/gitlab-runner register Hosted-MachineRecommend Notice: Running sudo as gitlab-runner to register 123456789101112131415161718192021222324Installing on Linux:#https://docs.gitlab.com/runner/install/linux-manually.htmlsudo curl -L --output /usr/local/bin/gitlab-runner \"https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64\"sudo chmod +x /usr/local/bin/gitlab-runnersudo useradd --comment 'GitLab Runner' --create-home gitlab-runner --shell /bin/bashsudo gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runnersudo gitlab-runner start#Remember git password,login as gitlab-runner:git config --global credential.helper store#Mocking cloning a certain repo, input username and password to store credential.#https://docs.gitlab.com/runner/shells/index.html#shell-profile-loadingTo troubleshoot this error, check /home/gitlab-runner/.bash_logout. For example, if the .bash_logout file has a script section like the following, comment it out and restart the pipeline:if [ \"$SHLVL\" = 1 ]; then [ -x /usr/bin/clear_console ] &amp;&amp; /usr/bin/clear_console -qfi#Running sudo as gitlab-runner:#sudo gitlab-runner register --name native-runner --url http://gitlab-prod.zerofinance.net/ --registration-token 1111111111choice shell as a executor.sudo gitlab-runner register .gitlab-ci.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219# default:# before_script:# - echo \"This is a gloab before_script...\"# after_script:# - echo \"This is a gloab after_script...\"variables: RUN_PIPELINE: value: \"false\" options: - \"false\" - \"true\" description: \"Runs pipeline immediately?\"stages: - stage-导出数据 - stage-分发数据 - stage-导入数据 - stage-数仓调度 - stage-数据清理# 前置检查:# #variables:# # CI_DEBUG_TRACE: \"true\"# stage: stage-前置检查# needs: []# tags:# - hkcos# - hkx8# script:# - echo \"前置检查 started\"# - sleep 5# - echo \"前置检查 done\"# rules:# - if: $RUN_PIPELINE == \"true\"HKCASH-导出数据: #variables: # CI_DEBUG_TRACE: \"true\" stage: stage-导出数据 needs: [] tags: - master-runner script: - echo \"[start] HKCASH-导出数据(65.105) -&gt; 数仓(84.101) - [$(date '+%F %T')]\" # - bash ./mysql_hkcash_dump.sh - echo \"[ end ] HKCASH-导出数据(65.105) -&gt; 数仓(84.101) - [$(date '+%F %T')]\" # Pass variables to next stage # - echo \"BUILD_VERSION1=hello1\" &gt;&gt; build.env # - echo \"BUILD_VERSION2=hello2\" &gt;&gt; build.env # artifacts: # reports: # dotenv: build.env rules: - if: $RUN_PIPELINE == \"true\"HKCASH-分发数据: stage: stage-分发数据 needs: [HKCASH-导出数据] tags: - master-runner script: - echo \"[start] HKCASH分发数据(65.105) -&gt; 数仓服务器(84.101) - [$(date '+%F %T')]\" # - bash ./mysql_hkcash_dump.sh - echo \"[ end ] HKCASH分发数据(65.105) -&gt; 数仓服务器(84.101) - [$(date '+%F %T')]\" dependencies: - HKCASH-导出数据 rules: - if: $RUN_PIPELINE == \"true\"HKCASH-导入数据: stage: stage-导入数据 needs: [HKCASH-分发数据] tags: - master-runner script: - echo \"HKCASH数据导入:数仓服务器(84.101) -&gt; ETL(84.101) - [$(date '+%F %T')]\" # - bash ./datahouse_ods_hkcash.sh - echo \"HKCASH数据导入:数仓服务器(84.101) -&gt; ETL(84.101) - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\"HKCOS-导出数据: stage: stage-导出数据 needs: [] tags: - slave-runner script: - echo \"[start] HKCOS-导出数据(65.106) -&gt; NAS - [$(date '+%F %T')]\" # - bash ./mysql_hkcos_dump.sh - echo \"[ end ] HKCOS-导出数据(65.106) -&gt; NAS - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\"HKCOS-分发数据: stage: stage-分发数据 needs: [HKCOS-导出数据] tags: - master-runner script: - echo \"[start] HKCASH分发数据(65.105) -&gt; 数仓服务器(84.101) - [$(date '+%F %T')]\" # - bash ./mysql_hkcos_dump.sh - echo \"[ end ] HKCASH分发数据(65.105) -&gt; 数仓服务器(84.101) - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\"HKCOS-导入数据: stage: stage-导入数据 needs: [HKCOS-分发数据] tags: - master-runner script: - echo \"HKCOS数据导入:数仓服务器(84.101) -&gt; ETL(84.101) - [$(date '+%F %T')]\" # - bash ./datahouse_ods_hkcos.sh - echo \"HKCOS数据导入:数仓服务器(84.101) -&gt; ETL(84.101) - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\"HKX8-导出数据: # variables: # CI_DEBUG_TRACE: \"true\" stage: stage-导出数据 needs: [] tags: - master-runner script: - echo \"[start] HKX8导出数据(65.105) -&gt; NAS - [$(date '+%F %T')]\" - sudo /bin/bash $&#123;CI_PROJECT_DIR&#125;/mysql_hkx8_dump.sh - echo \"[ end ] HKX8导出数据(65.105) -&gt; NAS - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\"HKX8-分发数据: stage: stage-分发数据 needs: [HKX8-导出数据] tags: - master-runner script: - echo \"[start] HKX8分发数据(65.105) -&gt; 数仓服务器(84.101) - [$(date '+%F %T')]\" - sudo /bin/bash $&#123;CI_PROJECT_DIR&#125;/mysql_hkx8_dump.sh rsync - echo \"[ end ] HKX8分发数据(65.105) -&gt; 数仓服务器(84.101) - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\"HKX8-导入数据: stage: stage-导入数据 needs: [HKX8-分发数据] tags: - master-runner script: - echo \"[start] HKX8数据导入:数仓服务器(84.101) -&gt; ETL(84.101) - [$(date '+%F %T')]\" # - bash ./datahouse_ods_hkx8.sh - echo \"[ end ] HKX8数据导入:数仓服务器(84.101) -&gt; ETL(84.101) - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\"XPAY-导出数据: stage: stage-导出数据 needs: [] tags: - master-runner script: - echo \"[start] HKX8-导出数据(65.105) -&gt; NAS - [$(date '+%F %T')]\" # - bash ./mysql_xpay_dump.sh - echo \"[ end ] HKX8-导出数据(65.105) -&gt; NAS - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\"XPAY-分发数据: stage: stage-分发数据 needs: [XPAY-导出数据] tags: - master-runner script: - echo \"[start] XPAY分发数据(65.105) -&gt; 数仓服务器(84.101) - [$(date '+%F %T')]\" # - bash ./mysql_hkx8_dump.sh rsync - echo \"[ end ] XPAY分发数据(65.105) -&gt; 数仓服务器(84.101) - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\"XPAY-导入数据: stage: stage-导入数据 needs: [XPAY-分发数据] tags: - master-runner script: - echo \"[start] XPAY数据导入:数仓服务器(84.101) -&gt; ETL(84.101) - [$(date '+%F %T')]\" # - bash ./datahouse_ods_hkx8.sh - echo \"[ end ] XPAY数据导入:数仓服务器(84.101) -&gt; ETL(84.101) - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\"Datahouse-ETL调度: stage: stage-数仓调度 needs: [HKCASH-导入数据, HKCOS-导入数据, XPAY-导入数据, HKX8-导入数据] tags: - datahouse-runner script: - echo \"[start] 数仓ETL调度 - 数仓服务器(84.101) -&gt; ETL(84.101) - [$(date '+%F %T')]\" # - ./datahouse_ods_etl.sh - echo \"[ end ] 数仓ETL调度 - 数仓服务器(84.101) -&gt; ETL(84.101) - [$(date '+%F %T')]\" #There are any variables can be showed: # - echo \"BUILD_VERSION1=$BUILD_VERSION1\" # - echo \"BUILD_VERSION2=$BUILD_VERSION2\" rules: - if: $RUN_PIPELINE == \"true\"Analyst-数据导入: stage: stage-数仓调度 needs: [HKCASH-导入数据, HKCOS-导入数据, XPAY-导入数据, HKX8-导入数据] tags: - master-runner script: - echo \"[start] Analyst数据导入 - 数据服务器(63.17) -&gt; 阿里云RDS(rm-3nslo2652x449k4oa) - [$(date '+%F %T')]\" - sleep 5 # - bash ./mysql_analyst_masking_report.sh - echo \"[ end ] Analyst数据导入 - 数据服务器(63.17) -&gt; 阿里云RDS(rm-3nslo2652x449k4oa) - [$(date '+%F %T')]\" rules: - if: $RUN_PIPELINE == \"true\" Pipeline UI Trigger with remote URL 123456789#https://blog.csdn.net/lenkty/article/details/124668164#https://blog.csdn.net/boling_cavalry/article/details/106991691#https://blog.csdn.net/sandaawa/article/details/112897733#https://github.com/lonly197/docs/blob/master/src/operation/GitLab%20CI%20%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90.mdcurl -X POST \\ -F token=111222 \\ -F ref=1.0.x \\ -F variables[project]=dwh-pipeline \\ http://gitlab.zerofinance.net/api/v4/projects/575/trigger/pipeline AutoK3sNot Recommend, recommend using k3s. 1234567891011121314151617181920#AutoK3s:(Not Recommend)##https://docs.rancher.cn/docs/k3s/autok3s/_index/##https://jasonkayzk.github.io/2022/10/22/%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2autok3s/#For docker:(Recommend)##docker run -itd --restart=unless-stopped --net host -v /var/run/docker.sock:/var/run/docker.sock cnrancher/autok3s:v0.6.0docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:stable##Installing on hosted machine:(optional)#curl -sS https://rancher-mirror.rancher.cn/autok3s/install.sh | INSTALL_AUTOK3S_MIRROR=cn sh##Starting#autok3s serve --bind-address 192.168.101.82 --bind-port 8080##Uninstalling:#/usr/local/bin/autok3s-uninstall.sh##Install instance:#Put &quot;--disable traefik&quot; param into &quot;Master Extra Args&quot;##execute once:#k get ns --insecure-skip-tls-verify#k get ns","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.gcalls.cn/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.gcalls.cn/tags/Kubernetes/"}]},{"title":"hadoop Ecosystem","slug":"hadoop-Ecosystem","date":"2023-08-25T00:32:11.000Z","updated":"2024-08-29T07:10:04.830Z","comments":true,"path":"/2023/08/hadoop-Ecosystem.html","link":"","permalink":"http://blog.gcalls.cn/2023/08/hadoop-Ecosystem.html","excerpt":"Manage tool: Ambari+BigtopHDFS/YARN/MapReduce2/Tez/Hive/HBase/ZooKeeper/Spark/Zeppelin/Flink Flink-cdc/datax/seatunnel/dolphinscheduler","text":"Manage tool: Ambari+BigtopHDFS/YARN/MapReduce2/Tez/Hive/HBase/ZooKeeper/Spark/Zeppelin/Flink Flink-cdc/datax/seatunnel/dolphinscheduler IntroductionRecommend: heibaiying/BigData-Notes: 大数据入门指南 :star: (github.com) BigtopBigtop is an Apache Foundation project for Infrastructure Engineers and Data Scientists looking for comprehensive packaging, testing, and configuration of the leading open source big data components.** Bigtop supports a wide range of components/projects, including, but not limited to, Hadoop, HBase and Spark. There are 2 ways to install bigtop: build package from source***Not recommend, it’s very complicate. especially in China mainland. Prerequisite: 123456789101112#Jdknvm install v12.22.1cat /etc/profile.d/java.sh #!/bin/bashexport JAVA_HOME=/Developer/jdk1.8.0_371export M2_HOME=/Developer/apache-maven-3.6.3export _JAVA_OPTIONS=\"-Xms4g -Xmx4g -Djava.awt.headless=true\"export PATH=/root/.nvm/versions/node/v12.22.1/bin:$JAVA_HOME/bin:$M2_HOME/bin:$PATH. /etc/profile Building: Notice: Need a non-root to compile. 12345678910sudo su - hadoopwget https://dlcdn.apache.org/bigtop/bigtop-3.2.0/bigtop-3.2.0-project.tar.gz (use the suggested mirror from above)tar xfvz bigtop-3.2.0-project.tar.gzcd bigtop-3.2.0#only for rpm packages./gradlew bigtop-groovy-rpm bigtop-jsvc-rpm bigtop-select-rpm bigtop-utils-rpm \\flink-rpm hadoop-rpm hbase-rpm hive-rpm kafka-rpm solr-rpm spark-rpm \\tez-rpm zeppelin-rpm zookeeper-rpm -Dbuildwithdeps=true -PparentDir=/usr/bigtop -PpkgSuffix | tee -a log.txt#it&apos;ll clean all of packages located inbuild/, be careful!#./gradlew allclean Troubleshooting: 123456789101112131415161718192021222324252627#lacking some of jarswget https://www.zhangjc.com/images/20210817/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jarmvn install:install-file -Dfile=./pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar -DgroupId=org.pentaho -DartifactId=pentaho-aggdesigner-algorithm -Dversion=5.1.5-jhyde -Dpackaging=jarwget https://packages.confluent.io/maven/io/confluent/kafka-schema-registry-client/6.2.2/kafka-schema-registry-client-6.2.2.jarmvn install:install-file -Dfile=./kafka-schema-registry-client-6.2.2.jar -DgroupId=io.confluent -DartifactId=kafka-schema-registry-client -Dversion=6.2.2 -Dpackaging=jarmvn install:install-file -Dfile=./kafka-clients-2.8.1.jar -DgroupId=org.apache.kafka -DartifactId=kafka-clients -Dversion=2.8.1 -Dpackaging=jarwget https://packages.confluent.io/maven/io/confluent/kafka-avro-serializer/6.2.2/kafka-avro-serializer-6.2.2.jarmvn install:install-file -Dfile=./kafka-avro-serializer-6.2.2.jar -DgroupId=io.confluent -DartifactId=kafka-avro-serializer -Dversion=6.2.2 -Dpackaging=jarcd dl/tar zxf flink-1.15.3.tar.gzrm -fr flink-1.15.3/flink-formats/flink-avro-confluent-registry/src/test/rm -fr flink-1.15.3/flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/src/testrm -fr flink-1.15.3.tar.gztar -zcf flink-1.15.3.tar.gz flink-1.15.3rm -fr flink-1.15.3rm -fr /Developer/bigtop-3.2.0/build/flink/tar zxf hadoop-3.3.4.tar.gzvim hadoop-3.3.4-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xm&lt;nodejs.version&gt;v14.0.0&lt;/nodejs.version&gt;rm -fr hadoop-3.3.4.tar.gz &amp;&amp; tar -zcf hadoop-3.3.4.tar.gz hadoop-3.3.4-src &amp;&amp; rm -fr hadoop-3.3.4-srcrm -fr /Developer/bigtop-3.2.0/build/hadoop/ bigtop RepositoriesIt’s a easy way to install, including ambari packages: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141#Clone to local repository:wget https://dlcdn.apache.org/bigtop/bigtop-3.2.1/repos/centos-7/bigtop.repo -O /etc/yum.repos.d/bigtop.reporeposync --gpgcheck -1 --repoid=bigtop --download_path=/data/bigtopcd /data/bigtop/bigtopyum install createrepocreaterepo .tree ./.├── bigtop│ ├── alluxio│ │ └── x86_64│ │ └── alluxio-2.8.0-2.el7.x86_64.rpm│ ├── ambari│ │ ├── noarch│ │ │ ├── ambari-agent-2.7.5.0-1.el7.noarch.rpm│ │ │ └── ambari-server-2.7.5.0-1.el7.noarch.rpm│ │ └── x86_64│ │ ├── ambari-metrics-collector-2.7.5.0-0.x86_64.rpm│ │ ├── ambari-metrics-grafana-2.7.5.0-0.x86_64.rpm│ │ ├── ambari-metrics-hadoop-sink-2.7.5.0-0.x86_64.rpm│ │ └── ambari-metrics-monitor-2.7.5.0-0.x86_64.rpm│ ├── bigtop-ambari-mpack│ │ └── noarch│ │ └── bigtop-ambari-mpack-2.7.5.0-1.el7.noarch.rpm│ ├── bigtop-groovy│ │ └── noarch│ │ └── bigtop-groovy-2.5.4-1.el7.noarch.rpm│ ├── bigtop-jsvc│ │ └── x86_64│ │ ├── bigtop-jsvc-1.2.4-1.el7.x86_64.rpm│ │ └── bigtop-jsvc-debuginfo-1.2.4-1.el7.x86_64.rpm│ ├── bigtop-utils│ │ └── noarch│ │ └── bigtop-utils-3.2.1-1.el7.noarch.rpm│ ├── flink│ │ └── noarch│ │ ├── flink-1.15.3-1.el7.noarch.rpm│ │ ├── flink-jobmanager-1.15.3-1.el7.noarch.rpm│ │ └── flink-taskmanager-1.15.3-1.el7.noarch.rpm│ ├── gpdb│ │ └── x86_64│ │ └── gpdb-5.28.5-1.el7.x86_64.rpm│ ├── hadoop│ │ └── x86_64│ │ ├── hadoop-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-client-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-conf-pseudo-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-debuginfo-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-doc-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-hdfs-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-hdfs-datanode-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-hdfs-dfsrouter-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-hdfs-fuse-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-hdfs-journalnode-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-hdfs-namenode-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-hdfs-secondarynamenode-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-hdfs-zkfc-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-httpfs-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-kms-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-libhdfs-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-libhdfs-devel-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-libhdfspp-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-libhdfspp-devel-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-mapreduce-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-mapreduce-historyserver-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-yarn-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-yarn-nodemanager-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-yarn-proxyserver-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-yarn-resourcemanager-3.3.5-1.el7.x86_64.rpm│ │ ├── hadoop-yarn-router-3.3.5-1.el7.x86_64.rpm│ │ └── hadoop-yarn-timelineserver-3.3.5-1.el7.x86_64.rpm│ ├── hbase│ │ ├── noarch│ │ │ └── hbase-doc-2.4.13-2.el7.noarch.rpm│ │ └── x86_64│ │ ├── hbase-2.4.13-2.el7.x86_64.rpm│ │ ├── hbase-master-2.4.13-2.el7.x86_64.rpm│ │ ├── hbase-regionserver-2.4.13-2.el7.x86_64.rpm│ │ ├── hbase-rest-2.4.13-2.el7.x86_64.rpm│ │ ├── hbase-thrift2-2.4.13-2.el7.x86_64.rpm│ │ └── hbase-thrift-2.4.13-2.el7.x86_64.rpm│ ├── hive│ │ └── noarch│ │ ├── hive-3.1.3-1.el7.noarch.rpm│ │ ├── hive-hbase-3.1.3-1.el7.noarch.rpm│ │ ├── hive-hcatalog-3.1.3-1.el7.noarch.rpm│ │ ├── hive-hcatalog-server-3.1.3-1.el7.noarch.rpm│ │ ├── hive-jdbc-3.1.3-1.el7.noarch.rpm│ │ ├── hive-metastore-3.1.3-1.el7.noarch.rpm│ │ ├── hive-server2-3.1.3-1.el7.noarch.rpm│ │ ├── hive-webhcat-3.1.3-1.el7.noarch.rpm│ │ └── hive-webhcat-server-3.1.3-1.el7.noarch.rpm│ ├── kafka│ │ └── noarch│ │ ├── kafka-2.8.1-2.el7.noarch.rpm│ │ └── kafka-server-2.8.1-2.el7.noarch.rpm│ ├── livy│ │ └── noarch│ │ └── livy-0.7.1-1.el7.noarch.rpm│ ├── oozie│ │ └── noarch│ │ ├── oozie-5.2.1-2.el7.noarch.rpm│ │ └── oozie-client-5.2.1-2.el7.noarch.rpm│ ├── phoenix│ │ └── noarch│ │ └── phoenix-5.1.2-1.el7.noarch.rpm│ ├── solr│ │ └── noarch│ │ ├── solr-8.11.2-1.el7.noarch.rpm│ │ ├── solr-doc-8.11.2-1.el7.noarch.rpm│ │ └── solr-server-8.11.2-1.el7.noarch.rpm│ ├── spark│ │ └── noarch│ │ ├── spark-3.2.3-1.el7.noarch.rpm│ │ ├── spark-core-3.2.3-1.el7.noarch.rpm│ │ ├── spark-datanucleus-3.2.3-1.el7.noarch.rpm│ │ ├── spark-external-3.2.3-1.el7.noarch.rpm│ │ ├── spark-history-server-3.2.3-1.el7.noarch.rpm│ │ ├── spark-master-3.2.3-1.el7.noarch.rpm│ │ ├── spark-python-3.2.3-1.el7.noarch.rpm│ │ ├── spark-sparkr-3.2.3-1.el7.noarch.rpm│ │ ├── spark-thriftserver-3.2.3-1.el7.noarch.rpm│ │ ├── spark-worker-3.2.3-1.el7.noarch.rpm│ │ └── spark-yarn-shuffle-3.2.3-1.el7.noarch.rpm│ ├── tez│ │ └── noarch│ │ └── tez-0.10.1-1.el7.noarch.rpm│ ├── ycsb│ │ └── noarch│ │ └── ycsb-0.17.0-2.el7.noarch.rpm│ ├── zeppelin│ │ └── x86_64│ │ └── zeppelin-0.10.1-1.el7.x86_64.rpm│ └── zookeeper│ └── x86_64│ ├── zookeeper-3.5.9-2.el7.x86_64.rpm│ ├── zookeeper-debuginfo-3.5.9-2.el7.x86_64.rpm│ ├── zookeeper-native-3.5.9-2.el7.x86_64.rpm│ ├── zookeeper-rest-3.5.9-2.el7.x86_64.rpm│ └── zookeeper-server-3.5.9-2.el7.x86_64.rpm AmbariThe Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs. Notice: Bigtop repository has included all of ambari packages, you don’t need to build. just need to build the latest version that bigtop not included. For installation, please follow this instructions: Installation Guide for Ambari 2.8.0 - Apache Ambari - Apache Software Foundation Build package from sourcePrerequisite: 123456789101112131415161718192021222324252627282930#Jdknvm install v12.22.1cat /etc/profile.d/java.sh #!/bin/bashexport JAVA_HOME=/Developer/jdk1.8.0_371export M2_HOME=/Developer/apache-maven-3.6.3export _JAVA_OPTIONS=\"-Xms4g -Xmx4g -Djava.awt.headless=true\"export PATH=/root/.nvm/versions/node/v12.22.1/bin:$JAVA_HOME/bin:$M2_HOME/bin:$PATH. /etc/profile#OS environment:#swap&gt;=6G:dd if=/dev/zero of=/myswap.swp bs=1k count=4194304 #The vm has been included 2g memory. mkswap /myswap.swpswapon /myswap.swpfree -mchmod +x /etc/rc.localchmod +x /etc/rc.d/rc.localecho \"swapon /myswap.swp\" &gt;&gt; /etc/rc.localgroupadd hadoopuseradd -m -g hadoop hadooppasswd hadoopchmod +w /etc/sudoersecho \"hadoop ALL=(ALL)NOPASSWD: ALL\" &gt;&gt; /etc/sudoerschmod -w /etc/sudoers Build package from source 1234567891011#https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.8.0Centos 7.9:yum install -y git python-devel rpm-build gcc-c++wget https://pypi.python.org/packages/2.7/s/setuptools/setuptools-0.6c11-py2.7.egg#md5=fe1f997bc722265116870bc7919059eash setuptools-0.6c11-py2.7.eggwget https://dlcdn.apache.org/ambari/ambari-2.8.0/apache-ambari-2.8.0-src.tar.gz (use the suggested mirror from above)tar xfvz apache-ambari-2.8.0-src.tar.gzcd apache-ambari-2.8.0-srcmvn clean install rpm:rpm -DskipTests -Drat.skip=true Build your yum repository: See: bigtop Section Installing AmbariPerformence: IP地址 Role 192.168.80.225 NameNode ResourceManager HBase Master MySQL Zeppelin Server Grafana flume ds-master ds-api ds-alert Ambari Server Ambari Agant 192.168.80.226 SNameNode HBase Master JobHistory Server Flink History Server Spark History Server Spark Thrift Server Hive Metastore HiveServer2 WebHCat Server Datax-webui flume Ambari Agant 192.168.80.227 DataNode NodeManager Zookeeper JournalNode RegionServer ds-worker Datax worknode Ambari Agant 192.168.80.228 DataNode NodeManager Zookeeper JournalNode RegionServer ds-worker Datax worknode Ambari Agant 192.168.80.229 DataNode NodeManager Zookeeper JournalNode RegionServer ds-worker Datax worknode Ambari Metrics Collectors Ambari Agant HA: IP地址 Role 192.168.80.225 NameNode ResourceManager(Single) JobHistory Server(Single) HBase Master Flink History Server Spark History Server Hive Metastore HiveServer2 WebHCat Server(Single) Zeppelin Server(Single) MySQL(Single) Grafana(Single) flume ds-master ds-api ds-alert Ambari Metrics Collectors Ambari Server Ambari Agant 192.168.80.226 SNameNode HBase Master Flink History Server Spark History Server Hive Metastore HiveServer2 ds-master Ambari Metrics Collectors flume Ambari Agant 192.168.80.227 DataNode NodeManager Zookeeper JournalNode Kafka Broker Spark Thrift Server RegionServer ds-worker Datax worknode Ambari Agant 192.168.80.228 DataNode NodeManager Zookeeper JournalNode Kafka Broker Spark Thrift Server RegionServer ds-worker Datax worknode Ambari Agant 192.168.80.229 DataNode NodeManager Zookeeper JournalNode Kafka Broker Spark Thrift Server RegionServer ds-worker Datax worknode Ambari Agant Vagrant DockerDockerfile.centos123456789101112131415161718cd /works/tools/vagrantcat Dockerfile.centos #version: 1.0.0FROM centos:7ENV WORK_SHELL /startupWORKDIR /worksADD script.sh docker-entrypoint.sh $WORK_SHELL/RUN chmod +x $WORK_SHELL/*.shRUN $WORK_SHELL/script.shENTRYPOINT [\"/startup/docker-entrypoint.sh\"]#CMD [\"bash\", \"-c\" ,\"$WORK_SHELL/init.sh\"] docker-entrypoint.sh12345cat docker-entrypoint.sh #!/bin/bash# run the command given as arguments from CMDexec \"$@\" script.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138cat script.sh #!/bin/bash -x#http://www.360doc.com/content/14/1125/19/7044580_428024359.shtml#http://blog.csdn.net/54powerman/article/details/50684844#http://c.biancheng.net/cpp/view/2739.htmlecho \"scripting......\"yum -y install net-tools iproute iproute-doc wget sudosed -i 's;SELINUX=.*;SELINUX=disabled;' /etc/selinux/configsetenforce 0getenforce#LANG=\"en_US.UTF-8\"#sed -i 's;LANG=.*;LANG=\"zh_CN.UTF-8\";' /etc/locale.confcat /etc/NetworkManager/NetworkManager.conf|grep \"dns=none\" &gt; /dev/nullif [[ $? != 0 ]]; then echo \"dns=none\" &gt;&gt; /etc/NetworkManager/NetworkManager.conf systemctl restart NetworkManager.servicefisystemctl disable iptablessystemctl stop iptablessystemctl disable firewalldsystemctl stop firewalld#ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimetimedatectl set-timezone Asia/Shanghai#logined limitcat /etc/security/limits.conf|grep \"^root\" &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOFroot - nofile 100000root - nproc 100000* - nofile 100000* - nproc 100000EOFfi#systemd service limitcat /etc/systemd/system.conf|egrep '^DefaultLimitNOFILE' &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/systemd/system.conf &lt;&lt; EOFDefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000EOFfi#user service limitcat /etc/systemd/user.conf|egrep '^DefaultLimitNOFILE' &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/systemd/system.conf &lt;&lt; EOFDefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000EOFficat /etc/sysctl.conf|grep \"net.ipv4.ip_local_port_range\" &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 300net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.ip_local_port_range = 1024 65535net.ipv4.ip_forward = 1#k8snet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl -pfisu - root -c \"ulimit -a\"#echo \"192.168.10.6 k8s-master#192.168.10.7 k8s-node1#192.168.10.8 k8s-node2\" &gt;&gt; /etc/hosts#tee /etc/resolv.conf &lt;&lt; EOF#search myk8s.com#nameserver 114.114.114.114#nameserver 8.8.8.8#EOF#yum -y install gcc kernel-develmv -f /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repowget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repowget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repoyum -y install epel-releasesudo mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backupsudo mv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backupcat &gt; /etc/yum.repos.d/epel.repo &lt;&lt; EOF[epel]name=Extra Packages for Enterprise Linux 7 - \\$basearchbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/\\$basearch#mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=\\$basearchfailovermethod=priorityenabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7[epel-debuginfo]name=Extra Packages for Enterprise Linux 7 - \\$basearch - Debugbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/\\$basearch/debug#mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-debug-7&amp;arch=\\$basearchfailovermethod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7gpgcheck=1[epel-source]name=Extra Packages for Enterprise Linux 7 - \\$basearch - Sourcebaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/SRPMS#mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-source-7&amp;arch=\\$basearchfailovermethod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7gpgcheck=1EOFyum clean allyum makecache#yum -y install createrepo rpm-sign rng-tools yum-utils yum -y install htop bind-utils bridge-utils ntpdate setuptool iptables system-config-securitylevel-tui system-config-network-tui \\ ntsysv net-tools lrzsz telnet lsof vim dos2unix unix2dos zip unzip \\ lsof openssl openssh-server openssh-clients expectsed -i 's;#PasswordAuthentication yes;PasswordAuthentication yes;g' /etc/ssh/sshd_configsed -i 's;#PermitRootLogin yes;PermitRootLogin yes;g' /etc/ssh/sshd_config#systemctl enable sshd#systemctl restart sshd buildImages.sh1234cat buildImages.sh #!/bin/bashDOCKER_BUILDKIT=0 docker build -t \"registry.zerofinance.net/library/centos:7\" . -f Dockerfile.centos Push image12345docker login registry.zerofinance.netadmin********docker push registry.zerofinance.net/library/centos:7 Vagrantfile1234567891011121314151617181920212223242526272829303132333435363738cat Vagrantfile# -*- mode: ruby -*-# vi: set ft=ruby :# All Vagrant configuration is done below. The \"2\" in Vagrant.configure# configures the configuration version (we support older styles for# backwards compatibility). Please don't change it unless you know what# you're doing.Vagrant.configure(\"2\") do |config| config.vm.hostname = \"namenode01-test.zerofinance.net\" config.vm.network \"public_network\", ip: \"192.168.80.225\", netmask: \"255.255.255.0\", gateway: \"192.168.80.254\", bridge: \"em1\" config.vm.provider \"docker\" do |d| d.image = \"registry.zerofinance.net/library/centos:7\" d.create_args = [\"--hostname=namenode01-test.zerofinance.net\", \"--cpus=12\", \"--cpu-shares=12000\", \"-m=30g\", \"--memory-reservation=1g\", \"-v\", \"/etc/hosts:/etc/hosts\", \"-v\", \"/data:/data\", \"-v\", \"/sys/fs/cgroup:/sys/fs/cgroup\"] d.privileged = true d.cmd = [\"/usr/sbin/init\"] end config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL #yum -y install net-tools &gt; /dev/null route del default gw 172.17.0.1 route add default gw 192.168.80.254 chmod +x /etc/rc.local chmod +x /etc/rc.d/rc.local echo \"route del default gw 172.17.0.1 route add default gw 192.168.80.254\" &gt;&gt; /etc/rc.local SHELL #config.vm.provision \"shell\", # run: \"always\", # inline: \"route del default gw 172.17.0.1\" #config.vm.provision \"shell\" do |s| # s.path = \"script.sh\" # #s.args = [\"--bip=10.1.10.1/24\"] #endend Vagrant start123456789101112131415161718192021222324vagrant up#When it's done, you need to change root passwd#https://developer.hashicorp.com/vagrant/docs/providers/docker/commandsvagrant docker-exec -it -- /bin/bash#Change password:passwd#If multiple nodes in Vagrantfile:#node1 can be shown with command: vagrant status#vagrant docker-exec node1 -it -- /bin/bash#Or using nature docker command docker exec -it &lt;ContainerId&gt; /bin/bash#shutdownvagrant halt#startvagrant up#restartvagrant restart#More usuage can be found: https://blog.gcalls.cn/2022/04/A-Guide-to-Vagrant.html InitiationSSH Without Password123456789101112131415161718192021222324252627#Working all machines:groupadd hadoopuseradd -m -g hadoop hadooppasswd hadoopchmod +w /etc/sudoersecho \"hadoop ALL=(ALL)NOPASSWD: ALL\" &gt;&gt; /etc/sudoerschmod -w /etc/sudoersmkdir -p ~/.ssh/#Working on 192.168.80.225sudo su - hadoopssh-keygen -t rsa#Writing to ~/.ssh/authorized_keys：ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@192.168.80.225#cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyssudo chmod 700 ~/.sshsudo chmod 600 ~/.ssh/authorized_keys#All machine can ssh without password: scp ~/.ssh/* hadoop@192.168.80.84:~/.ssh/scp ~/.ssh/* hadoop@192.168.80.85:~/.ssh/#Just need 80.225 can ssh without password:#scp ~/.ssh/authorized_keys hadoop@192.168.80.226:~/.ssh/#scp ~/.ssh/authorized_keys hadoop@192.168.80.227:~/.ssh/#scp ~/.ssh/authorized_keys hadoop@192.168.80.228:~/.ssh/#scp ~/.ssh/authorized_keys hadoop@192.168.80.229:~/.ssh/ Optional: Docker CentOS123456789101112131415161718#If your centos is installed on docker:#For example: 192.168.80.225, vice versa:route del default gw 172.17.0.1route add default gw 192.168.80.254chmod +x /etc/rc.localchmod +x /etc/rc.d/rc.localecho \"ifconfig eth0 downroute del default gw 172.17.0.1route add default gw 192.168.80.254\" &gt;&gt; /etc/rc.localecho \"192.168.80.225 localhost localhost.localdomain localhost4 localhost4.localdomain4#::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.80.225 namenode01-test.zerofinance.net namenode01-test192.168.80.226 namenode02-test.zerofinance.net namenode02-test192.168.80.227 datanode01-test.zerofinance.net datanode01-test192.168.80.228 datanode02-test.zerofinance.net datanode02-test192.168.80.229 datanode03-test.zerofinance.net datanode03-test\" &gt;&gt; /etc/hosts NTP123456789101112131415161718192021222324252627282930313233343536373839404142434445#https://www.cnblogs.com/Sungeek/p/10197345.html#Working on all:sudo yum -y install ntpsudo timedatectl set-timezone Asia/Shanghai192.168.80.225：vim /etc/ntp.confrestrict 0.0.0.0 mask 0.0.0.0 nomodify notrapserver 127.127.1.0fudge 127.127.1.0 stratum 10#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver 0.cn.pool.ntp.org iburstserver 1.cn.pool.ntp.org iburstserver 2.cn.pool.ntp.org iburstserver 3.cn.pool.ntp.org iburst#startsystemctl enable ntpdsystemctl start ntpd#checkntpq -p#NTP Client Config on：192.168.80.&#123;226,227,228,229&#125;vim /etc/ntp.confrestrict 192.168.80.225 nomodify notrap noquery#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver 192.168.80.225#startsystemctl start ntpdsystemctl enable ntpd#checkntpdate -u 192.168.80.225sudo ntpstat Environment variables12345678910cat /etc/profile.d/my_env.sh export JAVA_HOME=/works/app/jdk/jdk1.8.0_371export HADOOP_HOME=/usr/bigtop/current/hadoop-clientexport HADOOP_CONF_DIR=/usr/bigtop/current/hadoop-client/etc/hadoop/export HADOOP_CLASSPATH=`hadoop classpath`export SPARK_HOME=/usr/bigtop/current/spark-clientexport HIVE_HOME=/usr/bigtop/current/hive-clientexport FLINK_HOME=/usr/bigtop/current/flink-clientexport ZOOKEEPER_HOME=/usr/bigtop/current/zookeeper-clientexport PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$SPARK_HOME/bin:$HIVE_HOME/bin:$FLINK_HOME/bin:$ZOOKEEPER_HOME/bin:$PATH InstallationMySQL123456789101112131415161718192021222324252627282930313233#install on 80.225#https://blog.csdn.net/weixin_43967842/article/details/124515431#https://docs.cloudera.com/HDPDocuments/Ambari-latest/administering-ambari/content/amb_using_ambari_with_mysql_or_mariadb.htmlwget https://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpmyum -y install ./mysql57-community-release-el7-10.noarch.rpmvim /etc/yum.repos.d/mysql-community.repo[mysql57-community]...gpgcheck=0...yum -y install mysql-community-servervim /etc/my.cnfmax_connections=2000character-set-server=utf8collation-server=utf8_general_cilower_case_table_names=1systemctl enable mysqldsystemctl start mysqld#temporary password：grep 'temporary password' /var/log/mysqld.logmysql -uroot -pset global validate_password_policy=0;alter user 'root'@'localhost' identified by 'Aa123456';CREATE USER 'ambari'@'%' IDENTIFIED BY 'Aa123456';GRANT ALL PRIVILEGES ON ambari.* TO 'ambari'@'%';FLUSH PRIVILEGES;exit Ambari Server123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#192.168.80.225 With Root:#https://cloud.tencent.com/works/app/jdk/article/1375511cd /vagrantsudo yum install ./ambari-server-2.8.0.0-0.x86_64.rpm#Troubleshooting/usr/sbin/ambari-server: line 34: buildNumber: unbound variablesed -i 's;$&#123;buildNumber&#125;;$&#123;VERSION&#125;;g' /usr/sbin/ambari-serversed -i 's;$&#123;buildNumber&#125;;$&#123;VERSION&#125;;g' /etc/rc.d/init.d/ambari-serverambari-server setup --jdbc-db=mysql --jdbc-driver=/vagrant/mysql-connector-j-8.0.31.jar#Init MySQL&gt; mysql -u ambari -pCREATE DATABASE ambari;USE ambari;SOURCE /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql;exit&gt; mysql -uroot -pCREATE DATABASE hive;CREATE USER 'hive'@'%' IDENTIFIED BY 'Aa123456';GRANT ALL PRIVILEGES ON hive.* TO 'hive'@'%';FLUSH PRIVILEGES;exit&gt; ambari-server setupUsing python /usr/bin/pythonSetup ambari-serverChecking SELinux...WARNING: Could not run /usr/sbin/sestatus: OKCustomize user account for ambari-server daemon [y/n] (n)? yEnter user account for ambari-server daemon (root):hadoopAdjusting ambari-server permissions and ownership...Checking firewall status...Checking JDK...[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8[2] Custom JDK==============================================================================Enter choice (1): 2WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.Path to JAVA_HOME: /works/app/jdk/jdk1.8.0_371Validating JDK on Ambari Server...done.Check JDK version for Ambari Server...JDK version found: 8Minimum JDK version is 8 for Ambari. Skipping to setup different JDK for Ambari Server.Checking GPL software agreement...GPL License for LZO: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.htmlEnable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? yCompleting setup...Configuring database...Enter advanced database configuration [y/n] (n)? yConfiguring database...==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle[3] - MySQL / MariaDB[4] - PostgreSQL[5] - Microsoft SQL Server (Tech Preview)[6] - SQL Anywhere[7] - BDB==============================================================================Enter choice (1): 3Hostname (localhost): namenode01-test.zerofinance.netPort (3306): Database name (ambari): Username (ambari): Enter Database Password (bigdata): Re-enter password: Configuring ambari database...Enter full path to custom jdbc driver: /var/lib/ambari-server/resources/mysql-connector-java.jarCopying /var/lib/ambari-server/resources/mysql-connector-java.jar to /usr/share/javaConfiguring remote database connection properties...WARNING: Before starting Ambari Server, you must run the following DDL directly from the database shell to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sqlProceed with configuring remote database connection properties [y/n] (y)? yExtracting system views...ambari-admin-2.8.0.0.0.jarAmbari repo file doesn't contain latest json url, skipping repoinfos modificationAdjusting ambari-server permissions and ownership...Ambari Server 'setup' completed successfully.#Troubleshootingrm -fr /usr/share/java &amp;&amp; mkdir -p /usr/share/javacp -a /vagrant/mysql-connector-j-8.0.31.jar /usr/share/java/#Startsystemctl enable ambari-serversystemctl start ambari-server Ambari Agent1234567#Install ambari angent on all machines:cd /vagrant/yum install ./ambari-agent-2.8.0.0-0.x86_64.rpmsed -i &apos;s;$&#123;buildNumber&#125;;$&#123;VERSION&#125;;g&apos; /var/lib/ambari-agent/bin/ambari-agentsystemctl enable ambari-agent.servicesystemctl start ambari-agent.service bigtop repo123456#bigtop repo(192.168.80.225): cd /vagrant/bigdatarepoyum install createrepocreaterepo .nohup python -m SimpleHTTPServer &amp;http://192.168.80.225:8000/ Install Hadoop Ecosystem12345678910111213141516171819web portal:http://192.168.80.225:8080/admin/admin#input machine informations:namenode01-test.zerofinance.netnamenode02-test.zerofinance.netdatanode01-test.zerofinance.netdatanode02-test.zerofinance.netdatanode03-test.zerofinance.net#Using .ssh-key to setup#Notice:Cluster Name : dwh#Chose the hdfs account as \"hadoop\" not \"hdfs\"Repositories:http://192.168.80.225:8000/ Troubleshooting1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768691.hive went wrong by:Sys DB and Information Schema not created yet#Login on specific machine：cd /etc/hive/touch /etc/hive/sys.db.created#restart ambari-serversudo systemctl restart ambari-server#Add new component, an error was caucse:ambari 500 status code received on POST method for API:#https://www.jianshu.com/p/3b54ba251c9echown -R hadoop:hadoop /var/run/ambari-server#Cannot create /var/run/ambari-server/stack-recommendations:chown -R hadoop:hadoop /var/run/ambari-server#Web Portal：HDFS---&gt;CONFIGS: search for hive, changed hadoop.proxyuser.hive.hosts to *#mkdir: Permission denied: user=root, access=WRITE, inode=\"/\":hdfs:hdfs:drwxr-xr-xhttps://blog.csdn.net/gdkyxy2013/article/details/105254907#zeppelin cannot ran flink 1.15.3：cd /usr/bigtop/current/flink-client/libmv flink-dist-1.15.3.jar flink-dist_2.12-1.15.3.jar#zeppelin does not support with flink 1.15.3, see: #https://github.com/apache/zeppelin/blob/v0.10.1/flink/flink-shims/src/main/java/org/apache/zeppelin/flink/FlinkShims.java#zeppelin open job function:Ambari---&gt;Zeppelin---&gt;Custom zeppelin-site:zeppelin.jobmanager.enable: true#reboot zeppelin.#Get version error by command: flink -v#Version: &lt;unknown&gt;, Commit ID: DeadD0d0#Downloading flink-1.15.3-bin-scala_2.12.tgz from official web site, extract flink-dist-1.15.3.jar from lib, then:cp -a flink-dist-1.15.3.jar /usr/bigtop/current/flink-client/lib/flink-dist_2.12-1.15.3.jar#Troubleshooting ambari metric#https://cwiki.apache.org/confluence/display/AMBARI/Cleaning+up+Ambari+Metrics+System+Data#https://www.jianshu.com/p/3fa7a23818a1#https://xieshaohu.wordpress.com/2021/06/15/ambari-metrics%E5%90%AF%E5%8A%A8%E5%90%8E%E8%87%AA%E5%8A%A8%E5%81%9C%E6%AD%A2/CONFIG:hbase.tmp.dir---&gt;/var/lib/ambari-metrics-collector/hbase-tmpzkCli.shdeleteall /ams-hbase-unsecure /ambari-metrics-clustersudo -u hadoop hadoop fs -mv /user/ams/hbase /user/ams/hbase.baksudo -u hadoop hadoop fs -mkdir /user/ams/hbaserm -fr /var/lib/ambari-metrics-collector/*rm -fr /vagrant/var/#restart Ambari Metrics on web ui.#Ambari Metrics Grafana password creation failed. PUT request status: 401 Unauthorizedambari-metrics-monitor statusambari-metrics-collector statusmv /var/lib/ambari-metrics-grafana/grafana.db /tmp/or:#https://blog.csdn.net/qq_37865420/article/details/104040970#https://cloud.tencent.com/developer/ask/sof/114883574sqlite3 /var/lib/ambari-metrics-grafana/grafana.dbsqlite&gt; update user set password = '59acf18b94d7eb0694c61e60ce44c110c7a683ac6a8f09580d626f90f4a242000746579358d77dd9e570e83fa24faa88a8a6', salt = 'F3FAxVm33R' where login = 'admin';sqlite&gt; .exit dolphinscheduler1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#https://dolphinscheduler.apache.org/zh-cn/docs/3.1.8/guide/installation/pseudo-cluster#Must install with hadoop account:sudo su - hadoop#docker env: need to shutdown eth0 or cannot register the actual ip to zokeeper: ifconfig eth0 down#https://blog.csdn.net/Keyuchen_01/article/details/128653687mysql -uroot -pset global validate_password_policy=0;CREATE DATABASE dolphinscheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE USER 'ds'@'%' IDENTIFIED BY 'Aa123456';GRANT ALL PRIVILEGES ON dolphinscheduler.* TO 'ds'@'%';FLUSH PRIVILEGES;#mkdir lib#cp mysql-connector-j-8.0.31.jar ./apache-dolphinscheduler-3.1.2-bin/lib/cp mysql-connector-j-8.0.31.jar ./apache-dolphinscheduler-3.1.2-bin/api-server/libs/cp mysql-connector-j-8.0.31.jar ./apache-dolphinscheduler-3.1.2-bin/alert-server/libs/cp mysql-connector-j-8.0.31.jar ./apache-dolphinscheduler-3.1.2-bin/master-server/libs/cp mysql-connector-j-8.0.31.jar ./apache-dolphinscheduler-3.1.2-bin/worker-server/libs/cp mysql-connector-j-8.0.31.jar ./apache-dolphinscheduler-3.1.2-bin/tools/libs/vim bin/env/install_env.ships=$&#123;ips:-\"namenode01-test.zerofinance.net,namenode02-test.zerofinance.net,datanode01-test.zerofinance.net,datanode02-test.zerofinance.net,datanode03-test.zerofinance.net\"&#125;masters=$&#123;masters:-\"namenode01-test.zerofinance.net,namenode02-test.zerofinance.net\"&#125;workers=$&#123;workers:-\"datanode01-test.zerofinance.net:default,datanode02-test.zerofinance.net:default,datanode03-test.zerofinance.net:default\"&#125;alertServer=$&#123;alertServer:-\"namenode01-test.zerofinance.net\"&#125;apiServers=$&#123;apiServers:-\"namenode01-test.zerofinance.net\"&#125;deployUser=$&#123;deployUser:-\"hadoop\"&#125;installPath=$&#123;installPath:-\"/works/app/dolphinscheduler\"&#125;vim bin/env/dolphinscheduler_env.shexport JAVA_HOME=$&#123;JAVA_HOME:-/works/app/jdk/jdk1.8.0_371&#125;export DATABASE=$&#123;DATABASE:-mysql&#125;export SPRING_PROFILES_ACTIVE=$&#123;DATABASE&#125;export SPRING_DATASOURCE_URL=jdbc:mysql://192.168.80.225:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=falseexport SPRING_DATASOURCE_USERNAME=dsexport SPRING_DATASOURCE_PASSWORD=Aa123456export REGISTRY_ZOOKEEPER_CONNECT_STRING=$&#123;REGISTRY_ZOOKEEPER_CONNECT_STRING:-datanode01-test.zerofinance.net:2181,datanode02-test.zerofinance.net:2181,datanode03-test.zerofinance.net:2181&#125;export HADOOP_HOME=$&#123;HADOOP_HOME:-/usr/bigtop/current/hadoop-client&#125;export HADOOP_CONF_DIR=$&#123;HADOOP_CONF_DIR:-/usr/bigtop/current/hadoop-client/etc/hadoop/&#125;export SPARK_HOME1=$&#123;SPARK_HOME1:-/usr/bigtop/current/spark-client&#125;#export SPARK_HOME2=$&#123;SPARK_HOME2:-/opt/soft/spark2&#125;export PYTHON_HOME=$&#123;PYTHON_HOME:-/usr&#125;export HIVE_HOME=$&#123;HIVE_HOME:-/usr/bigtop/current/hive-client&#125;export FLINK_HOME=$&#123;FLINK_HOME:-/usr/bigtop/current/flink-client&#125;export DATAX_HOME=$&#123;DATAX_HOME:-/opt/soft/datax&#125;export SEATUNNEL_HOME=$&#123;SEATUNNEL_HOME:-/opt/soft/seatunnel&#125;export CHUNJUN_HOME=$&#123;CHUNJUN_HOME:-/opt/soft/chunjun&#125;cd /vagrant/apache-dolphinscheduler-3.1.2-binbash tools/bin/upgrade-schema.shsh bin/install.shWeb Portal:http://namenode01-test.zerofinance.net:12345/dolphinscheduler/ui Summary 1234567891011121314151617Components: HDFS/YARN/MapReduce2/Tez/Hive/Hbase/ZooKeeper/Spark/Zeppelin/Flink/Datax/Dolphinscheduler/FlumeCompoments path: /usr/bigtop/current/&#123;hadoop-client,spark-client,hive-client,flink-client&#125;Vagrant root folder: /works/tools/vagrantAmbari UI: http://namenode01-test.zerofinance.net:8080/ admin/adminDolphinescheduler UI: http://namenode01-test.zerofinance.net:12345/dolphinscheduler/ui admin/dolphinscheduler123Kafka Brokers: 192.168.65.107:9092,192.168.65.108:9092,192.168.66.110:9092Kafka UI: https://kafka-ui-test.zerofinance.net/ admin/admin Notice: dolphinscheduler 3.1.2 seems having a bug by working with Flink-Stream, the error as follows. I have no idea to resolve it: 123456789101112131415[ERROR] 2023-09-22 09:47:30.455 +0000 - Task execute failed, due to meet an exceptionjava.lang.RuntimeException: The jar for the task is required. at org.apache.dolphinscheduler.plugin.task.api.AbstractYarnTask.getResourceNameOfMainJar(AbstractYarnTask.java:133) at org.apache.dolphinscheduler.plugin.task.flink.FlinkStreamTask.setMainJarName(FlinkStreamTask.java:86) at org.apache.dolphinscheduler.plugin.task.flink.FlinkStreamTask.init(FlinkStreamTask.java:61) at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecuteRunnable.beforeExecute(WorkerTaskExecuteRunnable.java:231) at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecuteRunnable.run(WorkerTaskExecuteRunnable.java:170) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:750)[ERROR] 2023-09-22 09:47:30.456 +0000 - can not get appId, taskInstanceId:573 HadoopThe Apache™ Hadoop® project develops open-source software for reliable, scalable, distributed computing. The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures. Introduction: BigData-Notes/notes/Hadoop-HDFS.md at master · heibaiying/BigData-Notes (github.com) Shell: BigData-Notes/notes/HDFS常用Shell命令.md at master · heibaiying/BigData-Notes (github.com) ​ FileSystemShell HDFS: BigData-Notes/notes/Hadoop-HDFS.md at master · heibaiying/BigData-Notes (github.com) MapReduce2: BigData-Notes/notes/Hadoop-MapReduce.md at master · heibaiying/BigData-Notes (github.com) YARN: BigData-Notes/notes/Hadoop-YARN.md at master · heibaiying/BigData-Notes (github.com) JavaAPI: BigData-Notes/notes/HDFS-Java-API.md at master · heibaiying/BigData-Notes (github.com) Windows Client12345git clone https://gitcode.net/mirrors/cdarlint/winutilscp -a winutils/hadoop-3.3.5 /Developer/#Set variable in environment:set HADOOP_HOME=D:\\Developer\\hadoop-3.3.5Add PATH as: %HADOOP_HOME%\\bin ConfiguationPut core-site.xml and hdfs-site.xml to resources folder of your java project: core-site.xml12345678&lt;configuration xmlns:xi=\"http://www.w3.org/2001/XInclude\"&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;configuration xmlns:xi=\"http://www.w3.org/2001/XInclude\"&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.internal.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;namenode01-test.zerofinance.net:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;namenode02-test.zerofinance.net:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.https-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;namenode01-test.zerofinance.net:50470&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.https-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;namenode02-test.zerofinance.net:50470&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;namenode01-test.zerofinance.net:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;namenode02-test.zerofinance.net:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; HiveThe Apache Hive ™ is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage using SQL. GettingStarted internal tableIf table has beed deleted, all data will be delete accordingly, including meta data and file data. 123456789101112131415#https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML#LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]#filepath can be:#a relative path, such as project/data1#an absolute path, such as /user/hive/project/data1#a full URI with scheme and (optionally) an authority, such as hdfs://namenode:9000/user/hive/project/data1The keyword &apos;OVERWRITE&apos; signifies that existing data in the table is deleted. If the &apos;OVERWRITE&apos; keyword is omitted, data files are appended to existing data sets.#default as internal table: CREATE TABLE pokes (foo INT, bar STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos; &apos; STORED AS TEXTFILE;sudo -u hive hadoop fs -put -f /tmp/kv1.txt /user/hive/demo/LOAD DATA INPATH &apos;./demo/kv1.txt&apos; OVERWRITE INTO TABLE pokes;#When it&apos;s done, the file located in hdfs will be deleted.select * from pokes; external tableIf table has beed deleted, just meta data will be deleted. once you create table again, the data will be restored, no need load again. 123456#sudo -u hdfs hadoop fs -chown -R hive:hive /works/test/#sudo -u hive hadoop fs -cp /user/hive/demo/kv1.txt /works/test/sudo -u hive hadoop fs -put -f /tmp/kv1.txt /works/demo/create external table mytest ( id int, myfields string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos; &apos; STORED AS TEXTFILE location &apos;/works/test/&apos;;LOAD DATA INPATH &apos;/works/demo/kv1.txt&apos; OVERWRITE INTO TABLE mytest;describe formatted mytest; Partition12345CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos; &apos; STORED AS TEXTFILE;#sudo -u hive hadoop fs -put -f /tmp/kv1.txt /user/hive/demo/LOAD DATA INPATH &apos;./demo/kv1.txt&apos; OVERWRITE INTO TABLE invites PARTITION (ds=&apos;2008-08-15&apos;);select * from invites;SELECT a.foo FROM invites a WHERE a.ds=&apos;2008-08-15&apos;; Insert Directory123456#selects all rows from partition ds=2008-08-15 of the invites table into an HDFS directory. The result data is in files (depending on the number of mappers) in that directory.NOTE: partition columns if any are selected by the use of *. They can also be specified in the projection clauses.INSERT OVERWRITE DIRECTORY &apos;/tmp/hdfs_out&apos; SELECT a.* FROM invites a WHERE a.ds=&apos;2008-08-15&apos;;#local dirctory located on the same node of hiveserver2.INSERT OVERWRITE LOCAL DIRECTORY &apos;/tmp/local_out&apos; SELECT a.* FROM pokes a; Insert Table1234CREATE TABLE events (foo INT, bar STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos; &apos; STORED AS TEXTFILE;INSERT OVERWRITE TABLE events SELECT a.* FROM pokes a;FROM invites a INSERT OVERWRITE TABLE events SELECT a.bar, count(*) WHERE a.foo &gt; 0 GROUP BY a.bar;INSERT OVERWRITE TABLE events SELECT a.bar, count(*) FROM invites a WHERE a.foo &gt; 0 GROUP BY a.bar; Date TypeHive 数据类型 | Hive 教程 (hadoopdoc.com) A complex demo for data type. 12345678CREATE TABLE students( name STRING, -- 姓名 age INT, -- 年龄 subject ARRAY&lt;STRING&gt;, --学科 score MAP&lt;STRING,FLOAT&gt;, --各个学科考试成绩 address STRUCT&lt;houseNumber:int, street:STRING, city:STRING, province:STRING&gt; --家庭居住地址) ROW FORMAT DELIMITED FIELDS TERMINATED BY \"\\t\" STORED AS TEXTFILE; STRUCT123456789101112131415161718192021222324252627282930313233CREATE TABLE IF NOT EXISTS person_1 (id int,info struct&lt;name:string,country:string&gt;) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; COLLECTION ITEMS TERMINATED BY &apos;:&apos; STORED AS TEXTFILE;//创建一个文本文件test_struct.txt1,&apos;dd&apos;:&apos;jp&apos;2,&apos;ee&apos;:&apos;cn&apos;3,&apos;gg&apos;:&apos;jp&apos;4,&apos;ff&apos;:&apos;cn&apos;5,&apos;tt&apos;:&apos;jp&apos;sudo -u hive hadoop fs -put /works/test/test_struct.txt /user/hive/demo/LOAD DATA INPATH &apos;./demo/test_struct.txt&apos; OVERWRITE INTO TABLE person_1;select * from person_1;+--------------+-----------------------------------+| person_1.id | person_1.info |+--------------+-----------------------------------+| 1 | &#123;&quot;name&quot;:&quot;&apos;dd&apos;&quot;,&quot;country&quot;:&quot;&apos;jp&apos;&quot;&#125; || 2 | &#123;&quot;name&quot;:&quot;&apos;ee&apos;&quot;,&quot;country&quot;:&quot;&apos;cn&apos;&quot;&#125; || 3 | &#123;&quot;name&quot;:&quot;&apos;gg&apos;&quot;,&quot;country&quot;:&quot;&apos;jp&apos;&quot;&#125; || 4 | &#123;&quot;name&quot;:&quot;&apos;ff&apos;&quot;,&quot;country&quot;:&quot;&apos;cn&apos;&quot;&#125; || 5 | &#123;&quot;name&quot;:&quot;&apos;tt&apos;&quot;,&quot;country&quot;:&quot;&apos;jp&apos;&quot;&#125; |+--------------+------------------------select id,info.name,info.country from person_1 where info.name=&apos;\\&apos;dd\\&apos;&apos;;+-----+-------+----------+| id | name | country |+-----+-------+----------+| 1 | &apos;dd&apos; | &apos;jp&apos; |+-----+-------+----------+1 row selected (0.316 seconds) ARRAY123456789101112131415161718192021222324CREATE TABLE IF NOT EXISTS array_1 (id int,name array&lt;STRING&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; COLLECTION ITEMS TERMINATED BY &apos;:&apos; STORED AS TEXTFILE;//导入数据sudo -u hive hadoop fs -put /works/test/test_struct.txt /user/hive/demo/test_array.txtLOAD DATA INPATH &apos;./demo/test_array.txt&apos; OVERWRITE INTO TABLE array_1;//查询数据hive&gt; select * from array_1;OK1 [&quot;dd&quot;,&quot;jp&quot;]2 [&quot;ee&quot;,&quot;cn&quot;]3 [&quot;gg&quot;,&quot;jp&quot;]4 [&quot;ff&quot;,&quot;cn&quot;]5 [&quot;tt&quot;,&quot;jp&quot;]Time taken: 0.041 seconds, Fetched: 5 row(s)hive&gt; select id,name[0],name[1] from array_1 where name[1]=&apos;\\&apos;cn\\&apos;&apos;;+-----+-------+-------+| id | _c1 | _c2 |+-----+-------+-------+| 2 | &apos;ee&apos; | &apos;cn&apos; || 4 | &apos;ff&apos; | &apos;cn&apos; |+-----+-------+-------+2 rows selected (0.317 seconds) MAP123456789101112131415161718192021222324252627282930313233343536373839404142434445464748CREATE TABLE IF NOT EXISTS map_1 (id int,name map&lt;STRING,STRING&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;|&apos; COLLECTION ITEMS TERMINATED BY &apos;,&apos; MAP KEYS TERMINATED BY &apos;:&apos;STORED AS TEXTFILE;cat test_map.txt 1|&apos;name&apos;:&apos;jp&apos;,&apos;country&apos;:&apos;cn&apos;2|&apos;name&apos;:&apos;jp&apos;,&apos;country&apos;:&apos;cn&apos;sudo -u hive hadoop fs -put /works/test/test_map.txt /user/hive/demo/test_map.txt//加载数据LOAD DATA INPATH &apos;./demo/test_map.txt&apos; OVERWRITE INTO TABLE map_1;//查询数据hive&gt; select * from map_1;+-----------+---------------------------------------+| map_1.id | map_1.name |+-----------+---------------------------------------+| 1 | &#123;&quot;&apos;name&apos;&quot;:&quot;&apos;jp&apos;&quot;,&quot;&apos;country&apos;&quot;:&quot;&apos;cn&apos;&quot;&#125; || 2 | &#123;&quot;&apos;name&apos;&quot;:&quot;&apos;jp&apos;&quot;,&quot;&apos;country&apos;&quot;:&quot;&apos;cn&apos;&quot;&#125; |+-----------+-----------------------------------hive&gt; select id,name[&quot;&apos;name&apos;&quot;],name[&quot;&apos;country&apos;&quot;] from map_1;+-----+-------+-------+| id | _c1 | _c2 |+-----+-------+-------+| 1 | &apos;jp&apos; | &apos;cn&apos; || 2 | &apos;jp&apos; | &apos;cn&apos; |+-----+-------+-------+hive&gt; select * from map_1 where name[&quot;&apos;country&apos;&quot;]=&apos;\\&apos;cn\\&apos;&apos;;+-----------+---------------------------------------+| map_1.id | map_1.name |+-----------+---------------------------------------+| 1 | &#123;&quot;&apos;name&apos;&quot;:&quot;&apos;jp&apos;&quot;,&quot;&apos;country&apos;&quot;:&quot;&apos;cn&apos;&quot;&#125; || 2 | &#123;&quot;&apos;name&apos;&quot;:&quot;&apos;jp&apos;&quot;,&quot;&apos;country&apos;&quot;:&quot;&apos;cn&apos;&quot;&#125; |+-----------+---------------------------------------+2 rows selected (0.287 seconds)hive&gt; insert into map_1(id,name)values(1, str_to_map(&quot;name:jp1,country:cn1&quot;)),(2, str_to_map(&quot;name:jp2,country:cn2&quot;));No rows affected (11.664 seconds)hive&gt; select * from map_1;+-----------+---------------------------------------+| map_1.id | map_1.name |+-----------+---------------------------------------+| 1 | &#123;&quot;name&quot;:&quot;jp1&quot;,&quot;country&quot;:&quot;cn1&quot;&#125; || 2 | &#123;&quot;name&quot;:&quot;jp2&quot;,&quot;country&quot;:&quot;cn2&quot;&#125; |+-----------+---------------------------------------+4 rows selected (0.482 seconds) UINON 12345678910111213141516171819202122232425262728293031323334353637383940//创建DUAL表，插入一条记录，用于生成数据create table dual(d string);insert into dual values(&apos;X&apos;);//创建UNION表CREATE TABLE IF NOT EXISTS uniontype_1 (id int,info map&lt;STRING,array&lt;int&gt;&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; COLLECTION ITEMS TERMINATED BY &apos;-&apos;MAP KEYS TERMINATED BY &apos;:&apos;STORED AS TEXTFILE;//Insertinsert overwrite table uniontype_1select 1 as id,map(&apos;english&apos;,array(99,21,33)) as info from dualunion allselect 2 as id,map(&apos;english&apos;,array(44,33,76)) as info from dualunion allselect 3 as id,map(&apos;english&apos;,array(76,88,66)) as info from dual;select * from uniontype_1;+-----------------+-------------------------+| uniontype_1.id | uniontype_1.info |+-----------------+-------------------------+| 1 | &#123;&quot;english&quot;:[99,21,33]&#125; || 2 | &#123;&quot;english&quot;:[44,33,76]&#125; || 3 | &#123;&quot;english&quot;:[76,88,66]&#125; |+-----------------+-------------------------+3 rows selected (0.432 seconds)select * from uniontype_1 where info[&apos;english&apos;][2]&gt;30;+-----------------+-------------------------+| uniontype_1.id | uniontype_1.info |+-----------------+-------------------------+| 1 | &#123;&quot;english&quot;:[99,21,33]&#125; || 2 | &#123;&quot;english&quot;:[44,33,76]&#125; || 3 | &#123;&quot;english&quot;:[76,88,66]&#125; |+-----------------+-------------------------+ ES12345678910111213141516docker pull registry.zerofinance.net/library/elasticsearch:7.6.2docker pull registry.zerofinance.net/library/kibana:7.6.2mkdir -p /works/data/esdata-devchmod -R 777 /works/data/esdata-dev#https://www.cnblogs.com/baoshu/p/16128127.htmldocker network create es-networkdocker run -d --name elastic-dev --restart always --log-driver json-file --log-opt max-size=200m --log-opt max-file=3 --net es-network -p 9200:9200 -p 9300:9300 -v /works/data/esdata-dev:/usr/share/elasticsearch/data -e \"discovery.type=single-node\" --ulimit nofile=65535:65535 registry.zerofinance.net/library/elasticsearch:7.6.2curl http://192.168.63.102:9200/_catdocker run -d --name kibana-dev --net es-network -p 5601:5601 -e \"ELASTICSEARCH_HOSTS=http://192.168.63.102:9200\" registry.zerofinance.net/library/kibana:7.6.2http://192.168.63.102:5601/app/kibana#/dev_tools/console FlinkBigData-Notes/notes/Flink核心概念综述.md at master · heibaiying/BigData-Notes (github.com) Flink SQL史上最全干货！Flink SQL 成神之路（全文 18 万字、138 个案例、42 张图） | antigeneral’s blog (yangyichao-mango.github.io) Deployment ModesSee this Overview to understand: deployment-modes Standalonehttps://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/resource-providers/standalone/overview/ Session Mode123456789101112# we assume to be in the root directory of the unzipped Flink distribution# (1) Start Cluster$ ./bin/start-cluster.sh# (2) You can now access the Flink Web Interface on http://localhost:8081# (3) Submit example job$ ./bin/flink run ./examples/streaming/TopSpeedWindowing.jar# (4) Stop the cluster again$ ./bin/stop-cluster.sh In step (1), we’ve started 2 processes: A JVM for the JobManager, and a JVM for the TaskManager. The JobManager is serving the web interface accessible at localhost:8081. In step (3), we are starting a Flink Client (a short-lived JVM process) that submits an application to the JobManager. 123#Troubleshooting: 8081 can be visited only for localhostcat /etc/hosts192.168.80.225 localhost Application Mode1234567891011121314151617To start a Flink JobManager with an embedded application, we use the bin/standalone-job.sh script. We demonstrate this mode by locally starting the TopSpeedWindowing.jar example, running on a single TaskManager.The application jar file needs to be available in the classpath. The easiest approach to achieve that is putting the jar into the lib/ folder:$ cp ./examples/streaming/TopSpeedWindowing.jar lib/Then, we can launch the JobManager:$ ./bin/standalone-job.sh start --job-classname org.apache.flink.streaming.examples.windowing.TopSpeedWindowingThe web interface is now available at localhost:8081. However, the application won’t be able to start, because there are no TaskManagers running yet:$ ./bin/taskmanager.sh startNote: You can start multiple TaskManagers, if your application needs more resources.Stopping the services is also supported via the scripts. Call them multiple times if you want to stop multiple instances, or use stop-all:$ ./bin/taskmanager.sh stop$ ./bin/standalone-job.sh stop YARNhttps://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/resource-providers/yarn/ Session Modestarting-a-flink-session-on-yarn 1234567891011121314151617181920export HADOOP_CLASSPATH=`hadoop classpath`# we assume to be in the root directory of # the unzipped Flink distribution# (0) export HADOOP_CLASSPATHexport HADOOP_CLASSPATH=`hadoop classpath`# (1) Start YARN Session./bin/yarn-session.sh --detached# (2) You can now access the Flink Web Interface through the# URL printed in the last lines of the command output, or through# the YARN ResourceManager web UI.# (3) Submit example job./bin/flink run ./examples/streaming/TopSpeedWindowing.jar# (4) Stop YARN session (replace the application id based # on the output of the yarn-session.sh command)echo \"stop\" | ./bin/yarn-session.sh -id application_XXXXX_XXX Congratulations! You have successfully run a Flink application by deploying Flink on YARN. We describe deployment with the Session Mode in the Getting Started guide at the top of the page. The Session Mode has two operation modes: attached mode (default): The yarn-session.sh client submits the Flink cluster to YARN, but the client keeps running, tracking the state of the cluster. If the cluster fails, the client will show the error. If the client gets terminated, it will signal the cluster to shut down as well. detached mode (-d or --detached): The yarn-session.sh client submits the Flink cluster to YARN, then the client returns. Another invocation of the client, or YARN tools is needed to stop the Flink cluster. The session mode will create a hidden YARN properties file in /tmp/.yarn-properties-&lt;username&gt;, which will be picked up for cluster discovery by the command line interface when submitting a job. You can also manually specify the target YARN cluster in the command line interface when submitting a Flink job. Here’s an example: 123./bin/flink run -t yarn-session \\ -Dyarn.application.id=application_XXXX_YY \\ ./examples/streaming/TopSpeedWindowing.jar You can re-attach to a YARN session using the following command: 1./bin/yarn-session.sh -id application_XXXX_YY Besides passing configuration via the conf/flink-conf.yaml file, you can also pass any configuration at submission time to the ./bin/yarn-session.sh client using -Dkey=value arguments. The YARN session client also has a few “shortcut arguments” for commonly used settings. They can be listed with ./bin/yarn-session.sh -h. Application Mode123456789101112131415161718192021Application Mode will launch a Flink cluster on YARN, where the main() method of the application jar gets executed on the JobManager in YARN. The cluster will shut down as soon as the application has finished. You can manually stop the cluster using yarn application -kill &lt;ApplicationId&gt; or by cancelling the Flink job../bin/flink run-application -t yarn-application ./examples/streaming/TopSpeedWindowing.jarOnce an Application Mode cluster is deployed, you can interact with it for operations like cancelling or taking a savepoint.# List running job on the cluster./bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY# Cancel running job./bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;Note that cancelling your job on an Application Cluster will stop the cluster.To unlock the full potential of the application mode, consider using it with the yarn.provided.lib.dirs configuration option and pre-upload your application jar to a location accessible by all nodes in your cluster. In this case, the command could look like:./bin/flink run-application -t yarn-application \\ -Dyarn.provided.lib.dirs=\"hdfs://myhdfs/my-remote-flink-dist-dir\" \\ hdfs://myhdfs/jars/my-application.jar The above will allow the job submission to be extra lightweight as the needed Flink jars and the application jar are going to be picked up by the specified remote locations rather than be shipped to the cluster by the client. Native_kuberneteshttps://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/filesystems/oss/ https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/resource-providers/native_kubernetes/ https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/ha/overview/ https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/ha/kubernetes_ha/ https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/config/#kubernetes K8s On SessionCreating service account: 12345678910#https://blog.csdn.net/yy8623977/article/details/124989262#创建namespace、service账号和给账号授权kubectl create ns flink-testkubectl create serviceaccount flink-test -n flink-testkubectl create clusterrolebinding flink-role-bind --clusterrole=edit --serviceaccount=flink-test:flink-test#Resolved: configmaps is forbidden: User &quot;system:serviceaccount:flink-test:default&quot;#https://blog.csdn.net/wangmiaoyan/article/details/103254006kubectl create clusterrolebinding flink-test:flink-test --clusterrole=cluster-admin --user=system:serviceaccount:flink-test:default Building required jars into docker image(Or mount a folder from NAS). Dockerfile: 1234567891011121314151617FROM apache/flink:1.17.2-scala_2.12USER root# Pod的时区默认是UTC，时间会比我们的少八小时。修改时区为Asia/Shanghai#RUN rm -f /etc/localtime &amp;&amp; ln -sv /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo &quot;Asia/Shanghai&quot; &gt; /etc/timezoneRUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime#解决不能写日志到dev下的问题RUN groupmod -g 1001 flinkRUN usermod -u 1001 flinkRUN chown -R 1001:1001 $FLINK_HOME/USER flinkRUN mkdir -p $FLINK_HOME/usrlib# Copying libsCOPY ./lib-1.17/* $FLINK_HOME/lib/ ll ./lib-1.17/ 123456-rw-r--r-- 1 root root 266420 Jun 15 2023 flink-connector-jdbc-3.1.1-1.17.jar-rw-r--r-- 1 root root 25743957 Nov 10 16:32 flink-oss-fs-hadoop-1.17.2.jar-rw-r--r-- 1 root root 28440546 Apr 13 2023 flink-sql-connector-elasticsearch7-3.0.1-1.17.jar-rw-r--r-- 1 root root 5566107 Oct 26 04:26 flink-sql-connector-kafka-3.0.1-1.17.jar-rw-r--r-- 1 root root 23715175 Jan 19 12:07 flink-sql-connector-mysql-cdc-3.0.1.jar-rw-r--r-- 1 root root 2515447 Jan 18 15:01 mysql-connector-j-8.0.31.jar Build and push to registry: 12docker build -t registry.zerofinance.net/library/flink:1.17.2 .docker push registry.zerofinance.net/library/flink:1.17.2 Starting flink job manager: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#For 1.15.x 1.16.xbin/kubernetes-session.sh \\ -Dkubernetes.namespace=flink-test \\ -Dkubernetes.jobmanager.service-account=flink-test \\ -Dkubernetes.cluster-id=flink-cluster \\ -Dakka.ask.timeout=100s \\ -Dfs.oss.endpoint=https://oss-cn-hongkong.aliyuncs.com \\ -Dfs.oss.accessKeyId=xxx \\ -Dfs.oss.accessKeySecret=yyy \\ -Dkubernetes.container.image=registry.zerofinance.net/library/flink:1.17.2 \\ -Dkubernetes.container.image.pull-policy=Always \\ -Dhigh-availability=org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory \\ -Dhigh-availability.storageDir=oss://flink-ha-test/recovery \\ -Dstate.backend=rocksdb \\ -Dstate.backend.incremental=true \\ -Dstate.checkpoints.dir=oss://flink-ha-test/flink-checkpoints \\ -Dstate.savepoints.dir=oss://flink-ha-test/flink-savepoints \\ -Dkubernetes.container.image.pull-secrets=zzz \\ -Dkubernetes.jobmanager.replicas=2 \\ -Dkubernetes.jobmanager.cpu=0.2 \\ -Djobmanager.memory.process.size=1024m \\ -Dresourcemanager.taskmanager-timeout=3600000 \\ -Dkubernetes.taskmanager.node-selector=flink-env:test \\ -Dkubernetes.taskmanager.tolerations=flink-env:test,operator:Exists,effect:NoSchedule \\ -Dkubernetes.taskmanager.cpu=0.2 \\ -Dtaskmanager.memory.process.size=4096m \\ -Denv.java.opts.jobmanager=-Duser.timezone=GMT+08 \\ -Denv.java.opts.taskmanager=-Duser.timezone=GMT+08 \\ -Dtaskmanager.numberOfTaskSlots=4#For 1.17.xbin/kubernetes-session.sh \\ -Dkubernetes.namespace=flink-test \\ -Dkubernetes.jobmanager.service-account=flink-test \\ -Dkubernetes.cluster-id=flink-test \\-Dkubernetes.rest-service.exposed.type=NodePort \\ -Dakka.ask.timeout=100s \\ -Dfs.oss.endpoint=https://oss-cn-hongkong.aliyuncs.com \\ -Dfs.oss.accessKeyId=xxx \\ -Dfs.oss.accessKeySecret=yyy \\ -Dkubernetes.container.image.ref=registry.zerofinance.net/library/flink:1.17.2 \\ -Dkubernetes.container.image.pull-policy=Always \\ -Dhigh-availability.type=kubernetes \\ -Dhigh-availability.storageDir=oss://flink-cluster-test/recovery \\ -Dstate.backend=rocksdb \\ -Dstate.backend.incremental=true \\ -Dstate.checkpoints.dir=oss://flink-cluster-test/flink-checkpoints \\ -Dstate.savepoints.dir=oss://flink-cluster-test/flink-savepoints \\ -Dkubernetes.container.image.pull-secrets=zzz \\ -Dkubernetes.jobmanager.replicas=2 \\ -Dkubernetes.jobmanager.cpu.amount=0.2 \\ -Djobmanager.memory.process.size=1024m \\ -Dresourcemanager.taskmanager-timeout=3600000 \\ -Dkubernetes.taskmanager.node-selector=flink-env:test \\ -Dkubernetes.taskmanager.tolerations=flink-env:test,operator:Exists,effect:NoSchedule \\ -Dkubernetes.taskmanager.cpu.amount=0.2 \\ -Dtaskmanager.memory.process.size=1024m \\ -Denv.java.opts.jobmanager=-Duser.timezone=GMT+08 \\ -Denv.java.opts.taskmanager=-Duser.timezone=GMT+08 \\ -Dtaskmanager.numberOfTaskSlots=2 Enable cluster-rest ingress: flink-cluster-rest-ingress.yml: 1234567891011121314apiVersion: extensions/v1beta1kind: Ingressmetadata: name: flink-cluster-rest-ingress namespace: flink-testspec: tls: [] rules: - host: flink-rest-test.zerofinance.net http: paths: - backend: serviceName: flink-cluster-rest servicePort: 8081 Submits a new job: 1bin/flink run -m flink-rest-test.zerofinance.net examples/batch/WordCount.jar Or: 123456bin/flink run \\ -e kubernetes-session \\ -Dkubernetes.namespace=flink-test \\ -Dkubernetes.rest-service.exposed.type=NodePort \\ -Dkubernetes.cluster-id=flink-cluster \\ examples/batch/WordCount.jar Destroy a existing cluster: 1kubectl -n flink-test delete deploy flink-cluster K8s On Application12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#Starting:#For 1.15.x 1.16.xbin/flink run-application \\ --target kubernetes-application \\ -Dkubernetes.namespace=flink-test \\ -Dkubernetes.jobmanager.service-account=flink \\ -Dkubernetes.rest-service.exposed.type=NodePort \\ -Dkubernetes.cluster-id=flink-application-cluster \\ -Dkubernetes.container.image=registry.zerofinance.net/library/flink:1.17.2 \\ -Dkubernetes.container.image.pull-policy=Always \\ -Dfs.oss.endpoint=https://oss-cn-hongkong.aliyuncs.com \\ -Dfs.oss.accessKeyId=xxx \\ -Dfs.oss.accessKeySecret=yyy \\ -Dhigh-availability=org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory \\ -Dhigh-availability.storageDir=oss://flink-ha-test/native-recovery \\ -Dstate.backend=rocksdb \\ -Dstate.backend.incremental=true \\ -Dstate.checkpoints.dir=oss://flink-ha-test/flink-application-checkpoints \\ -Dstate.savepoints.dir=oss://flink-ha-test/flink-application-savepoints \\ -Dkubernetes.container.image.pull-secrets=zzz \\ -Dkubernetes.jobmanager.replicas=1 \\ -Denv.java.opts.jobmanager=-Duser.timezone=GMT+08 \\ -Dkubernetes.jobmanager.cpu=0.2 \\ -Djobmanager.memory.process.size=1024m \\ -Dresourcemanager.taskmanager-timeout=3600000 \\ -Denv.java.opts.taskmanager=-Duser.timezone=GMT+08 \\ -Dkubernetes.taskmanager.node-selector=flink-env:test \\ -Dkubernetes.taskmanager.cpu=0.2 \\ -Dtaskmanager.memory.process.size=4096m \\ -Dtaskmanager.numberOfTaskSlots=4 \\ local:///opt/flink/examples/streaming/TopSpeedWindowing.jar \\ --output /opt/flink/log/topSpeedWindowing-output#For 1.17.xbin/flink run-application \\ --target kubernetes-application \\ -Dkubernetes.namespace=flink-test \\ -Dkubernetes.jobmanager.service-account=flink \\ -Dkubernetes.rest-service.exposed.type=NodePort \\ -Dkubernetes.cluster-id=flink-application-cluster \\ -Dkubernetes.container.image.ref=registry.zerofinance.net/library/flink:1.17.2 \\ -Dkubernetes.container.image.pull-policy=Always \\ -Dfs.oss.endpoint=https://oss-cn-hongkong.aliyuncs.com \\ -Dfs.oss.accessKeyId=xxx \\ -Dfs.oss.accessKeySecret=yyy \\ -Dhigh-availability.type=kubernetes \\ -Dhigh-availability.storageDir=oss://flink-ha-test/native-recovery \\ -Dstate.backend=rocksdb \\ -Dstate.backend.incremental=true \\ -Dstate.checkpoints.dir=oss://flink-ha-test/flink-application-checkpoints \\ -Dstate.savepoints.dir=oss://flink-ha-test/flink-application-savepoints \\ -Dkubernetes.container.image.pull-secrets=zzz \\ -Dkubernetes.jobmanager.replicas=1 \\ -Denv.java.opts.jobmanager=-Duser.timezone=GMT+08 \\ -Dkubernetes.jobmanager.cpu.amount=0.2 \\ -Djobmanager.memory.process.size=1024m \\ -Dresourcemanager.taskmanager-timeout=3600000 \\ -Denv.java.opts.taskmanager=-Duser.timezone=GMT+08 \\ -Dkubernetes.taskmanager.node-selector=flink-env:test \\ -Dkubernetes.taskmanager.cpu.amount=0.2 \\ -Dtaskmanager.memory.process.size=4096m \\ -Dtaskmanager.numberOfTaskSlots=4 \\ local:///opt/flink/examples/streaming/TopSpeedWindowing.jar \\ --output /opt/flink/log/topSpeedWindowing-output #list running jobs:bin/flink list \\ --target kubernetes-application \\ -Dkubernetes.namespace=flink-test \\ -Dkubernetes.jobmanager.service-account=flink \\ -Dkubernetes.cluster-id=flink-application-cluster#Delete job:bin/flink cancel \\ --target kubernetes-application \\ -Dkubernetes.namespace=flink-test \\ -Dkubernetes.jobmanager.service-account=flink \\ -Dkubernetes.cluster-id=flink-application-cluster \\ 5d7a3c36c7d40ceeb8b83fd8a563ded5 Sql ClientFlink 使用之 SQL Client - 简书 (jianshu.com) Standalone1234567start-cluster.shsql-client.sh embedded#https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sqlclient/#Submit sql via sql-client.sh./flink-1.17.2/bin/sql-client.sh -f ./test.sql On yarn SessionSQL-Client On Yarn Session Configuring SQL Client for session mode | CDP Private Cloud (cloudera.com) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#Start a yarn session#提交yarn session和启动sql client需要使用同一个系统用户，否则会找不到yarn session对应的application id。sudo su - hadoop#yarn-session.sh -dyarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -dcat /works/demo.csv 1,a,112,b,223,c,334,d,44sudo -u hdfs hadoop fs -put /works/demo.csv /works/test/demo.csvsql-client.sh embedded -s yarn-session# 在专门的界面展示，使用分页table格式。可按照界面下方说明，使用快捷键前后翻页和退出到SQL命令行SET sql-client.execution.result-mode = table;# changelog格式展示，可展示数据增(I)删(D)改(U)SET sql-client.execution.result-mode = changelog;# 接近传统数据库的展示方式，不使用专门界面SET sql-client.execution.result-mode = tableau;Flink SQL&gt; CREATE TABLE MyTable( a INT, b STRING, c STRING) WITH ( 'connector' = 'filesystem', 'path' = 'hdfs:///works/test/demo.csv', 'format' = 'csv');Flink SQL&gt; select * from MyTable;#Kill an existing yarn-sessionyarn application -listecho \"stop\" | yarn-session.sh -id &lt;application_id&gt;#kafka Connector:wget https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka/1.15.3/flink-sql-connector-kafka-1.15.3.jarwget https://repo1.maven.org/maven2/org/apache/flink/flink-connector-jdbc/1.15.3/flink-connector-jdbc-1.15.3.jarscp flink-sql-connector-kafka-1.15.3.jar flink-connector-jdbc-1.15.3.jar mysql-connector-j-8.0.31.jar root@192.168.80.226:/usr/bigtop/current/flink-client/lib/scp flink-sql-connector-kafka-1.15.3.jar flink-connector-jdbc-1.15.3.jar mysql-connector-j-8.0.31.jar root@192.168.80.227:/usr/bigtop/current/flink-client/lib/scp flink-sql-connector-kafka-1.15.3.jar flink-connector-jdbc-1.15.3.jar mysql-connector-j-8.0.31.jar root@192.168.80.228:/usr/bigtop/current/flink-client/lib/scp flink-sql-connector-kafka-1.15.3.jar flink-connector-jdbc-1.15.3.jar mysql-connector-j-8.0.31.jar root@192.168.80.229:/usr/bigtop/current/flink-client/lib/Hive Connector:wget https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jarwget https://repo1.maven.org/maven2/org/apache/flink/flink-connector-hive_2.12/1.15.3/flink-connector-hive_2.12-1.15.3.jarwget https://repo1.maven.org/maven2/org/apache/hive/hive-exec/2.3.4/hive-exec-2.3.4.jarscp antlr-runtime-3.5.2.jar flink-connector-hive_2.12-1.15.3.jar hive-exec-2.3.4.jar root@192.168.80.226:/usr/bigtop/current/flink-client/lib/scp antlr-runtime-3.5.2.jar flink-connector-hive_2.12-1.15.3.jar hive-exec-2.3.4.jar root@192.168.80.227:/usr/bigtop/current/flink-client/lib/scp antlr-runtime-3.5.2.jar flink-connector-hive_2.12-1.15.3.jar hive-exec-2.3.4.jar root@192.168.80.228:/usr/bigtop/current/flink-client/lib/scp antlr-runtime-3.5.2.jar flink-connector-hive_2.12-1.15.3.jar hive-exec-2.3.4.jar root@192.168.80.229:/usr/bigtop/current/flink-client/lib/scp /usr/bigtop/current/flink-client/conf/flink-conf.yaml root@192.168.80.226:/usr/bigtop/current/flink-client/conf/scp /usr/bigtop/current/flink-client/conf/flink-conf.yaml root@192.168.80.227:/usr/bigtop/current/flink-client/conf/scp /usr/bigtop/current/flink-client/conf/flink-conf.yaml root@192.168.80.228:/usr/bigtop/current/flink-client/conf/scp /usr/bigtop/current/flink-client/conf/flink-conf.yaml root@192.168.80.229:/usr/bigtop/current/flink-client/conf/#java.lang.ClassNotFoundException: org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormat#Has to reboot flink-clusterstop-cluster.shstart-cluster.sh ConnectorsFlink doesn’t include any connector depended libraries, you need to download them manually. 12345678910111213141516171819202122232425262728293031323334353637383940#kafka Connector:wget https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka/1.15.3/flink-sql-connector-kafka-1.15.3.jarwget https://repo1.maven.org/maven2/org/apache/flink/flink-connector-jdbc/1.15.3/flink-connector-jdbc-1.15.3.jarscp flink-sql-connector-kafka-1.15.3.jar flink-connector-jdbc-1.15.3.jar mysql-connector-j-8.0.31.jar root@192.168.80.226:/usr/bigtop/current/flink-client/lib/scp flink-sql-connector-kafka-1.15.3.jar flink-connector-jdbc-1.15.3.jar mysql-connector-j-8.0.31.jar root@192.168.80.227:/usr/bigtop/current/flink-client/lib/scp flink-sql-connector-kafka-1.15.3.jar flink-connector-jdbc-1.15.3.jar mysql-connector-j-8.0.31.jar root@192.168.80.228:/usr/bigtop/current/flink-client/lib/scp flink-sql-connector-kafka-1.15.3.jar flink-connector-jdbc-1.15.3.jar mysql-connector-j-8.0.31.jar root@192.168.80.229:/usr/bigtop/current/flink-client/lib/#Hive Connector:wget https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jarwget https://repo1.maven.org/maven2/org/apache/flink/flink-connector-hive_2.12/1.15.3/flink-connector-hive_2.12-1.15.3.jarwget https://repo1.maven.org/maven2/org/apache/hive/hive-exec/2.3.4/hive-exec-2.3.4.jarscp antlr-runtime-3.5.2.jar flink-connector-hive_2.12-1.15.3.jar hive-exec-2.3.4.jar root@192.168.80.226:/usr/bigtop/current/flink-client/lib/scp antlr-runtime-3.5.2.jar flink-connector-hive_2.12-1.15.3.jar hive-exec-2.3.4.jar root@192.168.80.227:/usr/bigtop/current/flink-client/lib/scp antlr-runtime-3.5.2.jar flink-connector-hive_2.12-1.15.3.jar hive-exec-2.3.4.jar root@192.168.80.228:/usr/bigtop/current/flink-client/lib/scp antlr-runtime-3.5.2.jar flink-connector-hive_2.12-1.15.3.jar hive-exec-2.3.4.jar root@192.168.80.229:/usr/bigtop/current/flink-client/lib/#For hdfs Connector:wget https://repo1.maven.org/maven2/org/apache/flink/flink-table-planner_2.12/1.15.3/flink-table-planner_2.12-1.15.3.jarscp flink-table-planner_2.12-1.15.3.jar root@192.168.80.226:/usr/bigtop/current/flink-client/lib/scp flink-table-planner_2.12-1.15.3.jar root@192.168.80.227:/usr/bigtop/current/flink-client/lib/scp flink-table-planner_2.12-1.15.3.jar root@192.168.80.228:/usr/bigtop/current/flink-client/lib/scp flink-table-planner_2.12-1.15.3.jar root@192.168.80.229:/usr/bigtop/current/flink-client/lib/#delete flink-table-planner-loader-1.15.3.jar from each machines:rm flink-table-planner-loader-1.15.3.jar#Need to reboot flink cluster or flink on yarn.#Kill an existing yarn-sessionyarn application -listecho \"stop\" | yarn-session.sh -id &lt;application_id&gt;yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d#Copying them to all libs of machine:scp /usr/bigtop/current/flink-client/conf/flink-conf.yaml root@192.168.80.226:/usr/bigtop/current/flink-client/conf/scp /usr/bigtop/current/flink-client/conf/flink-conf.yaml root@192.168.80.227:/usr/bigtop/current/flink-client/conf/scp /usr/bigtop/current/flink-client/conf/flink-conf.yaml root@192.168.80.228:/usr/bigtop/current/flink-client/conf/scp /usr/bigtop/current/flink-client/conf/flink-conf.yaml root@192.168.80.229:/usr/bigtop/current/flink-client/conf/ Restore job123456789101112131415161718192021222324#https://mp.weixin.qq.com/s/srUyvNr7KX1PSOaG1d8qdQ?poc_token=HGNqmWWjWqONGGyPVzyzKzDhIsyauXyJ8kOW8Bfl#https://mp.weixin.qq.com/s/o8mL0UkH4j_h5mjyayn0XQvim flink/conf/flink-conf.yamlstate.savepoints.dir: file:///works/app/flink/flink-1.15.3/flink-savepoints#Restart cluster#Starting from the latest savepoints when restart the job:curl -XPOST http://jobmanager-host:port/jobs/jobId/savepointscurl -X PATCH http://jobmanager-host:port/jobs/jobIdDinky:经验证Dinky平台支持Savepoint机制。任务重启后仅消费最新的数据。前置条件：修改Flink家目录下 flink/conf/flink-conf.yaml 文件，指定savepoint目录位置操作步骤： 任务配置 开启右边 SavePoint 策略，选择 “最近一次” SavePoint 停止 FlinkSQL 作业 点击 Dinky 的运维中心菜单，在任务列表里点击上面运行的这个任务进入任务详情页面，在页面右上角点击三个点的省略号按钮，弹出框中点击 “SavePoint停止” 重启作业 在 Dinky 的运维中心，任务列表，任务详情页面，重启任务 Optimize1234567891011121314151617181920212223SET parallelism.default=1;SET execution.runtime-mode = streaming;SET table.local-time-zone =Asia/Shanghai; -- 10分钟SET execution.checkpointing.interval = 10m; -- 失败容忍度SET execution.checkpointing.tolerable-failed-checkpoints = 50; -- 超时时间SET execution.checkpointing.timeout =10000; -- 一次语义SET execution.checkpointing.mode = EXACTLY_ONCE; -- 固定频次SET restart-strategy= fixed-delay; -- 尝试5次SET restart-strategy.fixed-delay.attempts = 5; -- 重启延时50sSET restart-strategy.fixed-delay.delay = 50s;-- 调优参数SET table.exec.mini-batch.enabled = trueSET table.exec.mini-batch.allow-latency = 2sSET table.exec.mini-batch.size = 5000SET table.optimizer.distinct-agg.split.enabled = true StreamParkRecommend. Apache StreamPark (incubating) | Apache StreamPark (incubating) InstallationStandalone1234567891011121314151617181920212223242526272829303132333435363738394041424344#https://streampark.apache.org/docs/user-guide/deploymenttar zxvf apache-streampark_2.12-2.1.2-incubating-bin.tar.gz mv apache-streampark_2.12-2.1.2-incubating-bin apache-streampark_2.12cd apache-streampark_2.12/cd apache-streampark_2.12/script/schema/mysql -uroot -h127.0.0.1 -pCREATE USER 'streampark'@'%' IDENTIFIED BY 'Aa123456';GRANT ALL PRIVILEGES ON streampark.* TO 'streampark'@'%';exit;mysql -uroot -h127.0.0.1 -p &lt; mysql-schema.sql cd apache-streampark_2.12/script/data/mysql -uroot -h127.0.0.1 -p &lt; mysql-data.sql cd apache-streampark_2.12/conf/vim application.yml spring: profiles.active: mysql vim application-mysql.ymlspring: datasource: username: root password: xxxx driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/streampark?useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;allowPublicKeyRetrieval=false&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=GMT%2B8mysql -uroot -h127.0.0.1 -p streampark: # HADOOP_USER_NAME If it is on yarn mode ( yarn-prejob | yarn-application | yarn-session), you need to configure hadoop-user-name hadoop-user-name: hdfs # Local workspace, used to store project source code, build directory, etc. workspace: local: /data/streampark_workspace#Startingcp -a /works/app/flink/lib-1.17/mysql-connector-j-8.0.31.jar /works/app/flink/apache-streampark_2.12/lib/cd apache-streampark_2.12/binbash startup.sh Noticed: In order to launch kubernetes flink environment, you must have config file of kubectl(~/.kube/config) installed. Docker Dockerfile raw.githubusercontent.com/apache/incubator-streampark/v2.1.2/deploy/docker/Dockerfile 123456789101112131415161718192021222324252627282930313233343536FROM alpine:3.16 as deps-stageRUN mkdir -p ~/.kube ~/.m2 /works/app/flink/ /data/streampark_workspaceCOPY apache-streampark_2.12.tar.gz /WORKDIR /RUN tar xf apache-streampark_2.12.tar.gz &amp;&amp; mv apache-streampark_2.12 streampark RUN rm -rf /apache-streampark_2.12.tar.gzFROM docker:dindWORKDIR /streamparkCOPY --from=deps-stage /streampark /streamparkENV NODE_VERSION=16.1.0ENV NPM_VERSION=7.11.2RUN apk add openjdk8 \\ &amp;&amp; apk add maven \\ &amp;&amp; apk add wget \\ &amp;&amp; apk add vim \\ &amp;&amp; apk add bash \\ &amp;&amp; apk add curlENV JAVA_HOME=/usr/lib/jvm/java-1.8-openjdkENV MAVEN_HOME=/usr/share/java/maven-3ENV PATH $JAVA_HOME/bin:$PATHENV PATH $MAVEN_HOME/bin:$PATHRUN wget \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.gz\" \\ &amp;&amp; tar zxvf \"node-v$NODE_VERSION-linux-x64.tar.gz\" -C /usr/local --strip-components=1 \\ &amp;&amp; rm \"node-v$NODE_VERSION-linux-x64.tar.gz\" \\ &amp;&amp; ln -s /usr/local/bin/node /usr/local/bin/nodejs \\ &amp;&amp; curl -LO https://dl.k8s.io/release/v1.23.0/bin/linux/amd64/kubectl \\ &amp;&amp; install -o root -g root -m 0755 kubectl /usr/local/bin/kubectlEXPOSE 10000 Building 1DOCKER_BUILDKIT=0 docker build -t \"registry.zerofinance.net/flink/streampark:2.1.2\" . Creating docker 123456789docker run -d --name \"streampark\" \\--privileged=true \\-v /works/app/flink/streampark/config:/root/.kube/config:ro \\-v /works/app/flink/streampark/settings.xml:/Developer/apache-maven-3.5.4/conf/settings.xml:ro \\-v /works/app/flink/streampark/flink-1.17.2:/works/app/flink/flink-1.17.2 \\-v /works/app/flink/streampark/application.yml:/streampark/conf/application.yml \\-v /works/app/flink/streampark/application-mysql.yml:/streampark/conf/application-mysql.yml \\-p 10000:10000 \\registry.zerofinance.net/flink/streampark:2.1.2 Start streampark instance 12docker exec -it streampark bash/streampark/bin/startup.sh K8s12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697apiVersion: apps/v1kind: Deploymentmetadata: labels: app: streampark name: streampark namespace: flink-testspec: replicas: 1 selector: matchLabels: app: streampark strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: streampark spec: containers: - image: registry.zerofinance.net/flink/streampark:2.1.2 name: streampark imagePullPolicy: Always command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;wget -P lib https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.0.31/mysql-connector-j-8.0.31.jar &amp;&amp; bash bin/streampark.sh start_docker &quot;] securityContext: privileged: true ports: - containerPort: 10000 name: streampark protocol: TCP resources: limits: cpu: &quot;1&quot; memory: 1024Mi requests: cpu: 500m memory: 500Mi volumeMounts: - name: flink mountPath: /works/app/flink/flink-1.17.2 - name: maven-setting mountPath: /root/.m2/settings.xml - name: k8s-config mountPath: /root/.kube/config - name: application mountPath: /streampark/conf/application.yml - name: application-mysql mountPath: /streampark/conf/application-mysql.yml - name: workspace mountPath: /data/streampark_workspace - name: docker mountPath: /var/run/docker.sock restartPolicy: Always volumes: - name: flink hostPath: path: /data/data/streampark_flink/flink-1.17.2 - name: maven-setting hostPath: path: /data/data/streampark_flink/settings.xml - name: k8s-config hostPath: path: /data/data/streampark_flink/kube/config - name: application hostPath: path: /data/data/streampark_flink/application.yml - name: application-mysql hostPath: path: /data/data/streampark_flink/application-mysql.yml - name: workspace hostPath: path: /data/data/streampark_flink/workspace - name: docker hostPath: path: /var/run/docker.sock---apiVersion: v1kind: Servicemetadata: name: streampark namespace: flink-test labels: app: streamparkspec: externalTrafficPolicy: Local type: NodePort ports: - name: streampark port: 10000 protocol: TCP targetPort: 10000 selector: app: streampark sessionAffinity: None ConfigurationSystem Setting First, you need creating a registry project named “flink” from menu: “Project Quotas”. Flink Cluster Flink Home Applicationapplication sql Job(Recommend) session sql jobYou have to start the session instance from “Settings—&gt;Flink Cluster” application jar jobCreate Project first: Create a new jar job: Pod templatePod TemplateIn order to collect logs to Loki: 123456789101112131415apiVersion: v1kind: Podmetadata: name: taskmanager-pod-templatespec: containers: # Do not change the main container name - name: flink-main-container volumeMounts: - name: flink-logs mountPath: /opt/flink/log volumes: - name: flink-logs hostPath: path: /works/log/hkloan/dev/la-loan-account Dynamic PropertiesYou can simplify “Dynamic Properties”: 1234567891011121314151617181920-Dakka.ask.timeout=100s-Dfs.oss.endpoint=https://oss-cn-hongkong.aliyuncs.com-Dfs.oss.accessKeyId=xxx-Dfs.oss.accessKeySecret=yyy-Dkubernetes.container.image.pull-policy=IfNotPresent-Dhigh-availability.type=kubernetes-Dhigh-availability.storageDir=oss://flink-cluster-uat/recovery-application-Dstate.backend=rocksdb-Dstate.backend.incremental=true-Dstate.checkpoints.dir=oss://flink-cluster-uat/flink-application-checkpoints-Dstate.savepoints.dir=oss://flink-cluster-uat/flink-application-savepoints-Dkubernetes.container.image.pull-secrets=zzz-Dkubernetes.jobmanager.replicas=1-Dkubernetes.jobmanager.cpu.amount=0.5-Dresourcemanager.taskmanager-timeout=3600000-Dkubernetes.taskmanager.node-selector=flink-env:test-Dkubernetes.taskmanager.tolerations=flink-env:test,operator:Exists,effect:NoSchedule-Dkubernetes.taskmanager.cpu.amount=1-Denv.java.opts.jobmanager=\"-Duser.timezone=GMT+08\"-Denv.java.opts.taskmanager=\"-Duser.timezone=GMT+08\" Clean all Jobs12k -n flink-dev delete deploy myql2es-deploy-demok -n flink-dev delete cm myql2es-deploy-demo-cluster-config-map UDFCreate a project first: Adding dependency pom in a job: DinkyA alternative Flink stream platform, like StreamPark. But I recommend using StreamPark strongly. http://www.dlink.top/docs/0.7/get_started/docker_deploy http://www.dlink.top/docs/0.7/deploy_guide/build Prerequisite: dinky.sql 12345678mysql -uroot -pCREATE USER &apos;dinky&apos;@&apos;%&apos; IDENTIFIED BY &apos;Aa123456&apos;;GRANT ALL PRIVILEGES ON dinky.* TO &apos;dinky&apos;@&apos;%&apos;;mysql -udinky -h127.0.0.1 -pcreate database dinky;use dinky;source /works/app/flink/dinky-mysql.sql; Linux InstallFor 1.0.0 version: 123456789101112131415161718192021222324252627282930313233343536373839404142434445#http://www.dinky.org.cn/docs/next/deploy_guide/normal_deploywget https://github.com/DataLinkDC/dinky/releases/download/v1.0.0-rc4/dinky-release-1.17-1.0.0-rc4.tar.gztar zxvf dinky-release-1.17-1.0.0-rc4.tar.gz cd dinky-release-1.17-1.0.0-rc4/#Download sql file to local:cd sqlsz dinky-mysql.sql #Edit application filescd /works/app/dinky/dinky-release-1.17-1.0.0-rc4/cd config/vim application.yml vim application-mysql.yml #copy jdbc driver:cp -a /works/app/flink/lib-1.17/mysql-connector-j-8.0.31.jar /works/app/dinky/dinky-release-1.17-1.0.0-rc4/lib/#Copy Flink dependency lib jars:cp -a /works/app/flink/flink-1.17.2/lib/flink-*.jar /works/app/dinky/dinky-release-1.17-1.0.0-rc4/extends/flink1.17/#Copy Extra dependency lib jars:cp -a /works/app/flink/lib-1.17/* /works/app/dinky/dinky-release-1.17-1.0.0-rc4/extends/flink1.17/rm /works/app/dinky/dinky-release-1.17-1.0.0-rc4/extends/flink1.17/flink-table-planner-loader-1.17.2.jarcp -a /works/app/flink/flink-1.17.2/opt/flink-table-planner_2.12-1.17.2.jar /works/app/dinky/dinky-release-1.17-1.0.0-rc4/extends/flink1.17/#Mysql jdbc driver:cp -a /works/app/dinky/flink-1.17.2-lib/mysql-connector-j-8.0.31.jar /works/app/dinky/dinky-release-1.17-1.0.0-rc4/extends/flink1.17/#Hadoop dependency lib jars:cd /works/app/dinky/dinky-release-1.17-1.0.0-rc4/extends/flink1.17/wget https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/flink/flink-shaded-hadoop-3-uber/3.1.1.7.2.9.0-173-9.0/flink-shaded-hadoop-3-uber-3.1.1.7.2.9.0-173-9.0.jar#整库同步jar:cp -a /works/app/dinky/dinky-release-1.17-1.0.0-rc4/lib/dinky-client-base-1.0.0-rc4.jar /works/app/flink/flink-1.17.2/lib/cp -a /works/app/dinky/dinky-release-1.17-1.0.0-rc4/lib/dinky-common-1.0.0-rc4.jar /works/app/flink/flink-1.17.2/lib/cp -a /works/app/dinky/dinky-release-1.17-1.0.0-rc4/extends/flink1.17/dinky/dinky-client-1.17-1.0.0-rc4.jar /works/app/flink/flink-1.17.2/lib/#Need flink-cdc-common jar:wget https://repo1.maven.org/maven2/com/ververica/flink-cdc-common/3.0.1/flink-cdc-common-3.0.1.jar -P /works/app/flink/flink-1.17.2/lib/wget https://repo1.maven.org/maven2/com/ververica/flink-cdc-common/3.0.1/flink-cdc-common-3.0.1.jar -P /works/app/dinky/dinky-release-1.17-1.0.0-rc4/extends/flink1.17/#https://blog.csdn.net/lisi1129/article/details/101453563#在conf/flink-conf.yaml 添加如下内容并重启 flink.classloader.resolve-order: parent-first#Startsh auto.sh start 1.17#Stopsh auto.sh stop DockerFor 1.0.0 version: 123456789101112131415161718192021222324252627282930313233343536FROM openjdk:8u342-oracle as build-stageARG DINKY_VERSIONENV DINKY_VERSION=$&#123;DINKY_VERSION&#125;ARG FLINK_BIG_VERSIONENV FLINK_BIG_VERSION $&#123;FLINK_BIG_VERSION&#125;ADD ./build/dinky-release-$&#123;DINKY_VERSION&#125;.tar.gz /opt/USER rootRUN mv /opt/dinky-release-$&#123;DINKY_VERSION&#125; /opt/dinky/RUN mkdir /opt/dinky/confCOPY ./flink$&#123;FLINK_BIG_VERSION&#125;-lib/*.jar /opt/dinky/extends/flink$&#123;FLINK_BIG_VERSION&#125;/##不复制的话dinky applicaition下显示不了日志COPY ./flink$&#123;FLINK_BIG_VERSION&#125;-conf/* /opt/dinky/conf/ADD ./build/flink-python-1.17.2.jar /opt/dinky/lib/COPY ./flink$&#123;FLINK_BIG_VERSION&#125;-lib/mysql-connector-j-8.0.31.jar /opt/dinky/lib/RUN mkdir -p /opt/dinky/run &amp;&amp; mkdir -p /opt/dinky/logs &amp;&amp; touch /opt/dinky/logs/dinky.logRUN chmod -R 777 /opt/dinky/FROM openjdk:8u342-oracle as production-stageRUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeRUN export LANG=zh_CN.UTF-8COPY --from=build-stage /opt/dinky/ /opt/dinky/RUN microdnf install procps -yWORKDIR /opt/dinky/EXPOSE 8888CMD touch /opt/dinky/logs/dinky.log &amp;&amp; ./auto.sh restart $&#123;FLINK_BIG_VERSION&#125; &amp;&amp; tail -f /opt/dinky/logs/dinky.log All jars in flink1.17-lib: 12345678910111213141516171819202122232425ll build/总用量 208880-rw-rw-r-- 1 dev dev 180884317 1月 29 15:11 dinky-release-1.17-1.0.0-rc4.tar.gz-rw-r--r-- 1 dev dev 32998809 11月 13 12:47 flink-python-1.17.2.jarll flink1.17-lib/总用量 322180-rw-rw-r-- 1 dev dev 251405 1月 19 11:57 flink-cdc-common-3.0.1.jar-rw-r--r-- 1 dev dev 196491 11月 13 12:27 flink-cep-1.17.2.jar-rw-r--r-- 1 dev dev 542629 11月 13 12:33 flink-connector-files-1.17.2.jar-rw-r--r-- 1 dev dev 266420 6月 15 2023 flink-connector-jdbc-3.1.1-1.17.jar-rw-r--r-- 1 dev dev 102470 11月 13 12:40 flink-csv-1.17.2.jar-rw-r--r-- 1 dev dev 121809282 11月 13 12:57 flink-dist-1.17.2.jar-rw-r--r-- 1 dev dev 180246 11月 13 12:40 flink-json-1.17.2.jar-rw-r--r-- 1 dev dev 25743957 11月 10 16:32 flink-oss-fs-hadoop-1.17.2.jar-rw-r--r-- 1 dev dev 21043317 11月 13 12:54 flink-scala_2.12-1.17.2.jar-rw-rw-r-- 1 dev dev 59604787 8月 11 2023 flink-shaded-hadoop-3-uber-3.1.1.7.2.9.0-173-9.0.jar-rw-r--r-- 1 dev dev 28440546 4月 13 2023 flink-sql-connector-elasticsearch7-3.0.1-1.17.jar-rw-rw-r-- 1 dev dev 5566107 10月 26 04:26 flink-sql-connector-kafka-3.0.1-1.17.jar-rw-r--r-- 1 dev dev 23715175 2月 20 16:47 flink-sql-connector-mysql-cdc-3.0.1.jar-rw-r--r-- 1 dev dev 15407408 11月 13 12:55 flink-table-api-java-uber-1.17.2.jar-rw-r--r-- 1 dev dev 21333608 2月 22 16:19 flink-table-planner_2.12-1.17.2.jar-rw-r--r-- 1 dev dev 3146303 11月 13 12:27 flink-table-runtime-1.17.2.jar-rw-r--r-- 1 dev dev 2515447 2月 20 13:40 mysql-connector-j-8.0.31.jar Build and push to registry: 1234567891011121314151617docker build --build-arg FLINK_BIG_VERSION=1.17 --build-arg DINKY_VERSION=1.17-1.0.0-rc4 -t &quot;registry.zerofinance.net/flink/dinky-flink:1.17-1.0.0-rc4&quot; .docker push registry.zerofinance.net/flink/dinky-flink:1.17-1.0.0-rc4#Rundocker run \\-d \\--restart=always \\-p 8888:8888 \\-e FLINK_BIG_VERSION=1.17 \\-e DB_ACTIVE=mysql \\-e MYSQL_ADDR=rm-xxxxxx.mysql.rds.aliyuncs.com:3306 \\-e MYSQL_DATABASE=dinky_test \\-e MYSQL_USERNAME=dinky_test \\-e MYSQL_PASSWORD=xxxxxx \\--name dinky-server \\registry.zerofinance.net/flink/dinky-flink:1.17-1.0.0-rc4 flink-dinky-template.yml: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104apiVersion: apps/v1kind: Deploymentmetadata: labels: app: flink-dinky name: flink-dinky-test namespace: flink-testspec: selector: matchLabels: app: flink-dinky template: metadata: labels: app: flink-dinky spec: imagePullSecrets: - name: registry-private-secret containers: - image: registry.zerofinance.net/flink/dinky-flink:1.17-1.0.0-rc4 imagePullPolicy: IfNotPresent name: flink-dinky env: - name: FLINK_BIG_VERSION value: \"1.17\" - name: DB_ACTIVE value: mysql - name: MYSQL_ADDR value: rm-xxx.mysql.rds.aliyuncs.com:3306 - name: MYSQL_DATABASE value: dinky_test - name: MYSQL_USERNAME value: dinky_test - name: MYSQL_PASSWORD value: xxx ports: - name: dinky-port containerPort: 8888 protocol: TCP resources: limits: memory: \"1024Mi\" cpu: \"1000m\" requests: cpu: 500m memory: 500Mi livenessProbe: failureThreshold: 3 httpGet: path: / port: 8888 initialDelaySeconds: 30 periodSeconds: 10 #successThreshold: 1 timeoutSeconds: 5 readinessProbe: failureThreshold: 3 httpGet: path: / port: 8888 initialDelaySeconds: 30 periodSeconds: 10 #successThreshold: 1 timeoutSeconds: 5 volumeMounts: - name: kube-config mountPath: /root/.kube volumes: - name: kube-config hostPath: path: /data/data/kube-config---apiVersion: v1kind: Servicemetadata: name: flink-dinky-test namespace: flink-testspec: ports: - name: flink-dinky port: 8888 targetPort: 8888 protocol: TCP #nodePort: 32323 selector: app: flink-dinky #type: NodePort ---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: flink-dinky-ingress namespace: flink-testspec: tls: [] rules: - host: flink-dinky-test.zerofinance.net http: paths: - backend: serviceName: flink-dinky-test servicePort: 8888 On ApplicationMust build your own image: DinkyFlinkDockerfile(1.0.0): 123456789101112131415161718192021222324252627# 用来构建dinky环境ARG FLINK_VERSION=1.17.2ARG FLINK_BIG_VERSION=1.17#FROM flink:$&#123;FLINK_VERSION&#125;FROM registry.zerofinance.net/library/flink:$&#123;FLINK_VERSION&#125;ARG FLINK_VERSIONARG FLINK_BIG_VERSIONENV PYTHON_HOME /opt/miniconda3USER rootRUN wget \"https://s3.jcloud.sjtu.edu.cn/899a892efef34b1b944a19981040f55b-oss01/anaconda/miniconda/Miniconda3-py38_4.9.2-Linux-x86_64.sh\" -O \"miniconda.sh\" &amp;&amp; chmod +x miniconda.shRUN ./miniconda.sh -b -p $PYTHON_HOME &amp;&amp; chown -R flink $PYTHON_HOME &amp;&amp; ls $PYTHON_HOMEUSER flinkENV PATH $PYTHON_HOME/bin:$PATHRUN pip install \"apache-flink==$&#123;FLINK_VERSION&#125;\" -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com#RUN cp /opt/flink/opt/flink-python_* /opt/flink/lib/RUN cp /opt/flink/opt/flink-python-* /opt/flink/lib/#RUN wget -O dinky-app-$&#123;FLINK_BIG_VERSION&#125;.jar - $&#123;DINKY_HTTP&#125;/downloadAppJar/$&#123;FLINK_BIG_VERSION&#125; | mv dinky-app-$&#123;FLINK_BIG_VERSION&#125;.jarCOPY ./dinky-lib/* /opt/flink/lib/#Replace flink-table-planner-loader as flink-table-planner(Already included in dinky-lib folder, so need to delete)RUN rm -fr /opt/flink/lib/flink-table-planner-loader-1.17.2.jar All jars in dinky-lib: 12345678ll dinky-lib/总用量 49444-rwxrwxr-x 1 dev dev 28586238 1月 29 14:43 dinky-app-1.17-1.0.0-rc4-jar-with-dependencies.jar-rwxrwxr-x 1 dev dev 94360 1月 29 14:42 dinky-client-1.17-1.0.0-rc4.jar-rwxrwxr-x 1 dev dev 78413 1月 29 14:44 dinky-client-base-1.0.0-rc4.jar-rwxrwxr-x 1 dev dev 269936 1月 29 14:44 dinky-common-1.0.0-rc4.jar-rw-rw-r-- 1 dev dev 251405 1月 19 11:57 flink-cdc-common-3.0.1.jar-rw-r--r-- 1 dev dev 21333608 2月 22 16:19 flink-table-planner_2.12-1.17.2.jar Build and push to registry: 12docker build -t registry.zerofinance.net/flink/dinky-flink-application:1.17.2-1.0.0-rc4 . -f DinkyFlinkDockerfiledocker push registry.zerofinance.net/flink/dinky-flink-application:1.17.2-1.0.0-rc4 “提交FlinkSQL的jar文件路径”为打包到镜像registry.zerofinance.net/flink/dinky-flink-application:1.17.2-1.0.0-rc4中的路径，而不是dinky中的路径。 DataSource User-defined FunctionsUser-defined Functions | Apache Flink User-defined functions (UDFs) are extension points to call frequently used logic or custom logic that cannot be expressed otherwise in queries. User-defined functions can be implemented in a JVM language (such as Java or Scala) or Python. An implementer can use arbitrary third party libraries within a UDF. This page will focus on JVM-based languages, please refer to the PyFlink documentation for details on writing general and vectorized UDFs in Python. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8%E5%B9%B2%E8%B4%A7%EF%BC%81FlinkSQL%E6%88%90%E7%A5%9E%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%85%A8%E6%96%876%E4%B8%87%E5%AD%97%E3%80%81110%E4%B8%AA%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%81160%E5%BC%A0%E5%9B%BE%EF%BC%89/#https://www.cnblogs.com/wxm2270/p/17275442.html#https://juejin.cn/post/7103196993232568328#https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/udfs/#第一步，自定义数据类型public class User &#123; // 1. 基础类型，Flink 可以通过反射类型信息自动把数据类型获取到 // 关于 SQL 类型和 Java 类型之间的映射见：https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/types/#data-type-extraction public int age; public String name; // 2. 复杂类型，用户可以通过 @DataTypeHint(\"DECIMAL(10, 2)\") 注解标注此字段的数据类型 public @DataTypeHint(\"DECIMAL(10, 2)\") BigDecimal totalBalance;&#125;#第二步，在 UDF 中使用此数据类型public class UserScalarFunction extends ScalarFunction &#123; // 1. 自定义数据类型作为输出参数 public User eval(long i) &#123; if (i &gt; 0 &amp;&amp; i &lt;= 5) &#123; User u = new User(); u.age = (int) i; u.name = \"name1\"; u.totalBalance = new BigDecimal(1.1d); return u; &#125; else &#123; User u = new User(); u.age = (int) i; u.name = \"name2\"; u.totalBalance = new BigDecimal(2.2d); return u; &#125; &#125; // 2. 自定义数据类型作为输入参数 public String eval(User i) &#123; if (i.age &gt; 0 &amp;&amp; i.age &lt;= 5) &#123; User u = new User(); u.age = 1; u.name = \"name1\"; u.totalBalance = new BigDecimal(1.1d); return u.name; &#125; else &#123; User u = new User(); u.age = 2; u.name = \"name2\"; u.totalBalance = new BigDecimal(2.2d); return u.name; &#125; &#125;&#125;#Upload the packaged jar to /usr/bigtop/current/flink-client/lib/ of all machines and restart yarn-session instance.#第三步，在 Flink SQL 中使用-- 1. 创建 UDFCREATE FUNCTION user_scalar_func AS 'flink.examples.sql._12_data_type._02_user_defined.UserScalarFunction';-- 2. 创建数据源表CREATE TABLE source_table ( user_id BIGINT NOT NULL COMMENT '用户 id') WITH ( 'connector' = 'datagen', 'rows-per-second' = '1', 'fields.user_id.min' = '1', 'fields.user_id.max' = '10');-- 3. 创建数据汇表CREATE TABLE sink_table ( result_row_1 ROW&lt;age INT, name STRING, totalBalance DECIMAL(10, 2)&gt;, result_row_2 STRING) WITH ( 'connector' = 'print');-- 4. SQL 查询语句INSERT INTO sink_tableselect -- 4.a. 用户自定义类型作为输出 user_scalar_func(user_id) as result_row_1, -- 4.b. 用户自定义类型作为输出及输入 user_scalar_func(user_scalar_func(user_id)) as result_row_2from source_table;-- 5. 查询结果+I[+I[9, name2, 2.20], name2]+I[+I[1, name1, 1.10], name1]+I[+I[5, name1, 1.10], name1] Hive Catalog123456789101112131415161718192021222324252627282930313233#https://nightlies.apache.org/flink/flink-docs-release-1.17/zh/docs/connectors/table/hive/hive_catalog/CREATE CATALOG myhive WITH ( 'type' = 'hive', 'hive-conf-dir' = '/usr/bigtop/current/hive-client/conf');show catalogs;use catalog myhive;show databases;create table mykafka ( id STRING, use_rname STRING, age integer, gender STRING, goods_no STRING, goods_price Float, store_id integer, shopping_type STRING, tel STRING, email STRING, shopping_date Date) with ( 'connector' = 'kafka', 'properties.bootstrap.servers' = 'datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092', 'topic' = 'fludesc', 'scan.startup.mode' = 'earliest-offset', 'format' = 'csv', 'csv.ignore-parse-errors' = 'true');DESCRIBE mykafka;select * from mykafka; Flink Streaming Platform Webflink-streaming-platform-web Prerequisite: 1234567891011121314151617#https://github.com/zhp8341/flink-streaming-platform-web/blob/master/docs/deploy.md#https://www.cnblogs.com/data-magnifier/p/16943527.htmlsudo su - hadoopmkdir /usr/bigtop/3.2.0/usr/lib/cd /usr/bigtop/3.2.0/usr/lib/wget https://github.com/zhp8341/flink-streaming-platform-web/releases/download/tagV20230610(flink1.16.2)/flink-streaming-platform-web.tar.gztar zxf flink-streaming-platform-web.tar.gzcd /usr/bigtop/current/ln -s /usr/bigtop/3.2.0/usr/lib/flink-streaming-platform-web flink-streaming-platform-webcd /usr/bigtop/current/flink-streaming-platform-webwget https://github.com/zhp8341/flink-streaming-platform-web/blob/master/docs/sql/flink_web.sqlmysql -uroot -h127.0.0.1 -p&gt; source /usr/bigtop/current/flink-streaming-platform-web/flink_web.sql;&gt; exit; config/application.properties: 12345####jdbc信息server.port=9084spring.datasource.url=jdbc:mysql://192.168.63.102:3306/flink_web?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falsespring.datasource.username=flink_webspring.datasource.password=Aa123456 Build docker image: 123456789101112131415161718192021222324FROM centos:7RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeRUN echo &apos;Asia/Shanghai&apos; &gt;/etc/timezoneRUN yum -y install kde-l10n-Chinese &amp;&amp; yum -y reinstall glibc-common RUN localedef -c -f UTF-8 -i zh_CN zh_CN.utf8 ENV LC_ALL zh_CN.utf8RUN export LANG=zh_CN.UTF-8RUN yum install java-1.8.0-openjdk* -yRUN mkdir /data/RUN mkdir /data/projectsRUN mkdir /data/projects/flink-1.15.3WORKDIR /data/projects/ADD flink-streaming-platform-web.tar.gz /data/projects/#ADD flink-1.15.3-bin-scala_2.12.tgz /data/projects/COPY flink-1.15.3 /data/projects/flink-1.15.3ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;java -jar flink-streaming-platform-web/lib/flink-streaming-web-1.5.0.RELEASE.jar --spring.profiles.active=prod --spring.config.additional-location=flink-streaming-platform-web/conf/application.properties&quot;]EXPOSE 9084 5007 8081 build and push to registry: 12docker build -t registry.zerofinance.net/library/flink-streaming-platform-web:1.16.2 . -f Dockerfile.webdocker push registry.zerofinance.net/library/flink-streaming-platform-web:1.16.2 Starting a new instance: 12345678docker run -d --name flink-streaming-platform-web --restart=always \\-p 9084:9084 \\-v /works/app/flink/flink-streaming-platform-web/conf:/data/projects/flink-streaming-platform-web/conf \\registry.zerofinance.net/library/flink-streaming-platform-web:1.16.2http://192.168.64.102:32061#/data/projects/flink-1.15.3/bin/flink run -d -m 192.168.63.102:8081 -c com.flink.streaming.core.JobApplication /data/projects/flink-streaming-platform-web/lib/flink-streaming-core-1.5.0.RELEASE.jar -sql /data/projects/flink-streaming-platform-web/sql/job_sql_6.sql -type 0 On K8s: flink-streaming-platform-web.yml: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112apiVersion: v1kind: ConfigMapmetadata: name: application-propertiesdata: application.properties: | server.port=9084 spring.datasource.url=jdbc:mysql://192.168.63.102:3306/flink_fspw?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false spring.datasource.username=flink_fspw spring.datasource.password=Aa123456---apiVersion: apps/v1kind: Deploymentmetadata: name: flink-streaming-platform-web namespace: flink-test labels: app: flink-streaming-platform-webspec: replicas: 1 selector: matchLabels: app: flink-streaming-platform-web template: metadata: labels: app: flink-streaming-platform-web spec: imagePullSecrets: - name: registry-private-secret containers: - name: flink-streaming-platform-web image: registry.zerofinance.net/library/flink-streaming-platform-web:1.16.2 imagePullPolicy: Always ports: - name: fspw-9084 containerPort: 9084 protocol: TCP - name: fspw-5007 containerPort: 5007 protocol: TCP - name: fspw-8081 containerPort: 8081 protocol: TCP resources: limits: memory: &quot;1024Mi&quot; cpu: &quot;1000m&quot; requests: cpu: 500m memory: 500Mi livenessProbe: failureThreshold: 3 httpGet: path: /static/ui/index.html port: 9084 initialDelaySeconds: 30 periodSeconds: 10 #successThreshold: 1 timeoutSeconds: 5 readinessProbe: failureThreshold: 3 httpGet: path: /static/ui/index.html port: 9084 initialDelaySeconds: 30 periodSeconds: 10 #successThreshold: 1 timeoutSeconds: 5 volumeMounts: - name: application-properties-volume mountPath: /data/projects/flink-streaming-platform-web/conf/ volumes: - name: application-properties-volume configMap: name: application-properties items: - key: application.properties path: application.properties---apiVersion: v1kind: Servicemetadata: name: flink-streaming-platform-web namespace: flink-test labels: app: flink-streaming-platform-webspec: #type: NodePort ports: - name: kafka port: 9084 targetPort: 9084 #nodePort: 30900 selector: app: flink-streaming-platform-web---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: flink-streaming-platform-web namespace: flink-testspec: tls: [] rules: - host: fspw-test.zerofinance.net http: paths: - backend: serviceName: flink-streaming-platform-web servicePort: 9084 Or: 123456789101112vim conf/application.properties####jdbc信息server.port=9084spring.datasource.url=jdbc:mysql://192.168.80.225:3306/flink_web?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falsespring.datasource.username=rootspring.datasource.password=xxxxxxcd bin./deploy.sh start#http://192.168.80.226:9084/admin/123456 Settings Job Flink SQL CDC基于 Flink SQL CDC的实时数据同步方案 (dreamwu.com) docs/sql_demo/demo_6.md · 朱慧培/flink-streaming-platform-web - Gitee.com Overview — CDC Connectors for Apache Flink® documentation (ververica.github.io) Enable mysql bin-log function: 1234567891011121314151617181920212223242526272829#temporary password：grep &apos;temporary password&apos; /var/log/mysqld.logmysql -uroot -pset global validate_password_policy=0;alter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;Aa123456&apos;;CREATE USER &apos;flink_web&apos;@&apos;%&apos; IDENTIFIED BY &apos;Aa123456&apos;;GRANT ALL PRIVILEGES ON flink_web.* TO &apos;flink_web&apos;@&apos;%&apos;;CREATE USER &apos;demo_db&apos;@&apos;%&apos; IDENTIFIED BY &apos;Aa123456&apos;;GRANT ALL PRIVILEGES ON demo_db.* TO &apos;demo_db&apos;@&apos;%&apos;;GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO demo_db@&apos;%&apos;;FLUSH PRIVILEGES;mysql-cdc:#https://support.huaweicloud.com/trouble-rds/rds_12_0040.htmlAccess denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operationGRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO demo_db@&apos;%&apos;;FLUSH PRIVILEGES;#https://blog.csdn.net/wochunyang/article/details/132210928?spm=1001.2014.3001.5501Cannot read the binlog filename and position via &apos;SHOW MASTER STATUS&apos;. Make sure your server is correctly configuredvim /etc/my.cnfserver_id = 1binlog_format = ROWlog-bin = mysql_log_binsystemctl restart mysqld MySQL ON Docker 123456789101112131415161718192021222324252627282930313233343536373839404142ES:docker run -d --name elastic-dev --restart always \\--log-driver json-file --log-opt max-size=200m --log-opt max-file=3 \\--net es-network -p 9200:9200 -p 9300:9300 \\-v /data/esdata-dev:/usr/share/elasticsearch/data \\-e &quot;discovery.type=single-node&quot; --ulimit nofile=65535:65535 registry.zerofinance.net/library/elasticsearch:7.6.2MySQL:cat /works/app/mysql/my.cnf[mysqld]bind-address=0.0.0.0port=3306#socket=/Developer/mysql-5.7.37/data/mysql.sock#pid-file=/Developer/mysql-5.7.37/logs/mysql.pid#basedir=/Developer/mysql-5.7.37#datadir=/Developer/mysql-5.7.37/datadefault-time_zone=&apos;+8:00&apos;max_connections=2000character-set-server=utf8collation-server=utf8_general_cilower_case_table_names=1server_id = 1binlog_format = ROWlog-bin = mysql_log_bindocker run -d -p 3306:3306 --restart=always --name mysql \\-e TZ=Asia/Shanghai \\-e MYSQL_ROOT_PASSWORD=Aa123456 \\-e MYSQL_DATABASE=demo_db \\-e MYSQL_USER=demo_db \\-e MYSQL_PASSWORD=Aa123456 \\-v /works/app/mysql/my.cnf:/etc/my.cnf \\mysql:5.7.32#https://support.huaweicloud.com/trouble-rds/rds_12_0040.htmlAccess denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operationGRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO demo_db@&apos;%&apos;;FLUSH PRIVILEGES; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113&gt; sudo su - hadoop&gt; mysql -uroot -h127.0.0.1 -p-- MySQLCREATE DATABASE mydb;USE mydb;CREATE TABLE products ( id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(512));ALTER TABLE products AUTO_INCREMENT = 101;INSERT INTO productsVALUES (default,\"scooter\",\"Small 2-wheel scooter\"), (default,\"car battery\",\"12V car battery\"), (default,\"12-pack drill bits\",\"12-pack of drill bits with sizes ranging from #40 to #3\"), (default,\"hammer\",\"12oz carpenter's hammer\"), (default,\"hammer\",\"14oz carpenter's hammer\"), (default,\"hammer\",\"16oz carpenter's hammer\"), (default,\"rocks\",\"box of assorted rocks\"), (default,\"jacket\",\"water resistent black wind breaker\"), (default,\"spare tire\",\"24 inch spare tire\");CREATE TABLE orders ( order_id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY, order_date DATETIME NOT NULL, customer_name VARCHAR(255) NOT NULL, price DECIMAL(10, 5) NOT NULL, product_id INTEGER NOT NULL, order_status BOOLEAN NOT NULL -- Whether order has been placed) AUTO_INCREMENT = 10001;INSERT INTO ordersVALUES (default, '2020-07-30 10:08:22', 'Jark', 50.50, 102, false), (default, '2020-07-30 10:11:09', 'Sally', 15.00, 105, false), (default, '2020-07-30 12:00:30', 'Edward', 25.25, 106, false); &gt; yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d&gt; sql-client.sh embedded -s yarn-sessionFlink SQL&gt; SET sql-client.execution.result-mode = tableau;-- checkpoint every 3000 milliseconds Flink SQL&gt; SET 'execution.checkpointing.interval' = '3s'; #Create in flinksql-- Flink SQL#Mysql sourceFlink SQL&gt; CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( 'connector' = 'mysql-cdc', 'server-time-zone' = 'Asia/Shanghai', 'hostname' = '192.168.80.225', 'port' = '3306', 'username' = 'root', 'password' = 'Aa123#@!', 'database-name' = 'mydb', 'table-name' = 'products' );Flink SQL&gt; CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( 'connector' = 'mysql-cdc', 'server-time-zone' = 'Asia/Shanghai', 'hostname' = '192.168.80.225', 'port' = '3306', 'username' = 'root', 'password' = 'Aa123#@!', 'database-name' = 'mydb', 'table-name' = 'orders' );#Kafka sinkCREATE TABLE enriched_orders( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, product_name STRING, product_description STRING, PRIMARY KEY (order_id) NOT ENFORCED) WITH ( 'connector' = 'upsert-kafka', 'topic' = 'fludesc', 'properties.bootstrap.servers' = 'datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092', 'key.format' = 'csv', 'value.format' = 'csv');#SinkINSERT INTO enriched_orders SELECT o.*, p.name, p.description FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id;#Monitoring the changed data streamskafka-console-consumer.sh --topic fludesc --bootstrap-server datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092 --from-beginning The connector named kafka doesn’t support flink-sql-cdc, using ‘upset-kafka’ instead. The error as blow: Demokafka to mysql DemoThis demo illustrate how to sink data from Kafka to MySQL: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#https://www.jianshu.com/p/266449b9a0f4mysql -uroot -p#Create table in mysqlcreate database demo_db character set utf8mb4;use demo_db;create table fludesc ( id varchar(32), use_rname varchar(32), age int, gender varchar(32), goods_no varchar(32), goods_price Float, store_id int, shopping_type varchar(32), tel varchar(32), email varchar(32), shopping_date date);&gt; sudo su - hadoop&gt; yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d&gt; sql-client.sh embedded -s yarn-session&gt; SET sql-client.execution.result-mode = tableau;#Create in flinksqlFlink SQL&gt; create table kafka_source ( id STRING, use_rname STRING, age integer, gender STRING, goods_no STRING, goods_price Float, store_id integer, shopping_type STRING, tel STRING, email STRING, shopping_date Date) with ( 'connector' = 'kafka', 'properties.bootstrap.servers' = 'datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092', 'topic' = 'fludesc', 'properties.group.id' = 'testGroup', 'scan.startup.mode' = 'earliest-offset', 'format' = 'csv', 'csv.ignore-parse-errors' = 'true');Flink SQL&gt; CREATE TABLE mysql_sink ( id STRING, use_rname STRING, age integer, gender STRING, goods_no STRING, goods_price Float, store_id integer, shopping_type STRING, tel STRING, email STRING, shopping_date Date) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://192.168.80.225:3306/demo_db', 'table-name' = 'fludesc', 'username' = 'root', 'password' = 'Aa123#@!');Flink SQL&gt; insert into mysql_sink select * from kafka_source;#Mock data from kafka:kafka-console-producer.sh --broker-list datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092 --topic fludesc&gt;511653962048,Zomfq,53,woman,532120,534.61,313020,cart,15926130785,UyxghCpKMD@huawei.com,2019-08-03&gt;751653962048,Qvtil,27,man,532120,655.7,313023,cart,13257423096,cJfbNhRYow@163.com,2019-08-05&gt;121653962048,Spdwh,35,woman,480071,97.35,313018,cart,18825789463,LkVYmpcWXC@qq.com,2019-08-05&gt;871653962048,Fdhpc,18,man,650012,439.40,313012,cart,15059872140,sfzuPWvNEe@qq.com,2019-08-06&gt;841653962048,Iqoyh,51,woman,152121,705.6,313012,buy,13646513897,jISbcYdxZO@126.com,2019-08-04&gt;761653962048,Xgzhy,29,woman,480071,329.60,313013,cart,15069315824,NtTDRlAdeZ@qq.com,2019-08-04#kafka-console-consumer.sh --topic fludesc --bootstrap-server datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092 --from-beginning kafka to hdfs Demo1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&gt; sudo su - hadoop&gt; yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d&gt; sql-client.sh embedded -s yarn-session&gt; SET sql-client.execution.result-mode = tableau;#Create in flinksqlFlink SQL&gt; create table kafka_source ( id STRING, use_rname STRING, age integer, gender STRING, goods_no STRING, goods_price Float, store_id integer, shopping_type STRING, tel STRING, email STRING, shopping_date Date) with ( 'connector' = 'kafka', 'properties.bootstrap.servers' = 'datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092,datanode01-test.zerofinance.net:9092', 'topic' = 'fludesc', 'properties.group.id' = 'testGroup', 'scan.startup.mode' = 'earliest-offset', 'format' = 'csv', 'csv.ignore-parse-errors' = 'true');CREATE TABLE hadoop_sink ( id STRING, use_rname STRING, age integer, gender STRING, goods_no STRING, goods_price Float, store_id integer, shopping_type STRING, tel STRING, email STRING, shopping_date Date) PARTITIONED BY (id) WITH ( 'connector' = 'filesystem', 'path' = 'hdfs:///works/test/hadoop_sink', 'format' = 'csv', 'partition.default-name' = '9999', 'sink.shuffle-by-partition.enable' = 'false');insert into hadoop_sink select * from kafka_source; Mysql to hdfs Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&gt; sudo su - hadoop&gt; yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d&gt; sql-client.sh embedded -s yarn-session&gt; SET sql-client.execution.result-mode = tableau;#Create in flinksqlFlink SQL&gt; CREATE TABLE mysql_source ( id STRING, use_rname STRING, age integer, gender STRING, goods_no STRING, goods_price Float, store_id integer, shopping_type STRING, tel STRING, email STRING, shopping_date Date) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://192.168.80.225:3306/demo_db', 'table-name' = 'fludesc', 'username' = 'root', 'password' = 'Aa123#@!');CREATE TABLE hadoop_sink ( id STRING, use_rname STRING, age integer, gender STRING, goods_no STRING, goods_price Float, store_id integer, shopping_type STRING, tel STRING, email STRING, shopping_date Date) PARTITIONED BY (id) WITH ( 'connector' = 'filesystem', 'path' = 'hdfs:///works/test/hadoop_sink', 'format' = 'csv', 'partition.default-name' = '9999', 'sink.shuffle-by-partition.enable' = 'false');insert into hadoop_sink select * from mysql_source; Mysql to ES DemoONE TO ONE: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&gt; sudo su - hadoop&gt; yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d&gt; sql-client.sh embedded -s yarn-session&gt; SET sql-client.execution.result-mode = tableau;#Create in flinksqlCREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( 'connector' = 'mysql-cdc', 'server-time-zone' = 'Asia/Shanghai', 'hostname' = '192.168.63.102', 'port' = '3306', 'username' = 'demo_db', 'password' = 'Aa123456', 'database-name' = 'demo_db', 'table-name' = 'products' ); CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( 'connector' = 'mysql-cdc', 'server-time-zone' = 'Asia/Shanghai', 'hostname' = '192.168.63.102', 'port' = '3306', 'username' = 'demo_db', 'password' = 'Aa123456', 'database-name' = 'demo_db', 'table-name' = 'orders' );CREATE TABLE enriched_orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, product_name STRING, product_description STRING, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( 'connector' = 'elasticsearch-7', 'hosts' = 'http://192.168.63.102:9200', 'index' = 'enriched_orders_1' );INSERT INTO enriched_orders SELECT o.*, p.name, p.description FROM orders AS o LEFT JOIN products AS p ON o.product_id = p.id; ONE TO MANY UDF: #https://www.decodable.co/blog/array-aggregation-with-flink-sql-data-streaming #https://github.com/decodableco/examples/blob/main/flink-learn/3-array-agg/src/main/java/co/decodable/demos/arrayagg/ArrayAggr.java ArrayAccumulator: 1234567891011121314151617181920212223242526272829303132package com.zerofinance.function;import java.util.Objects;import org.apache.flink.table.api.dataview.ListView;/** * https://github.com/decodableco/examples/blob/main/flink-learn/3-array-agg/src/main/java/co/decodable/demos/arrayagg/ArrayAccumulator.java * * @param &lt;T&gt; */public class ArrayAccumulator&lt;T&gt; &#123; public ListView&lt;T&gt; values = new ListView&lt;T&gt;(); @Override public int hashCode() &#123; return Objects.hash(values); &#125; @Override public boolean equals(Object obj) &#123; if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; ArrayAccumulator&lt;?&gt; other = (ArrayAccumulator&lt;?&gt;) obj; return Objects.equals(values, other.values); &#125;&#125; ArrayAggr: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.zerofinance.function;import java.lang.reflect.Array;import java.util.ArrayList;import java.util.List;import java.util.Optional;import org.apache.flink.table.api.DataTypes;import org.apache.flink.table.api.dataview.ListView;import org.apache.flink.table.catalog.DataTypeFactory;import org.apache.flink.table.functions.AggregateFunction;import org.apache.flink.table.types.DataType;import org.apache.flink.table.types.inference.InputTypeStrategies;import org.apache.flink.table.types.inference.TypeInference;/** * https://github.com/decodableco/examples/blob/main/flink-learn/3-array-agg/src/main/java/co/decodable/demos/arrayagg/ArrayAggr.java * * @param &lt;T&gt; */public class ArrayAggr &lt;T&gt; extends AggregateFunction&lt;T[], ArrayAccumulator&lt;T&gt;&gt; &#123; private static final long serialVersionUID = 6560271654419701770L; private DataType elementType; @Override public ArrayAccumulator&lt;T&gt; createAccumulator() &#123; return new ArrayAccumulator&lt;T&gt;(); &#125; @SuppressWarnings(\"unchecked\") @Override public T[] getValue(ArrayAccumulator&lt;T&gt; acc) &#123; if (acc.values.getList().isEmpty()) &#123; return null; &#125; else &#123; List&lt;T&gt; values = new ArrayList&lt;T&gt;(acc.values.getList()); return values.toArray((T[]) Array.newInstance(elementType.getConversionClass(), values.size())); &#125; &#125; public void accumulate(ArrayAccumulator&lt;T&gt; acc, T o) throws Exception &#123; if (o != null) &#123; acc.values.add(o); &#125; &#125; public void retract(ArrayAccumulator&lt;T&gt; acc, T o) throws Exception &#123; if (o != null) &#123; acc.values.remove(o); &#125; &#125; public void resetAccumulator(ArrayAccumulator&lt;T&gt; acc) &#123; acc.values.clear(); &#125; @Override public TypeInference getTypeInference(DataTypeFactory typeFactory) &#123; return TypeInference.newBuilder() .inputTypeStrategy(InputTypeStrategies.sequence(InputTypeStrategies.ANY)) .accumulatorTypeStrategy(ctx -&gt; &#123; return Optional.of( DataTypes.STRUCTURED( ArrayAccumulator.class, DataTypes.FIELD(\"values\",ListView.newListViewDataType(ctx.getArgumentDataTypes().get(0)))//, )); &#125;) .outputTypeStrategy(ctx -&gt; &#123; this.elementType = ctx.getArgumentDataTypes().get(0); return Optional.of(DataTypes.ARRAY(elementType)); &#125;).build(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455CREATE FUNCTION ARRAY_AGGR AS 'com.zerofinance.function.ArrayAggr';CREATE TABLE products ( id INT, name STRING, description STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( 'connector' = 'mysql-cdc', 'server-time-zone' = 'Asia/Shanghai', 'hostname' = '192.168.63.102', 'port' = '3306', 'username' = 'demo_db', 'password' = 'Aa123456', 'database-name' = 'demo_db', 'table-name' = 'products' ); CREATE TABLE orders ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( 'connector' = 'mysql-cdc', 'server-time-zone' = 'Asia/Shanghai', 'hostname' = '192.168.63.102', 'port' = '3306', 'username' = 'demo_db', 'password' = 'Aa123456', 'database-name' = 'demo_db', 'table-name' = 'orders' ); CREATE TABLE enriched_orders ( product_id INT, product_name STRING, product_description STRING, lines ARRAY&lt;ROW&lt;order_id INT,order_date TIMESTAMP(0),customer_name STRING,price DECIMAL(10, 5),order_status BOOLEAN&gt;&gt;, PRIMARY KEY (product_id) NOT ENFORCED ) WITH ( 'connector' = 'elasticsearch-7', 'hosts' = 'http://192.168.63.102:9200', 'index' = 'enriched_orders_0' ); INSERT INTO enriched_orders SELECT p.id AS product_id, p.name AS product_name, p.description AS product_description, (select ARRAY_AGGR(ROW(order_id,order_date,customer_name,price,order_status)) from orders o where o.product_id=p.id) as lines FROM products AS p; Another way is put sub data to a single string filed: 123456789101112131415161718CREATE TABLE enriched_orders ( product_id INT, product_name STRING, product_description STRING, lines STRING, PRIMARY KEY (product_id) NOT ENFORCED ) WITH ( 'connector' = 'elasticsearch-7', 'hosts' = 'http://192.168.63.102:9200', 'index' = 'enriched_orders_0' ); INSERT INTO enriched_orders SELECT p.id AS product_id, p.name AS product_name, p.description AS product_description, (select JSON_ARRAYAGG( JSON_OBJECT('order_id' VALUE o.order_id,'order_date' VALUE o.order_date,'customer_name' VALUE o.customer_name,'price' VALUE o.price,'order_status' VALUE o.order_status)) from orders o where o.product_id=p.id) as lines FROM products AS p; Window AggregationTUMBLEWindowing TVFWindowing TVF | Apache Flink 1TUMBLE(TABLE data, DESCRIPTOR(timecol), size [, offset ]) data: is a table parameter that can be any relation with a time attribute column. timecol: is a column descriptor indicating which time attributes column of data should be mapped to tumbling windows. size: is a duration specifying the width of the tumbling windows. offset: is an optional parameter to specify the offset which window start would be shifted by. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#简单且常见的分维度分钟级别同时在线用户数、总销售额&gt; sudo su - hadoop&gt; yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d&gt; sql-client.sh embedded -s yarn-session&gt; SET sql-client.execution.result-mode = tableau;-- 数据源表CREATE TABLE source_table ( -- 维度数据 dim STRING, -- 用户 id user_id BIGINT, -- 用户 price BIGINT, -- 事件时间戳 row_time AS cast(CURRENT_TIMESTAMP as timestamp(3)), -- watermark 设置 WATERMARK FOR row_time AS row_time - INTERVAL '5' SECOND) WITH ( 'connector' = 'datagen', 'rows-per-second' = '10', 'fields.dim.length' = '1', 'fields.user_id.min' = '1', 'fields.user_id.max' = '100000', 'fields.price.min' = '1', 'fields.price.max' = '100000');-- 数据汇表CREATE TABLE sink_table ( dim STRING, pv BIGINT, sum_price BIGINT, max_price BIGINT, min_price BIGINT, uv BIGINT, window_start bigint) WITH ( 'connector' = 'print');-- 数据处理逻辑insert into sink_tableSELECT dim, UNIX_TIMESTAMP(CAST(window_start AS STRING)) * 1000 as window_start, count(*) as pv, sum(price) as sum_price, max(price) as max_price, min(price) as min_price, count(distinct user_id) as uvFROM TABLE(TUMBLE( TABLE source_table , DESCRIPTOR(row_time) , INTERVAL '60' SECOND))GROUP BY window_start, window_end, dim; Group Window AggregationDeprecated: Group Window Aggregation, supported both batch and streaming. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&gt; sudo su - hadoop&gt; yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d&gt; sql-client.sh embedded -s yarn-session&gt; SET sql-client.execution.result-mode = tableau;-- 数据源表CREATE TABLE source_table ( -- 维度数据 dim STRING, -- 用户 id user_id BIGINT, -- 用户 price BIGINT, -- 事件时间戳 row_time AS cast(CURRENT_TIMESTAMP as timestamp(3)), -- watermark 设置 WATERMARK FOR row_time AS row_time - INTERVAL '5' SECOND) WITH ( 'connector' = 'datagen', 'rows-per-second' = '10', 'fields.dim.length' = '1', 'fields.user_id.min' = '1', 'fields.user_id.max' = '100000', 'fields.price.min' = '1', 'fields.price.max' = '100000');-- 数据汇表CREATE TABLE sink_table ( dim STRING, pv BIGINT, sum_price BIGINT, max_price BIGINT, min_price BIGINT, uv BIGINT, window_start bigint) WITH ( 'connector' = 'print');-- 数据处理逻辑insert into sink_tableselect dim, count(*) as pv, sum(price) as sum_price, max(price) as max_price, min(price) as min_price, -- 计算 uv 数 count(distinct user_id) as uv, UNIX_TIMESTAMP(CAST(tumble_start(row_time, interval '1' minute) AS STRING)) * 1000 as window_startfrom source_tablegroup by dim, tumble(row_time, interval '1' minute); HOPWindowing TVFWindowing TVF | Apache Flink 1HOP(TABLE data, DESCRIPTOR(timecol), slide, size [, offset ]) data: is a table parameter that can be any relation with an time attribute column. timecol: is a column descriptor indicating which time attributes column of data should be mapped to hopping windows. slide: is a duration specifying the duration between the start of sequential hopping windows size: is a duration specifying the width of the hopping windows. offset: is an optional parameter to specify the offset which window start would be shifted by. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#简单且常见的分维度分钟级别同时在线用户数，1 分钟输出一次，计算最近 5 分钟的数据&gt; sudo su - hadoop&gt; yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d&gt; sql-client.sh embedded -s yarn-session&gt; SET sql-client.execution.result-mode = tableau;-- 数据源表CREATE TABLE source_table ( -- 维度数据 dim STRING, -- 用户 id user_id BIGINT, -- 用户 price BIGINT, -- 事件时间戳 row_time AS cast(CURRENT_TIMESTAMP as timestamp(3)), -- watermark 设置 WATERMARK FOR row_time AS row_time - INTERVAL '5' SECOND) WITH ( 'connector' = 'datagen', 'rows-per-second' = '10', 'fields.dim.length' = '1', 'fields.user_id.min' = '1', 'fields.user_id.max' = '100000', 'fields.price.min' = '1', 'fields.price.max' = '100000');-- 数据汇表CREATE TABLE sink_table ( dim STRING, uv BIGINT, window_start bigint) WITH ( 'connector' = 'print');-- 数据处理逻辑insert into sink_tableSELECT dim, UNIX_TIMESTAMP(CAST(hop_start(row_time, interval '1' minute, interval '5' minute) AS STRING)) * 1000 as window_start, count(distinct user_id) as uvFROM source_tableGROUP BY dim , hop(row_time, interval '1' minute, interval '5' minute); Group Window AggregationDeprecated. SessionWindowing TVFTVF doesn’t support Session mode, using group window aggregation instread. Group Window Aggregation Group Window Function Description SESSION(time_attr, interval) Defines a session time window. Session time windows do not have a fixed duration but their bounds are defined by a time interval of inactivity, i.e., a session window is closed if no event appears for a defined gap period. For example a session window with a 30 minute gap starts when a row is observed after 30 minutes inactivity (otherwise the row would be added to an existing window) and is closed if no row is added within 30 minutes. Session windows can work on event-time (stream + batch) or processing-time (stream). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#Session 时间窗口和滚动、滑动窗口不一样，其没有固定的持续时间，如果在定义的间隔期（Session Gap）内没有新的数据出现，则 Session 就会窗口关闭#计算每个用户在活跃期间（一个 Session）总共购买的商品数量，如果用户 5 分钟没有活动则视为 Session 断开#Group Window Aggregation &gt; sudo su - hadoop&gt; yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d&gt; sql-client.sh embedded -s yarn-session&gt; SET sql-client.execution.result-mode = tableau;-- 数据源表，用户购买行为记录表CREATE TABLE source_table ( -- 维度数据 dim STRING, -- 用户 id user_id BIGINT, -- 用户 price BIGINT, -- 事件时间戳 row_time AS cast(CURRENT_TIMESTAMP as timestamp(3)), -- watermark 设置 WATERMARK FOR row_time AS row_time - INTERVAL '5' SECOND) WITH ( 'connector' = 'datagen', 'rows-per-second' = '10', 'fields.dim.length' = '1', 'fields.user_id.min' = '1', 'fields.user_id.max' = '100000', 'fields.price.min' = '1', 'fields.price.max' = '100000');-- 数据汇表CREATE TABLE sink_table ( dim STRING, pv BIGINT, -- 购买商品数量 window_start bigint) WITH ( 'connector' = 'print');-- 数据处理逻辑insert into sink_tableSELECT dim, UNIX_TIMESTAMP(CAST(session_start(row_time, interval '5' minute) AS STRING)) * 1000 as window_start, count(1) as pvFROM source_tableGROUP BY dim , session(row_time, interval '5' minute); #上述 SQL 任务是在整个 Session 窗口结束之后才会把数据输出。Session 窗口即支持 处理时间 也支持 事件时间。但是处理时间只支持在 Streaming 任务中运行，Batch 任务不支持。 CUMULATEWindowing TVF1CUMULATE(TABLE data, DESCRIPTOR(timecol), step, size) data: is a table parameter that can be any relation with an time attribute column. timecol: is a column descriptor indicating which time attributes column of data should be mapped to cumulating windows. step: is a duration specifying the increased window size between the end of sequential cumulating windows. size: is a duration specifying the max width of the cumulating windows. size must be an integral multiple of step. offset: is an optional parameter to specify the offset which window start would be shifted by. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#每天的截止当前分钟的累计 money（sum(money)），去重 id 数（count(distinct id)）。每天代表渐进式窗口大小为 1 天，分钟代表渐进式窗口移动步长为分钟级别&gt; sudo su - hadoop&gt; yarn-session.sh -jm 2048MB -tm 2048MB -nm flink-sql-test -d&gt; sql-client.sh embedded -s yarn-session&gt; SET sql-client.execution.result-mode = tableau;-- 数据源表CREATE TABLE source_table ( -- 用户 id id BIGINT, -- 用户 money BIGINT, -- 事件时间戳 row_time AS cast(CURRENT_TIMESTAMP as timestamp(3)), -- watermark 设置 WATERMARK FOR row_time AS row_time - INTERVAL '5' SECOND) WITH ( 'connector' = 'datagen', 'rows-per-second' = '10', 'fields.user_id.min' = '1', 'fields.user_id.max' = '100000', 'fields.price.min' = '1', 'fields.price.max' = '100000');-- 数据汇表CREATE TABLE sink_table ( window_end bigint, window_start bigint, sum_money BIGINT, count_distinct_id bigint) WITH ( 'connector' = 'print');-- 数据处理逻辑insert into sink_tableSELECT UNIX_TIMESTAMP(CAST(window_end AS STRING)) * 1000 as window_end, window_start, sum(money) as sum_money, count(distinct id) as count_distinct_idFROM TABLE(CUMULATE( TABLE source_table , DESCRIPTOR(row_time) , INTERVAL '60' SECOND , INTERVAL '1' DAY))GROUP BY window_start, window_end; #You will get wrong with: [ERROR] Could not execute SQL statement. Reason:org.apache.flink.table.api.ValidationException: Unsupported options found for 'datagen'. Group Window AggregationDeprecated. Troubleshooting#https://www.cnblogs.com/yeyuzhuanjia/p/17942445 Web UI cannot be visited by external: vim conf/flink-conf.yaml: 12rest.address: 0.0.0.0rest.bind-address: 0.0.0.0 High-AvailabilityRecommend working on Yarn High-Availability on YARN High-Availability on YARN is achieved through a combination of YARN and a high availability service. Once a HA service is configured, it will persist JobManager metadata and perform leader elections. YARN is taking care of restarting failed JobManagers. The maximum number of JobManager restarts is defined through two configuration parameters. First Flink’s yarn.application-attempts configuration will default 2. This value is limited by YARN’s yarn.resourcemanager.am.max-attempts, which also defaults to 2. Note that Flink is managing the high-availability.cluster-id configuration parameter when deploying on YARN. Flink sets it per default to the YARN application id. You should not overwrite this parameter when deploying an HA cluster on YARN. The cluster ID is used to distinguish multiple HA clusters in the HA backend (for example Zookeeper). Overwriting this configuration parameter can lead to multiple YARN clusters affecting each other. ZooKeeper HA Services Configure high availability mode and ZooKeeper quorum in conf/flink-conf.yaml: 1234high-availability: zookeeperhigh-availability.zookeeper.quorum: datanode03-test.zerofinance.net:2181,datanode01-test.zerofinance.net:2181,datanode02-test.zerofinance.net:2181high-availability.zookeeper.path.root: /flinkhigh-availability.storageDir: hdfs:///flink/ha/ Histroy ServerHistory Server | Apache Flink Flink has a history server that can be used to query the statistics of completed jobs after the corresponding Flink cluster has been shut down. By default, this server binds to localhost and listens at port 8082. Troubleshooting#https://www.jianshu.com/p/877868b6f829 NoResourceAvailableException: Could not acquire the minimum required resources 12taskmanager.memory.process.size: 6048mjobmanager.memory.process.size: 6048m Seatunnel12345678910cat /etc/profile.d/hadoop.sh export HADOOP_HOME=/usr/bigtop/current/hadoop-clientexport HADOOP_CONF_DIR=/usr/bigtop/current/hadoop-client/etc/hadoop/export SPARK_HOME=/usr/bigtop/current/spark-clientexport PYTHON_HOME=/usrexport HIVE_HOME=/usr/bigtop/current/hive-clientexport FLINK_HOME=/usr/bigtop/current/flink-clientexport SEATUNNEL_HOME=/works/app/apache-seatunnel-2.3.3export ZOOKEEPER_HOME=/usr/bigtop/current/zookeeper-clientexport PATH=$HADOOP_HOME/bin:$SPARK_HOME/bin:$HIVE_HOME/bin:$FLINK_HOME/bin:$SEATUNNEL_HOME/bin:$ZOOKEEPER_HOME/bin:$PATH","categories":[{"name":"Bigdata","slug":"Bigdata","permalink":"http://blog.gcalls.cn/categories/Bigdata/"}],"tags":[{"name":"Bigdata","slug":"Bigdata","permalink":"http://blog.gcalls.cn/tags/Bigdata/"}]},{"title":"Linux Shell Script","slug":"Linux-Shell-Script","date":"2022-08-03T02:31:44.000Z","updated":"2024-08-02T05:39:00.967Z","comments":true,"path":"/2022/08/Linux-Shell-Script.html","link":"","permalink":"http://blog.gcalls.cn/2022/08/Linux-Shell-Script.html","excerpt":"记录Linux脚本的常用技巧。","text":"记录Linux脚本的常用技巧。 cat12345678910111213141516171819202122232425262728293031#输出多行到屏幕：cat &lt;&lt; USAGE &gt;&amp;2Usage: $WAITFORIT_cmdname host:port [-s] [-t timeout] [-- command args] -h HOST | --host=HOST Host or IP under test -p PORT | --port=PORT TCP port under test Alternatively, you specify the host and port as host:port -s | --strict Only execute subcommand if the test succeeds -q | --quiet Don't output any status messages -t TIMEOUT | --timeout=TIMEOUT Timeout in seconds, zero for no timeout -- COMMAND ARGS Execute command with args after the test finishesUSAGEcat &lt;&lt; EOF &gt;&amp;2This is a test1This is a test1EOF#输出多行到文件：#追加：cat &gt;&gt; /tmp/tmp.log &lt;&lt; EOFnet.ipv4.tcp_syncookies = 1EOF#不追加：cat &gt; /tmp/tmp.log &lt;&lt; EOFnet.ipv4.tcp_syncookies = 1EOF#同时输出日志与文件./ping_check.sh |&amp; tee -a ping_full.log SEDhttps://coolshell.cn/articles/9104.html 插入换行1sed $'s;AAA;\\\\\\nAAA;g' my.txt 多个匹配123sed '1,3s/my/your/g; 3,$s/This/That/g' my.txtsed -e '1,3s/my/your/g' -e '3,$s/This/That/g' my.txt 我们可以使用&amp;来当做被匹配的变量，然后可以在基本左右加点东西： 1234567891011cat my.txtThis is my cat, my cat's name is bettyThis is my dog, my dog's name is frankThis is my fish, my fish's name is georgeThis is my goat, my goat's name is adam$ sed 's/my/[&amp;]/g' my.txtThis is [my] cat, [my] cat's name is bettyThis is [my] dog, [my] dog's name is frankThis is [my] fish, [my] fish's name is georgeThis is [my] goat, [my] goat's name is adam 圆括号匹配12345678910111213141516cat my.txtThis is my cat, my cat's name is bettyThis is my dog, my dog's name is frankThis is my fish, my fish's name is georgeThis is my goat, my goat's name is adamsed 's/This is my \\([^,&amp;]*\\),.*is \\(.*\\)/\\1:\\2/g' my.txtcat:bettydog:frankfish:georgegoat:adam正则为：This is my ([^,]*),.*is (.*)匹配为：This is my (cat),……….is (betty)然后：\\1就是cat，\\2就是betty 也就是：&amp;匹配所有的内容；\\1匹配到的第一个 \\ ( \\)中的内容；\\2匹配到的第二个\\ ( \\)内容。 参考 https://www.yiibai.com/sed/sed_regular_expressions.html https://coolshell.cn/articles/9104.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"Seata","slug":"Seata","date":"2022-06-08T05:40:28.000Z","updated":"2024-08-02T05:39:00.967Z","comments":true,"path":"/2022/06/Seata.html","link":"","permalink":"http://blog.gcalls.cn/2022/06/Seata.html","excerpt":"Seata is an open source distributed transaction solution that delivers high performance and easy to use distributed transaction services under a microservices architecture.","text":"Seata is an open source distributed transaction solution that delivers high performance and easy to use distributed transaction services under a microservices architecture. Installationmysql: 1234567docker run -d -p 3306:3306 --restart=always --name mysql \\-e MYSQL_ROOT_PASSWORD=Aa654321 \\-e MYSQL_DATABASE=seata-server \\-e MYSQL_USER=seata \\-e MYSQL_PASSWORD=Aa123456 \\mysql:5.7.24 \\--character-set-server=utf8 --collation-server=utf8_general_ci --lower_case_table_names=1 https://seata.io/zh-cn/docs/ops/deploy-by-docker.html #Copying out configs from docker container:123docker run -d -p 8091:8091 -p 7091:7091 --name seata-server seataio/seata-server:latestdocker cp seata-serve:/seata-server/resources ./configdocker rm -vf seata-server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#Nacos:#https://github.com/nacos-group/nacos-docker/tree/master/exampledocker run -d --name nacos-server -p 8848:8848 -p 9848:9848 -e MODE=standalone nacos/nacos-server:v2.1.0#Cluster#Create Dataabse:https://github.com/alibaba/nacos/blob/master/distribution/conf/nacos-mysql.sqldocker run -d --name nacos-cluster \\ -p 8848:8848 \\ -p 9848:9848 \\ -e TZ=Asia/Shanghai \\ --env SPRING_DATASOURCE_PLATFORM=mysql \\ --env MYSQL_SERVICE_HOST=192.168.101.82 \\ --env MYSQL_SERVICE_DB_NAME=nacos-server \\ --env MYSQL_SERVICE_USER=root \\ --env MYSQL_SERVICE_PASSWORD=Aa123#@! \\ nacos/nacos-server:v2.1.0#集群模式下Seata连接会报错：server is DOWNnow, detailed error message: Optional[Distro protocol is not initialized]，先用standalone模式启动，后面再解决。 #Browser, login with nacos/nacos http://192.168.101.82:8848/nacos/#/login#Seata:#Create Database:https://github.com/seata/seata/blob/develop/script/server/db/mysql.sqldocker run -d --name seata-server \\ -p 8091:8091 \\ -p 7091:7091 \\ -e SEATA_IP=192.168.101.82 \\ -e SEATA_PORT=8091 \\ -v /data/vagrant/boxes/docker/seata/config:/seata-server/resources \\ seataio/seata-server:1.5.1#Browser, login with seata/seatahttp://192.168.101.82:7091/#Sentinel Dashboard#https://www.cnblogs.com/wintersoft/p/11235192.htmlwget https://github.com/alibaba/Sentinel/releases/download/1.8.4/sentinel-dashboard-1.8.4.jarjava -Dserver.port=8082 \\-Dcsp.sentinel.dashboard.server=192.168.101.82:8082 \\-Dproject.name=sentinel-dashboard \\-jar sentinel-dashboard-1.8.4.jar#cat sentinel-dashboard-Dockerfile FROM openjdk:8ENV SENTINEL_HOME /opt/sentinel-dashboardRUN mkdir -p $&#123;SENTINEL_HOME&#125;COPY ./sentinel-dashboard-1.8.4.jar $&#123;SENTINEL_HOME&#125;RUN chmod -R +x $&#123;SENTINEL_HOME&#125;/*jarWORKDIR $&#123;SENTINEL_HOME&#125;EXPOSE 8080CMD java $&#123;JAVA_OPTS&#125; -jar -Dcsp.sentinel.dashboard.server=192.168.101.82:8082 -Dproject.name=sentinel-dashboard sentinel-dashboard-1.8.4.jar#builddocker build -t \"dave/sentinel-dashboard:1.8.4\" . -f sentinel-dashboard-Dockerfile#startdocker run -d --name sentinel-dashboard -p 8082:8080 dave/sentinel-dashboard:1.8.4#Browser, login with sentinel/sentinel http://192.168.101.82:8082/ application.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970server: port: 7091spring: application: name: seata-serverlogging: config: classpath:logback-spring.xml file: path: $&#123;user.home&#125;/logs/seata extend: logstash-appender: destination: 127.0.0.1:4560 kafka-appender: bootstrap-servers: 127.0.0.1:9092 topic: logback_to_logstashconsole: user: username: seata password: seataseata: config: # support: nacos, consul, apollo, zk, etcd3 type: nacos nacos: server-addr: 192.168.101.82:8848 namespace: group: SEATA_GROUP username: nacos password: nacos registry: # support: nacos, eureka, redis, zk, consul, etcd3, sofa type: nacos nacos: application: seata-server server-addr: 192.168.101.82:8848 group: SEATA_GROUP namespace: cluster: default username: nacos password: nacos store: # support: file 、 db 、 redis mode: db db: datasource: druid db-type: mysql driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://192.168.101.82:3306/seata-server?rewriteBatchedStatements=true user: root password: Aa123#@! min-conn: 5 max-conn: 100 global-table: global_table branch-table: branch_table lock-table: lock_table distributed-lock-table: distributed_lock query-limit: 100 max-wait: 5000# server:# service-port: 8091 #If not configured, the default is &apos;$&#123;server.port&#125; + 1000&apos; security: secretKey: SeataSecretKey0c382ef121d778043159209298fd40bf3850a017 tokenValidityInMilliseconds: 1800000 ignore: urls: /,/**/*.css,/**/*.js,/**/*.html,/**/*.map,/**/*.svg,/**/*.png,/**/*.ico,/console-fe/public/**,/api/v1/auth/login The scripts are located at: https://github.com/seata/seata/tree/develop/script Demohttps://seata.io/zh-cn/docs/user/quickstart.html https://github.com/seata/seata-samples/tree/master/dubbo Nacoshttps://nacos.io/zh-cn/docs/quick-start-spring-boot.html https://github.com/nacos-group/nacos-examples.git Shardingsphere-proxy123456789101112mkdir /data/shardingsphere-proxy/conf /data/ext-libwget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.11/mysql-connector-java-8.0.11.jar -P /data/ext-lib/docker run -d --name tmp --entrypoint=bash apache/shardingsphere-proxy:5.2.1docker cp tmp:/opt/shardingsphere-proxy/conf /data/shardingsphere-proxy/confdocker rm tmpdocker run -d --name shardingsphere-proxy \\ -v /data/shardingsphere-proxy/conf:/opt/shardingsphere-proxy/conf \\ -v /data/shardingsphere-proxy/ext-lib:/opt/shardingsphere-proxy/ext-lib \\ -e PORT=3308 -p13308:3308 apache/shardingsphere-proxy:5.2.1 cat server.yaml: 123456789rules: - !AUTHORITY users: - root@%:Aa1234$#@! provider: type: ALL_PERMITTED - !TRANSACTION defaultType: XA providerType: Atomikos 一个库一个配置文件。 config-sharding-test.yaml: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849schemaName: seata_testdataSources: ds_0: url: jdbc:mysql://192.168.101.82/seata_test?serverTimezone=UTC&amp;useSSL=false username: root password: aaa connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 minPoolSize: 1rules:- !SHARDING tables: service_at: actualDataNodes: ds_0.service_at_$-&gt;&#123;0..2&#125; tableStrategy: standard: shardingColumn: name shardingAlgorithmName: database_consistent #shardingAlgorithmName: database_inline keyGenerateStrategy: column: id keyGeneratorName: snowflake service_tm: actualDataNodes: ds_0.service_tm_$-&gt;&#123;0..2&#125; tableStrategy: standard: shardingColumn: aName shardingAlgorithmName: database_consistent #shardingAlgorithmName: database_inline keyGenerateStrategy: column: id keyGeneratorName: snowflake shardingAlgorithms: database_consistent: type: CONSISTENT_HASH #database_inline: # type: INLINE # props: # algorithm-expression: service_at_$-&gt;&#123;account_id % 3&#125; keyGenerators: snowflake: type: SNOWFLAKE props: worker-id: 123 cat config-sharding-account.yaml: 123456789101112131415161718192021222324252627282930313233343536373839schemaName: account_devdataSources: ds_0: url: jdbc:mysql://rm-3ns38ayfen92i16o6.mysql.rds.aliyuncs.com:3306/account_dev?serverTimezone=UTC&amp;useSSL=false username: root password: aaa connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 minPoolSize: 1rules:- !SHARDING tables: capital_accounting_record: actualDataNodes: ds_0.capital_accounting_record_$-&gt;&#123;0..2&#125; tableStrategy: standard: shardingColumn: account_number shardingAlgorithmName: database_consistent #shardingAlgorithmName: database_inline keyGenerateStrategy: column: id keyGeneratorName: snowflake shardingAlgorithms: database_consistent: type: CONSISTENT_HASH #database_inline: # type: INLINE # props: # algorithm-expression: capital_accounting_record_$-&gt;&#123;account_id % 3&#125; keyGenerators: snowflake: type: SNOWFLAKE props: worker-id: 123 自定义分片规则: 12345678910111213141516#https://shardingsphere.apache.org/document/5.2.1/cn/user-manual/shardingsphere-proxy/startup/bin/1. 实现 `ShardingAlgorithm` 接口定义的算法实现类。2. 在项目 `resources` 目录下创建 `META-INF/services` 目录。3. 在 `META-INF/services` 目录下新建文件 `org.apache.shardingsphere.sharding.spi.ShardingAlgorithm`4. 将实现类的全限定类名写入至文件 `org.apache.shardingsphere.sharding.spi.ShardingAlgorithm`5. 将上述 Java 文件打包成 jar 包。6. 将上述 jar 包拷贝至 `ext-lib` 目录。7. 将上述自定义算法实现类的 Java 文件引用配置在 YAML 文件中: tableStrategy: standard: ... shardingAlgorithmName: database_consistent ... shardingAlgorithms: database_consistent: type: CONSISTENT_HASH Reference https://help.aliyun.com/document_detail/157850.html https://www.macrozheng.com/cloud/seata.html https://github.com/macrozheng/springcloud-learning https://seata.io/zh-cn/docs/ops/deploy-guide-beginner.html https://github.com/seata/seata/tree/v1.5.1/script https://nacos.io/zh-cn/docs/what-is-nacos.html http://c.biancheng.net/springcloud/seata.html http://c.biancheng.net/springcloud/nacos.html https://www.apolloconfig.com/#/zh/README","categories":[{"name":"java","slug":"java","permalink":"http://blog.gcalls.cn/categories/java/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"},{"name":"java","slug":"java","permalink":"http://blog.gcalls.cn/tags/java/"}]},{"title":"Fabric Getting Started","slug":"Fabric-Getting-Started","date":"2022-06-08T03:33:18.000Z","updated":"2024-08-02T05:39:00.967Z","comments":true,"path":"/2022/06/Fabric-Getting-Started.html","link":"","permalink":"http://blog.gcalls.cn/2022/06/Fabric-Getting-Started.html","excerpt":"A getting started guilde to Fabric. Following https://hyperledger-fabric.readthedocs.io/en/latest/whatis.html to what it is.","text":"A getting started guilde to Fabric. Following https://hyperledger-fabric.readthedocs.io/en/latest/whatis.html to what it is. InstallationUsing Vagant and docker as a platform to try. The detailed instructions refer to: https://github.com/zhaoxunyong/boxes/tree/main/docker/fabric Startup12345678910111213Step 1:cd /data/vagrant/dockervagrant destroy node1 -frm -fr /data/var-lib-docker/vagrant up node1#Entering the container:#vagrant docker-exec -it node1 -- /bin/bash#docker exec -it `docker ps|grep node1|awk '&#123;print $1&#125;'` /vagrant/changepwd.shdocker exec -it `docker ps|grep node1|awk '&#123;print $1&#125;'` /vagrant/images/importImages.shvagrant reload node1#Then login with terminal, not docker exec#docker restart `docker ps|grep node1|awk '&#123;print $1&#125;'` Starting a chaincode on the channel1234567891011121314151617181920212223242526272829#https://hyperledger-fabric.readthedocs.io/en/latest/test_network.html#Openning a new terminal to input the following command:cd /data/fabric/fabric-samples/test-network./network.sh up #Creating a channel./network.sh createChannel#./network.sh up createChannel#Monitor:./monitordocker.sh fabric_test#The following instruction need GO to be installed../network.sh deployCC -ccn basic -ccp ../asset-transfer-basic/chaincode-go -ccl go#The following instruction need node.js to be installed../network.sh deployCC -ccn basic -ccp ../asset-transfer-basic/chaincode-javascript -ccl javascript#The following instruction need java to be installed.# Not need:# #Setting the proxy to download gradle-7.3.1-bin.zip. Or downloading maybe is very slow...# tee /data/fabric/fabric-samples/asset-transfer-basic/chaincode-java/gradle.properties &lt;&lt;-&apos;EOF&apos;# systemProp.http.proxyHost=192.168.102.82# systemProp.http.proxyPort=1082# systemProp.https.proxyHost=192.168.102.82# systemProp.https.proxyPort=1082# systemProp.http.nonProxyHosts=192.*|172.*|127.*|localhost# EOF./network.sh deployCC -ccn basic -ccp ../asset-transfer-basic/chaincode-java -ccl java#Don&apos;t use the proxy to build, Or you will encouter the following errors:Error: chaincode install failed with status: 500 - error in simulation: failed to execute transaction c6d2553ed5f14fa4336438c686f538730f90f51bf1c5737f60c0cd3f0e17561a: error sending: timeout expired while executing transaction Interacting with the network1234567891011121314151617181920212223242526272829303132333435363738cd /data/fabric/fabric-samples/test-networkexport PATH=$&#123;PWD&#125;/../bin:$PATHexport FABRIC_CFG_PATH=$PWD/../config/#You can now set the environment variables that allow you to operate the peer CLI as Org1:#Environment variables for Org1export CORE_PEER_TLS_ENABLED=trueexport CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/users/Admin@org1.example.com/mspexport CORE_PEER_ADDRESS=localhost:7051#Run the following command to initialize the ledger with assets.peer chaincode invoke -o localhost:7050 --ordererTLSHostnameOverride orderer.example.com --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot; -C mychannel -n basic --peerAddresses localhost:7051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt&quot; --peerAddresses localhost:9051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt&quot; -c &apos;&#123;&quot;function&quot;:&quot;InitLedger&quot;,&quot;Args&quot;:[]&#125;&apos;#You can now query the ledger from your CLI. Run the following command to get the list of assets that were added to your channel ledger:peer chaincode query -C mychannel -n basic -c &apos;&#123;&quot;Args&quot;:[&quot;GetAllAssets&quot;]&#125;&apos; | prettyjson#Use the following command to change the owner of an asset on the ledger by invoking the asset-transfer (basic) chaincode:peer chaincode invoke -o localhost:7050 --ordererTLSHostnameOverride orderer.example.com --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot; -C mychannel -n basic --peerAddresses localhost:7051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt&quot; --peerAddresses localhost:9051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt&quot; -c &apos;&#123;&quot;function&quot;:&quot;TransferAsset&quot;,&quot;Args&quot;:[&quot;asset6&quot;,&quot;Christopher&quot;]&#125;&apos;#Set the following environment variables to operate as Org2:#Environment variables for Org2export CORE_PEER_TLS_ENABLED=trueexport CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/users/Admin@org2.example.com/mspexport CORE_PEER_ADDRESS=localhost:9051#You can now query the asset-transfer (basic) chaincode running on peer0.org2.example.com:peer chaincode query -C mychannel -n basic -c &apos;&#123;&quot;Args&quot;:[&quot;ReadAsset&quot;,&quot;asset6&quot;]&#125;&apos;#Bring down the network./network.sh down#relogin to take effect from trying again.&lt;!-- docker rm -f $(docker ps -aq)docker rmi -f $(docker images -q)docker rmi -f $(docker images | grep dev-peer[0-9] | awk &apos;&#123;print $3&#125;&apos;) --&gt; Deploying a smart contract to a channelPackage the smart contract1234567#GOcd /data/fabric/fabric-samples/test-networkexport PATH=$&#123;PWD&#125;/../bin:$PATHexport FABRIC_CFG_PATH=$PWD/../config/peer version#You can now create the chaincode package using the peer lifecycle chaincode package command:peer lifecycle chaincode package basic.tar.gz --path ../asset-transfer-basic/chaincode-go/ --lang golang --label basic_1.0 Install the chaincode package123456789101112131415161718both Org1 and org2#We can now install the chaincode on the Org1 peerexport CORE_PEER_TLS_ENABLED=trueexport CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/users/Admin@org1.example.com/mspexport CORE_PEER_ADDRESS=localhost:7051peer lifecycle chaincode install basic.tar.gz#We can now install the chaincode on the Org2 peerexport CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/users/Admin@org2.example.com/mspexport CORE_PEER_ADDRESS=localhost:9051peer lifecycle chaincode install basic.tar.gz Approve a chaincode definition12345678910111213141516171819202122both Org1 and org2#You can find the package ID of a chaincode by using the peer lifecycle chaincode queryinstalled command to query your peerpeer lifecycle chaincode queryinstalled#let’s go ahead and save it as an environment variable. Paste the package ID returned by peer lifecycle chaincode queryinstalled into the command belowexport CC_PACKAGE_ID=basic_1.0:2e20ce421c8037420718c8a3918a1eea76343b7361fffdac454181c54e5736c7#Chaincode is approved at the organization level, so the command only needs to target one peer. The approval is distributed to the other peers within the organization using gossip. Approve the chaincode definition using the peer lifecycle chaincode approveformyorg command:peer lifecycle chaincode approveformyorg -o localhost:7050 --ordererTLSHostnameOverride orderer.example.com --channelID mychannel --name basic --version 1.0 --package-id $CC_PACKAGE_ID --sequence 1 --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot;#We still need to approve the chaincode definition as Org1export CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;export CORE_PEER_MSPCONFIGPATH=$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/users/Admin@org1.example.com/mspexport CORE_PEER_TLS_ROOTCERT_FILE=$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crtexport CORE_PEER_ADDRESS=localhost:7051peer lifecycle chaincode approveformyorg -o localhost:7050 --ordererTLSHostnameOverride orderer.example.com --channelID mychannel --name basic --version 1.0 --package-id $CC_PACKAGE_ID --sequence 1 --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot;#We now have the majority we need to deploy the asset-transfer (basic) the chaincode to the channel. Committing the chaincode definition to the channel1234567891011121314151617&quot;peer lifecycle chaincode commit&quot; need to imply the both of peers of Org1 and Org2:--peerAddresses localhost:7051--peerAddresses localhost:9051 #After a sufficient number of organizations have approved a chaincode definition, one organization can commit the chaincode definition to the channelYou can use the peer lifecycle chaincode checkcommitreadiness command to check whether channel members have approved the same chaincode definitionpeer lifecycle chaincode checkcommitreadiness --channelID mychannel --name basic --version 1.0 --sequence 1 --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot; --output json#You can use the peer lifecycle chaincode commit command to commit the chaincode definition to the channel. The commit command also needs to be submitted by an organization admin.peer lifecycle chaincode commit -o localhost:7050 --ordererTLSHostnameOverride orderer.example.com --channelID mychannel --name basic --version 1.0 --sequence 1 --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot; --peerAddresses localhost:7051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt&quot; --peerAddresses localhost:9051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt&quot;#You can use the peer lifecycle chaincode querycommitted command to confirm that the chaincode definition has been committed to the channel.peer lifecycle chaincode querycommitted --channelID mychannel --name basic --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot; Invoking the chaincode1234567#Use the following command to create an initial set of assets on the ledger. Note the CLI does not access the Fabric Gateway peer, so each endorsing peer must be specified.peer chaincode invoke -o localhost:7050 --ordererTLSHostnameOverride orderer.example.com --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot; -C mychannel -n basic --peerAddresses localhost:7051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt&quot; --peerAddresses localhost:9051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt&quot; -c &apos;&#123;&quot;function&quot;:&quot;InitLedger&quot;,&quot;Args&quot;:[]&#125;&apos;#We can use a query function to read the set of cars that were created by the chaincode:peer chaincode query -C mychannel -n basic -c &apos;&#123;&quot;Args&quot;:[&quot;GetAllAssets&quot;]&#125;&apos; | prettyjson Upgrading a smart contract1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#Openning a new terminal to input the following command:cd /data/fabric/fabric-samples/asset-transfer-basic/chaincode-javascriptnpm installcd ../../test-networkexport PATH=$&#123;PWD&#125;/../bin:$PATHexport FABRIC_CFG_PATH=$PWD/../config/#You can then issue the following commands to package the JavaScript chaincode from the test-network directory. We will set the environment variables needed to use the peer CLI again in case you closed your terminal.export CORE_PEER_MSPCONFIGPATH=$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msppeer lifecycle chaincode package basic_2.tar.gz --path ../asset-transfer-basic/chaincode-javascript/ --lang node --label basic_2.0#Run the following commands to operate the peer CLI as the Org1 admin:export CORE_PEER_TLS_ENABLED=trueexport CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/users/Admin@org1.example.com/mspexport CORE_PEER_ADDRESS=localhost:7051#https://charlielin.top/2020/03/26/%E5%9C%A8-fabric-%E4%B8%8A%E6%89%A7%E8%A1%8C-chaincode-%E7%9A%84%E6%A2%B3%E7%90%86/#We can now use the following command to install the new chaincode package on the Org1 peer.#It much more slower than installing go, which needs a few seconds to be installed, just wait with patient:peer lifecycle chaincode install basic_2.tar.gzpeer lifecycle chaincode queryinstalled#ou can use the package label to find the package ID of the new chaincode and save it as a new environment variable. This output is for example only – your package ID will be different, so DO NOT COPY AND PASTE!export NEW_CC_PACKAGE_ID=basic_2.0:65dc35f2a2bfc653fc329254b5e0ab2646b4fd65f7613f7772d9954da064a224#Org1 can now approve a new chaincode definition:#Because the sequence parameter is used by the Fabric chaincode lifecycle to keep track of chaincode upgrades, Org1 also needs to increment the sequence number from 1 to 2peer lifecycle chaincode approveformyorg -o localhost:7050 --ordererTLSHostnameOverride orderer.example.com --channelID mychannel --name basic --version 2.0 --package-id $NEW_CC_PACKAGE_ID --sequence 2 --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot;#We now need to install the chaincode package and approve the chaincode definition as Org2 in order to upgrade the chaincode. export CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/users/Admin@org2.example.com/mspexport CORE_PEER_ADDRESS=localhost:9051#We can now use the following command to install the new chaincode package on the Org2 peer.peer lifecycle chaincode install basic_2.tar.gz#You can now approve the new chaincode definition for Org2.peer lifecycle chaincode approveformyorg -o localhost:7050 --ordererTLSHostnameOverride orderer.example.com --channelID mychannel --name basic --version 2.0 --package-id $NEW_CC_PACKAGE_ID --sequence 2 --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot;#check if the chaincode definition with sequence 2 is ready to be committed to the channel:peer lifecycle chaincode checkcommitreadiness --channelID mychannel --name basic --version 2.0 --sequence 2 --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot; --output json#Org2 can use the following command to upgrade the chaincode:peer lifecycle chaincode commit -o localhost:7050 --ordererTLSHostnameOverride orderer.example.com --channelID mychannel --name basic --version 2.0 --sequence 2 --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot; --peerAddresses localhost:7051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt&quot; --peerAddresses localhost:9051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt&quot;#we can test our new JavaScript chaincode by creating a new car:peer chaincode invoke -o localhost:7050 --ordererTLSHostnameOverride orderer.example.com --tls --cafile &quot;$&#123;PWD&#125;/organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem&quot; -C mychannel -n basic --peerAddresses localhost:7051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt&quot; --peerAddresses localhost:9051 --tlsRootCertFiles &quot;$&#123;PWD&#125;/organizations/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt&quot; -c &apos;&#123;&quot;function&quot;:&quot;CreateAsset&quot;,&quot;Args&quot;:[&quot;asset8&quot;,&quot;blue&quot;,&quot;16&quot;,&quot;Kelley&quot;,&quot;750&quot;]&#125;&apos;#Clean updocker stop logspoutdocker rm logspoutcd /data/fabric/fabric-samples/test-network./network.sh down Running a Fabric ApplicationSet up the blockchain network./network.sh up createChannel -c mychannel -ca Deploy the smart contract./network.sh deployCC -ccn basic -ccp ../asset-transfer-basic/chaincode-typescript/ -ccl typescript Prepare the sample application12345cd ../asset-transfer-basic/application-gateway-typescriptapt install -y make g++npm installnpm run-script buildnpm start Running chaincode in development modeSet up environment12345678910#git clone https://github.com/hyperledger/fabricapt install -y make g++cp -a /vagrant/fabric /Developer/cd /Developer/fabricmake orderer peer configtxgenexport PATH=$(pwd)/build/bin:$PATHexport FABRIC_CFG_PATH=$(pwd)/sampleconfigsudo mkdir /var/hyperledgerconfigtxgen -profile SampleDevModeSolo -channelID syschannel -outputBlock genesisblock -configPath $FABRIC_CFG_PATH -outputBlock &quot;$(pwd)/sampleconfig/genesisblock&quot; Start the ordererORDERER_GENERAL_GENESISPROFILE=SampleDevModeSolo orderer Start the peer in DevMode123456#Open another terminal window and set the required environment variables to override the peer configuration and start the peer node:cd /Developer/fabricexport CORE_OPERATIONS_LISTENADDRESS=127.0.0.1:9444export PATH=$(pwd)/build/bin:$PATHexport FABRIC_CFG_PATH=$(pwd)/sampleconfigFABRIC_LOGGING_SPEC=chaincode=debug CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052 peer node start --peer-chaincodedev=true Create channel and join peer12345678#Open another terminal windowcd /Developer/fabricexport PATH=$(pwd)/build/bin:$PATHexport FABRIC_CFG_PATH=$(pwd)/sampleconfigconfigtxgen -channelID ch1 -outputCreateChannelTx ch1.tx -profile SampleSingleMSPChannel -configPath $FABRIC_CFG_PATHpeer channel create -o 127.0.0.1:7050 -c ch1 -f ch1.txpeer channel join -b ch1.block Build the chaincode12cd /Developer/fabricgo build -o simpleChaincode ./integration/chaincode/simple/cmd Start the chaincodeCORE_CHAINCODE_LOGLEVEL=debug CORE_PEER_TLS_ENABLED=false CORE_CHAINCODE_ID_NAME=mycc:1.0 ./simpleChaincode -peer.address 127.0.0.1:7052 Approve and commit the chaincode definition12345678#Open another terminal windowcd /Developer/fabricexport PATH=$(pwd)/build/bin:$PATHexport FABRIC_CFG_PATH=$(pwd)/sampleconfigpeer lifecycle chaincode approveformyorg -o 127.0.0.1:7050 --channelID ch1 --name mycc --version 1.0 --sequence 1 --init-required --signature-policy &quot;OR (&apos;SampleOrg.member&apos;)&quot; --package-id mycc:1.0peer lifecycle chaincode checkcommitreadiness -o 127.0.0.1:7050 --channelID ch1 --name mycc --version 1.0 --sequence 1 --init-required --signature-policy &quot;OR (&apos;SampleOrg.member&apos;)&quot;peer lifecycle chaincode commit -o 127.0.0.1:7050 --channelID ch1 --name mycc --version 1.0 --sequence 1 --init-required --signature-policy &quot;OR (&apos;SampleOrg.member&apos;)&quot; --peerAddresses 127.0.0.1:7051 Next steps123CORE_PEER_ADDRESS=127.0.0.1:7051 peer chaincode invoke -o 127.0.0.1:7050 -C ch1 -n mycc -c &apos;&#123;&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;100&quot;,&quot;b&quot;,&quot;200&quot;]&#125;&apos; --isInitCORE_PEER_ADDRESS=127.0.0.1:7051 peer chaincode invoke -o 127.0.0.1:7050 -C ch1 -n mycc -c &apos;&#123;&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]&#125;&apos;CORE_PEER_ADDRESS=127.0.0.1:7051 peer chaincode invoke -o 127.0.0.1:7050 -C ch1 -n mycc -c &apos;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]&#125;&apos;","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"},{"name":"BlockChain","slug":"BlockChain","permalink":"http://blog.gcalls.cn/tags/BlockChain/"}]},{"title":"Containerd","slug":"Containerd","date":"2022-05-11T03:44:06.000Z","updated":"2024-08-29T07:10:04.829Z","comments":true,"path":"/2022/05/Containerd.html","link":"","permalink":"http://blog.gcalls.cn/2022/05/Containerd.html","excerpt":"An industry-standard container runtime with an emphasis on simplicity, robustness and portability: https://containerd.io/","text":"An industry-standard container runtime with an emphasis on simplicity, robustness and portability: https://containerd.io/ Installationnerdctl is a Docker-compatible CLI for containerd. The release full version has been included dependencies such as containerd, runc, and CNI.Maybe nerdctl is not the latest version, you can install the latest version step by step by the following instructions: https://github.com/containerd/containerd/blob/main/docs/getting-started.md 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#https://github.com/containerd/containerd/blob/main/docs/getting-started.md##Option 1: From the official binaries#Step 1: Installing containerd##Step 2: Installing runc#Step 2: Installing CNI plugins#Option 2: From apt-get or dnf#The containerd.io packages in DEB and RPM formats are distributed by Docker (not by the containerd project). See the Docker documentation for how to set up apt-get or dnf to install containerd.io packages:#The containerd.io package contains runc too, but does not contain CNI plugins.#https://docs.docker.com/engine/install/ubuntu/# Add Docker's official GPG key:sudo apt-get updatesudo apt-get install ca-certificates curlsudo install -m 0755 -d /etc/apt/keyringssudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.ascsudo chmod a+r /etc/apt/keyrings/docker.asc# Add the repository to Apt sources:echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullsudo apt-get updateapt install containerd.io#Installing CNI pluginswget https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz#Extract it under /opt/cni/binmkdir -p /opt/cni/bintar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.5.1.tgz#https://github.com/containerd/containerd/blob/main/containerd.service#wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -P /usr/local/lib/systemd/system/#sed -i 's;/usr/local/bin/containerd;/usr/bin/containerd;g' /usr/local/lib/systemd/system/containerd.servicesystemctl enable --now containerd#Optional: Proxysudo mkdir -p /etc/systemd/system/containerd.service.dsudo touch /etc/systemd/system/containerd.service.d/http-proxy.confsudo tee /etc/systemd/system/containerd.service.d/http-proxy.conf &lt;&lt;-'EOF'[Service]Environment=\"HTTP_PROXY=http://127.0.0.1:1082\"Environment=\"HTTPS_PROXY=http://127.0.0.1:1082\"Environment=\"NO_PROXY=127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net,*.aliyun.com,*.163.com,*.docker-cn.com,kubernetes.docker.internal\"EOF# Restart service:sudo systemctl daemon-reloadsudo systemctl restart containerdsudo systemctl show --property=Environment containerd#Nerdctl(Optional)wget https://github.com/containerd/nerdctl/releases/download/v1.7.6/nerdctl-1.7.6-linux-amd64.tar.gz#Extract the archive to a path like /usr/local/bin or ~/bintar Cxzvvf /usr/local/bin nerdctl-1.7.6-linux-amd64.tar.gz#ln -s /usr/local/bin/nerdctl /usr/local/bin/docker#Buildkitwget https://github.com/moby/buildkit/releases/download/v0.15.2/buildkit-v0.15.2.linux-amd64.tar.gztar Cxzvvf /usr/local/ buildkit-v0.15.2.linux-amd64.tar.gz#https://github.com/moby/buildkit/tree/master/examples/systemd/systemwget https://raw.githubusercontent.com/moby/buildkit/master/examples/systemd/system/buildkit.service -P /usr/local/lib/systemd/system/wget https://raw.githubusercontent.com/moby/buildkit/master/examples/systemd/system/buildkit.socket -P /usr/local/lib/systemd/system/#cp -a buildkit.service /usr/local/lib/systemd/system/buildkit.servicesystemctl enable --now buildkit#Uninstallsystemctl stop containerdsystemctl stop buildkittar -tf nerdctl-1.7.4-linux-amd64.tar.gz | sed 's;^;rm /usr/local/bin/;' | sh +xtar -tf buildkit-v0.13.0.linux-amd64.tar.gz | grep \"^bin/.+*\" | sed 's;^;rm /usr/local/;' | sh +xrm -fr /opt/cni/bin/rm /usr/local/lib/systemd/system/buildkit.socketrm /usr/local/lib/systemd/system/buildkit.serviceapt remove containerd.iorm /usr/local/lib/systemd/system/containerd.service#Testsudo nerdctl run --rm --name nginx -p 80:80 nginx:alpine cat Dockerfile: 12FROM nginx:alpineRUN echo 'Hello Nerdctl From Containerd' &gt; /usr/share/nginx/html/index.html Relocated /var/lib/containerd: 1234systemctl stop containerdmv /var/lib/containerd /data/containerd-libln -s /data/containerd-lib /var/lib/containerdsystemctl start containerd Usage123456#Build:nerdctl build -t nginx:nerctl -f Dockerfile .#Run:nerdctl run -d --name nginx -p 80:80 nginx:nerctl#Test:curl localhost The native containderd command: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#https://cloudyuga.guru/blogs/containerd-and-ctr/#https://medium.com/@seifeddinerajhi/understanding-and-using-containerd-a-comprehensive-guide-7b34f6136058ctr --help#container can short as c#task can short as tctr c lsctr t ls#vie namespacectr ns ls#pullctr images pull docker.io/library/nginx:alpine#push#ctr image push localhost:5000/saif/test:latest#List out the imagesctr images ls#For listing the images with namesctr images ls -q#mount imagesmkdir /tmp/nginxctr images mount docker.io/library/nginx:alpine /tmp/golangls -l /tmp/nginx/#unmount pointctr images unmount /tmp/nginx#extract the tarball to a temporary directory and explore its contentsmkdir /tmp/nginx_imagetar -xf /tmp/nginx.tar -C /tmp/nginx_image/ls -lah /tmp/nginx_image/#delete the imagesctr images rm docker.io/library/nginx:alpinectr image remove docker.io/library/nginx:alpine#tagctr image tag docker.io/library/nginx:alpine localhost:5000/library/nginx:alpine#export imagesctr images export /data/images/nginx.tar docker.io/library/nginx:alpine --platform linux/amd64#import imagesctr images import /data/images/nginx.tar#create a containerctr container create docker.io/library/nginx:alpine nginx_ctr#List out the containersctr containers ls#startctr task start nginx_ctr#List the tasksctr task ls#create and startctr run -d docker.io/library/nginx:alpine nginx_web#To see the stdout and stderr of a running task#But be careful, the ctr task attach command will also reconnect the stdin stream and start forwarding signals from the controlling terminal to the task processes, so hitting Ctrl+C might kill the task.#Unfortunately, ctr doesn't support the Ctrl+P+Q shortcut to detach from a task - it's solely docker's feature. There is also no ctr task logs, so you can't see the stdout/stderr of a task without attaching to it. Neither can you easily see the logs of a stopped task. It's a lower-level tool, remember?ctr task attach nginx_web#interact with the containerctr task exec -t --exec-id bash_1 nginx_web sh#check the usage of the metrics by the taskctr task metrics nginx_web#stop all the tasksctr task kill nginx_web#remove the containerctr container rm nginx_web#inspectctr container info nginx_web#snapshot commitctr snapshot commit dave_nginx_ctr nginx_ctr#snapshot listctr snapshot ls Reference https://github.com/containerd/containerd/blob/main/docs/getting-started.md https://www.qikqiak.com/post/containerd-usage/ https://kubernetes.io/zh/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/ https://www.51cto.com/article/678323.html https://developer.51cto.com/article/700609.html https://www.jianshu.com/p/4c31554df8c9","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.gcalls.cn/tags/Kubernetes/"}]},{"title":"A Guide to Vagrant","slug":"A-Guide-to-Vagrant","date":"2022-04-29T02:43:46.000Z","updated":"2024-08-02T05:39:00.967Z","comments":true,"path":"/2022/04/A-Guide-to-Vagrant.html","link":"","permalink":"http://blog.gcalls.cn/2022/04/A-Guide-to-Vagrant.html","excerpt":"This is a guide to how to use Vagrant to virtual some OS, like Linux/Windows/MacOS etc.","text":"This is a guide to how to use Vagrant to virtual some OS, like Linux/Windows/MacOS etc. IntroductionSimple and Powerful HashiCorp Vagrant provides the same, easy workflow regardless of your role as a developer, operator, or designer. It leverages a declarative configuration file which describes all your software requirements, packages, operating system configuration, users, and more. Works where you work Vagrant works on Mac, Linux, Windows, and more. Remote development environments force users to give up their favorite editors and programs. Vagrant works on your local system with the tools you’re already familiar with. Easily code in your favorite text editor, edit images in your favorite manipulation program, and debug using your favorite tools, all from the comfort of your local laptop. Installationvagrant+dockerRecommanding by vagrant plus docker: https://www.vagrantup.com/docs/providers/docker 12345curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"sudo apt-get update &amp;&amp; sudo apt-get install vagrant#Installing docker following this: http://blog.gcalls.cn/blog/2018/12/ubuntu-os.html#Docker vagrant+virtualbox123456789101112131415161718192021#https://www.vagrantup.com/downloads#The executable 'bsdtar' Vagrant is trying to run was not found in the PATH variable. This is an error. Please verify this software is installed and on the path.# wget https://releases.hashicorp.com/vagrant/2.2.19/vagrant_2.2.19_linux_amd64.zip# unzip vagrant_2.2.19_linux_amd64.zip#Recommend:curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"sudo apt-get update &amp;&amp; sudo apt-get install vagrant#VirtualBoxwget https://download.virtualbox.org/virtualbox/6.1.34/virtualbox-6.1_6.1.34-150636.1~Ubuntu~eoan_amd64.debdpkg -i virtualbox-6.1_6.1.34-150636.1~Ubuntu~eoan_amd64.deb##If some error occurred, executing the following command, and run again:# sudo apt install -f# dpkg -i virtualbox-6.1_6.1.34-150636.1~Ubuntu~eoan_amd64.debsudo apt install -y gcc make perlsudo /sbin/vboxconfig#If some error occurred, just following the messge to resolve.#extpackVBoxManage extpack install Oracle_VM_VirtualBox_Extension_Pack-6.1.34.vbox-extpack Configurationvagrant+dockerDockerfile: 1234567891011121314151617181920#Version: 1.0.0FROM ubuntu:22.04RUN mkdir -p /container/shell ENV WORK_SHELL /container/shellWORKDIR $WORK_SHELL#ADD ./sources.list /etc/apt/sources.list#RUN apt update &amp;&amp; apt install -y initADD docker-entrypoint.sh script.sh $WORK_SHELL/RUN chmod +x $WORK_SHELL/*.shRUN $WORK_SHELL/script.shENTRYPOINT [\"/container/shell/docker-entrypoint.sh\"]#CMD [\"bash\", \"-c\" ,\"$WORK_SHELL/init.sh\"] docker-entrypoint.sh:1234#!/bin/bash# run the command given as arguments from CMDexec \"$@\" script.sh: Finding the details from belows. Building docker image: 1docker build -t dave/ubuntu:22.04 . Vagrangfile: 1234567891011121314151617181920212223242526# -*- mode: ruby -*-# vi: set ft=ruby :# All Vagrant configuration is done below. The \"2\" in Vagrant.configure# configures the configuration version (we support older styles for# backwards compatibility). Please don't change it unless you know what# you're doing.Vagrant.configure(\"2\") do |config| config.vm.network :public_network, ip: \"192.168.109.50\", netmask: \"255.255.255.0\", bridge: \"eno1\", docker_network__gateway: \"192.168.109.254\" config.vm.provider \"docker\" do |d| d.image = \"registry.zerofinance.net/library/ubuntu:22.04\" #d.build_dir = \".\" #d.create_args = [\"--hostname=config\", \"-v\", \"/data/fisco:/data/fisco\", \"-v\", \"/data/vagrant/docker/fisco/shell:/data/shell\"] d.create_args = [\"--cpus=40\", \"--memory=64g\", \"--hostname=ubuntu-analysis\", \"-v\", \"/data/containerd:/var/lib/containerd\", \"-v\", \"/works/app/container_apps:/works/app\", \"-v\", \"/data/container_datas:/data\"] d.privileged = true d.cmd = [\"/sbin/init\"] end config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL sudo route del default gw 172.17.0.1 sudo route add default gw 192.168.109.254 SHELLend Or just using docker by itself: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#https://www.cnblogs.com/bakari/p/10893589.html#https://blog.oddbit.com/post/2018-03-12-using-docker-macvlan-networks/#Solved: Container can communicate with host machine:docker network create \\ -d 'macvlan' \\ -o parent=enp2s0 \\ --subnet=192.168.101.0/24 \\ --gateway=192.168.101.254 \\ --aux-address 'host=192.168.101.81' \\ my-network#/etc/rc.localip link add mynet-shim link enp2s0 type macvlan mode bridge#192.168.101.81: a host ipip addr add 192.168.101.81/32 dev mynet-shimip link set mynet-shim up#192.168.101.85: a container ipip route add 192.168.101.85/32 dev mynet-shimip route add 192.168.101.86/32 dev mynet-shimip route add 192.168.101.87/32 dev mynet-shimip route add 192.168.101.88/32 dev mynet-shimmkdir -p /data/dave/rancher /data/peter/rancher /data/eino/rancher /data/xiaotie/rancherdocker run -d \\ --name=dave-server --hostname=dave-server \\ --network=my-network \\ --ip=192.168.101.85 \\ --privileged=true \\ -v /etc/localtime:/etc/localtime \\ -v /etc/hosts:/etc/hosts \\ -v /data/dave:/data \\ -v /works/app:/works/app \\ -v /data/dave/rancher:/var/lib/rancher \\ registry.zerofinance.net/library/ubuntu:22.04 \\ /sbin/initdocker run -d \\ --name=peter-server --hostname=peter-server \\ --network=my-network \\ --ip=192.168.101.86 \\ --privileged=true \\ -v /etc/localtime:/etc/localtime \\ -v /etc/hosts:/etc/hosts \\ -v /data/peter:/data \\ -v /works/app:/works/app \\ -v /data/peter/rancher:/var/lib/rancher \\ registry.zerofinance.net/library/ubuntu:22.04 \\ /sbin/initdocker run -d \\ --name=eino-server --hostname=eino-server \\ --network=my-network \\ --ip=192.168.101.87 \\ --privileged=true \\ -v /etc/localtime:/etc/localtime \\ -v /etc/hosts:/etc/hosts \\ -v /data/eino:/data \\ -v /works/app:/works/app \\ -v /data/eino/rancher:/var/lib/rancher \\ registry.zerofinance.net/library/ubuntu:22.04 \\ /sbin/initdocker run -d \\ --name=xiaotie-server --hostname=xiaotie-server \\ --network=my-network \\ --ip=192.168.101.88 \\ --privileged=true \\ -v /etc/localtime:/etc/localtime \\ -v /etc/hosts:/etc/hosts \\ -v /data/xiaotie:/data \\ -v /works/app:/works/app \\ -v /data/xiaotie/rancher:/var/lib/rancher \\ registry.zerofinance.net/library/ubuntu:22.04 \\ /sbin/init# docker run -d \\# --name=my-other-nginx \\# --network=my-network \\# --ip=192.168.101.84 \\# nginx:latest Or multiple nodes with Vagrantfile: 1234567891011121314151617181920212223242526272829303132# -*- mode: ruby -*-# vi: set ft=ruby :# All Vagrant configuration is done below. The \"2\" in Vagrant.configure# configures the configuration version (we support older styles for# backwards compatibility). Please don't change it unless you know what# you're doing.Vagrant.configure(\"2\") do |config| config.vm.define \"node1\" do |node1| node1.vm.network :public_network, ip: \"192.168.101.83\", netmask: \"255.255.255.0\", bridge: \"enp2s0\", docker_network__gateway: \"192.168.101.254\" node1.vm.provider \"docker\" do |d| d.image = \"dave/docker:22.04\" #d.build_dir = \".\" d.create_args = [\"--hostname=node1\", \"-v\", \"/data/var-lib-docker:/var/lib/docker\", \"-v\",\"/data/fabric:/data/fabric\"] d.privileged = true d.cmd = [\"/sbin/init\"] end end config.vm.define \"node2\" do |node2| node2.vm.network :public_network, ip: \"192.168.101.84\", netmask: \"255.255.255.0\", bridge: \"enp2s0\", docker_network__gateway: \"192.168.101.254\" node2.vm.provider \"docker\" do |d| d.image = \"dave/docker:22.04\" #d.build_dir = \".\" d.create_args = [\"--hostname=node2\", \"-v\", \"/data/var-lib-docker:/var/lib/docker\", \"-v\",\"/data/fabric:/data/fabric\"] d.privileged = true d.cmd = [\"/sbin/init\"] end endend Installing docker in dokcer: docker.sh: 1234567891011121314151617181920212223#!/bin/bashapt-get -y install apt-transport-https ca-certificates software-properties-commoncurl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | apt-key add -add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\"apt-get -y updateapt-get -y install docker-ce docker-ce-cli containerd.io docker-compose-pluginmkdir -p /etc/docker/tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; \"dns\" : [ \"8.8.4.4\", \"8.8.8.8\", \"114.114.114.114\" ], \"registry-mirrors\": [ \"https://registry.docker-cn.com\", \"https://3laho3y3.mirror.aliyuncs.com\", \"http://hub-mirror.c.163.com\" ]&#125;EOF Starting: 1vagrant up Entering the container: 123vagrant docker-exec -it -- /bin/bash#Ordocker exec -it dockerid /bin/bash vagrant+virtualboxHow to get the box images? Search from https://app.vagrantup.com/boxes/search to get a certain box, like “ubuntu 22.04”, going to the details you can get the following command: 12vagrant init generic/ubuntu2004vagrant up Just running the command above, you will see the box url. Since the internet connection is very slow in China, you can interrupt the operation when you get the final download url. 123456#Centoshttps://vagrantcloud.com/centos/boxes/7/versions/2004.01/providers/virtualbox.box#Ubuntuhttps://vagrantcloud.com/bento/boxes/ubuntu-22.04/versions/202112.19.0/providers/virtualbox.box#Win10http://vagrantcloud.com/gusztavvargadr/boxes/windows-10/versions/2102.0.2204/providers/virtualbox.box UbuntuAdding box: 12sudo vagrant box add ubuntu22.04 box/ubuntu-22.04.boxsudo vagrant plugin install vagrant-disksize Vagrantfile: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# -*- mode: ruby -*-# vi: set ft=ruby :# All Vagrant configuration is done below. The \"2\" in Vagrant.configure# configures the configuration version (we support older styles for# backwards compatibility). Please don't change it unless you know what# you're doing.Vagrant.configure(\"2\") do |config| #config.vm.box = \"ubuntu22.04\" #config.vm.hostname = \"mydocker\" #config.vm.network \"private_network\", ip: \"192.168.10.9\"# #config.vm.synced_folder \"/data/docker/registry\", \"/docker/registry\" #config.vm.synced_folder \"/data/docker/works\", \"/docker/works\"# #config.vm.provider \"virtualbox\" do |vb| # #vb.gui = true # vb.memory = \"2048\" #end # #config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL # systemctl restart network #SHELL #config.vm.provision \"shell\", path: \"script.sh\" config.vm.define :node1 do |node1| node1.vm.box = \"ubuntu22.04\" node1.vm.hostname = \"node1\" node1.vm.network \"public_network\", ip: \"192.168.101.83\", netmask: \"255.255.255.0\", gateway: \"192.168.101.254\", bridge: \"enp2s0\" #excel01_prod.vm.network \"public_network\", bridge: \"enp0s31f6\", auto_config: false #node1.vm.synced_folder \"/home/dev/vagrant\", \"/data/vagrant\" node1.vm.provider \"virtualbox\" do |vb| #vb.gui = true vb.cpus = 4 vb.memory = \"4096\" end node1.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL #cp -a /vagrant/sources.list /etc/apt/sources.list #apt-get install sudo net-tools -y #sudo route del default gw 10.0.2.2 #ifconfig eth1 192.168.69.200 netmask 255.255.255.0 up #route add default gw 192.168.69.254 #If cannot ping from remote server, do the following command, but it won't be login by vagrant ssh: #ip link set eth0 down SHELL node1.vm.provision \"shell\" do |s| s.path = \"script.sh\" #s.args = [\"--bip=10.1.10.1/24\"] end #node1.vm.provision \"shell\", path: \"script.sh\" end config.vm.define :node2 do |node2| node2.vm.box = \"ubuntu22.04\" node2.vm.hostname = \"node2\" node2.vm.network \"public_network\", ip: \"192.168.101.84\", netmask: \"255.255.255.0\", gateway: \"192.168.101.254\", bridge: \"enp2s0\" #node1.vm.synced_folder \"/home/dev/vagrant\", \"/data/vagrant\" node2.vm.provider \"virtualbox\" do |vb| #vb.gui = true vb.cpus = 4 vb.memory = \"4096\" end node2.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL #cp -a /vagrant/sources.list /etc/apt/sources.list #apt-get install sudo net-tools -y #sudo route del default gw 10.0.2.2 #If cannot ping from remote server, do the following command, but it won't be login by vagrant ssh: #ip link set eth0 down SHELL node2.vm.provision \"shell\" do |s| s.path = \"script.sh\" #s.args = [\"--bip=10.1.20.1/24\"] end endend script.sh 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#!/bin/bash#http://www.360doc.com/content/14/1125/19/7044580_428024359.shtml#http://blog.csdn.net/54powerman/article/details/50684844#http://c.biancheng.net/cpp/view/2739.htmlecho \"scripting......\"cp /etc/apt/sources.list /etc/apt/sources.list.bak# cat &gt;&gt; /etc/apt/sources.list.d/aliyun.list &lt;&lt; EOFtee /etc/apt/sources.list &lt;&lt; EOFdeb http://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ jammy-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ jammy-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverseEOFapt-get update#apt-get install make g++ init inetutils-ping sudo jq iproute2 net-tools wget htop vim screen curl lsof lrzsz zip unzip expect openssh-server -yapt-get install init inetutils-ping sudo iptables psmisc jq iproute2 net-tools wget htop vim screen curl lsof lrzsz zip unzip expect openssh-server -y#LANG=\"en_US.UTF-8\"#sed -i 's;LANG=.*;LANG=\"zh_CN.UTF-8\";' /etc/locale.confsystemctl disable iptablessystemctl stop iptablessystemctl disable firewalldsystemctl stop firewalld#ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimetimedatectl set-timezone Asia/Shanghai#logined limitcat /etc/security/limits.conf|grep \"^root\" &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOFroot - nofile 100000root - nproc 100000* - nofile 100000* - nproc 100000EOFfi#systemd service limitcat /etc/systemd/system.conf|egrep '^DefaultLimitNOFILE' &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/systemd/system.conf &lt;&lt; EOFDefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000EOFfi#user service limitcat /etc/systemd/user.conf|egrep '^DefaultLimitNOFILE' &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/systemd/system.conf &lt;&lt; EOFDefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000EOFficat /etc/sysctl.conf|grep \"net.ipv4.ip_local_port_range\" &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.tcp_syncookies = 1net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 300net.ipv4.tcp_tw_reuse = 1net.ipv4.ip_local_port_range = 1024 65535net.ipv4.tcp_max_syn_backlog = 8192net.ipv4.tcp_max_tw_buckets = 500net.ipv4.ip_forward = 1fs.inotify.max_user_instances=1280fs.inotify.max_user_watches=655360vm.overcommit_memory=1fs.protected_regular=0EOFsysctl -pfisu - root -c \"ulimit -a\"#tee /etc/resolv.conf &lt;&lt; EOF#search myk8s.com#nameserver 114.114.114.114#nameserver 8.8.8.8#EOFsed -i 's;#PermitRootLogin.*;PermitRootLogin yes;g' /etc/ssh/sshd_config#systemctl enable ssh#systemctl restart ssh Starting: 1vagrant up CentOSAdding box: 1sudo vagrant box add centos7 box/CentOS-7-x86_64-Vagrant-2004_01.VirtualBox.box Vagrantfile 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# -*- mode: ruby -*-# vi: set ft=ruby :# All Vagrant configuration is done below. The \"2\" in Vagrant.configure# configures the configuration version (we support older styles for# backwards compatibility). Please don't change it unless you know what# you're doing.Vagrant.configure(\"2\") do |config| #config.vm.box = \"centos7\" #config.vm.hostname = \"mydocker\" #config.vm.network \"private_network\", ip: \"192.168.10.9\"# #config.vm.synced_folder \"/data/docker/registry\", \"/docker/registry\" #config.vm.synced_folder \"/data/docker/works\", \"/docker/works\"# #config.vm.provider \"virtualbox\" do |vb| # #vb.gui = true # vb.memory = \"2048\" #end # #config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL # systemctl restart network #SHELL #config.vm.provision \"shell\", path: \"script.sh\" config.vm.define :node1 do |node1| node1.vm.box = \"centos7\" node1.vm.hostname = \"node1\" node1.vm.network \"public_network\", ip: \"192.168.101.83\", netmask: \"255.255.255.0\", gateway: \"192.168.101.254\", bridge: \"enp2s0\" #node1.vm.synced_folder \"/home/dev/vagrant\", \"/data/vagrant\" node1.vm.provider \"virtualbox\" do |vb| #vb.gui = true vb.cpus = 4 vb.memory = \"4096\" end node1.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL yum -y install net-tools &gt; /dev/null #ifconfig eth1 192.168.101.83 netmask 255.255.255.0 up #route add default gw 192.168.101.254 sudo route del default gw 10.0.2.2 SHELL node1.vm.provision \"shell\" do |s| s.path = \"script.sh\" #s.args = [\"--bip=10.1.10.1/24\"] end #node1.vm.provision \"shell\", path: \"script.sh\" end config.vm.define :node2 do |node2| node2.vm.box = \"centos7\" node2.vm.hostname = \"node2\" node2.vm.network \"public_network\", ip: \"192.168.101.84\", netmask: \"255.255.255.0\", gateway: \"192.168.101.254\", bridge: \"enp2s0\" #node1.vm.synced_folder \"/home/dev/vagrant\", \"/data/vagrant\" node2.vm.provider \"virtualbox\" do |vb| #vb.gui = true vb.cpus = 4 vb.memory = \"4096\" end node2.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL yum -y install net-tools &gt; /dev/null #ifconfig eth1 192.168.101.84 netmask 255.255.255.0 up #route add default gw 192.168.101.254 sudo route del default gw 10.0.2.2 SHELL node2.vm.provision \"shell\" do |s| s.path = \"script.sh\" #s.args = [\"--bip=10.1.20.1/24\"] end endend script.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128#!/bin/sh#http://www.360doc.com/content/14/1125/19/7044580_428024359.shtml#http://blog.csdn.net/54powerman/article/details/50684844#http://c.biancheng.net/cpp/view/2739.htmlecho \"scripting......\"yum -y install net-tools wgetsed -i 's;SELINUX=.*;SELINUX=disabled;' /etc/selinux/configsetenforce 0getenforce#LANG=\"en_US.UTF-8\"#sed -i 's;LANG=.*;LANG=\"zh_CN.UTF-8\";' /etc/locale.confcat /etc/NetworkManager/NetworkManager.conf|grep \"dns=none\" &gt; /dev/nullif [[ $? != 0 ]]; then echo \"dns=none\" &gt;&gt; /etc/NetworkManager/NetworkManager.conf systemctl restart NetworkManager.servicefisystemctl disable iptablessystemctl stop iptablessystemctl disable firewalldsystemctl stop firewalld#ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimetimedatectl set-timezone Asia/Shanghai#logined limitcat /etc/security/limits.conf|grep 100000 &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF* - nofile 100000* - nproc 100000EOFfised -i 's;4096;100000;g' /etc/security/limits.d/20-nproc.conf#systemd service limitcat /etc/systemd/system.conf|egrep '^DefaultLimitCORE' &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/systemd/system.conf &lt;&lt; EOFDefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000EOFficat /etc/sysctl.conf|grep \"net.ipv4.ip_local_port_range\" &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 300net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.ip_local_port_range = 1024 65535net.ipv4.ip_forward = 1#k8snet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl -pfisu - root -c \"ulimit -a\"echo \"192.168.10.6 k8s-master192.168.10.7 k8s-node1192.168.10.8 k8s-node2\" &gt;&gt; /etc/hosts#tee /etc/resolv.conf &lt;&lt; EOF#search myk8s.com#nameserver 114.114.114.114#nameserver 8.8.8.8#EOF#yum -y install gcc kernel-develmv -f /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repowget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repowget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repoyum -y install epel-releasesudo mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backupsudo mv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backupcat &gt; /etc/yum.repos.d/epel.repo &lt;&lt; EOF[epel]name=Extra Packages for Enterprise Linux 7 - \\$basearchbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/\\$basearch#mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=\\$basearchfailovermethod=priorityenabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7[epel-debuginfo]name=Extra Packages for Enterprise Linux 7 - \\$basearch - Debugbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/\\$basearch/debug#mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-debug-7&amp;arch=\\$basearchfailovermethod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7gpgcheck=1[epel-source]name=Extra Packages for Enterprise Linux 7 - \\$basearch - Sourcebaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/SRPMS#mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-source-7&amp;arch=\\$basearchfailovermethod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7gpgcheck=1EOFyum clean allyum makecache#yum -y install createrepo rpm-sign rng-tools yum-utils yum -y install htop bind-utils bridge-utils ntpdate setuptool iptables system-config-securitylevel-tui system-config-network-tui \\ ntsysv net-tools lrzsz telnet lsof vim dos2unix unix2dos zip unzip \\ lsof openssl openssh-server openssh-clientssystemctl enable sshdsed -i 's;#PasswordAuthentication yes;PasswordAuthentication yes;g' /etc/ssh/sshd_configsystemctl restart sshd Starting: 1vagrant up Windows10Adding box: 1sudo vagrant box add win10 box/win10.box Vagrantfile 123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :# All Vagrant configuration is done below. The \"2\" in Vagrant.configure# configures the configuration version (we support older styles for# backwards compatibility). Please don't change it unless you know what# you're doing.Vagrant.configure(\"2\") do |config| config.vm.box = \"win10\" config.vm.hostname = \"node3\" config.vm.network \"public_network\", ip: \"192.168.101.85\", netmask: \"255.255.255.0\", gateway: \"192.168.101.254\", bridge: \"enp2s0\" config.vm.provider \"virtualbox\" do |vb| #vb.gui = true vb.cpus = 2 vb.memory = \"2048\" end config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL netsh advfirewall set allprofiles state off ROUTE ADD 0.0.0.0 MASK 0.0.0.0 192.168.101.254 METRIC 25 SHELLend Starting: 1vagrant up MacOSIt doesn’t work, reslove it later. Adding box: 1sudo vagrant box add macos box/macos-10.15.box Vagrantfile 123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :# All Vagrant configuration is done below. The \"2\" in Vagrant.configure# configures the configuration version (we support older styles for# backwards compatibility). Please don't change it unless you know what# you're doing.Vagrant.configure(\"2\") do |config| config.vm.box = \"macos\" config.vm.hostname = \"node4\" config.vm.network \"public_network\", ip: \"192.168.101.86\", netmask: \"255.255.255.0\", gateway: \"192.168.101.254\", bridge: \"enp2s0\" config.vm.provider \"virtualbox\" do |vb| #vb.gui = true vb.cpus = 2 vb.memory = \"2048\" end config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL #netsh advfirewall set allprofiles state off #ROUTE ADD 0.0.0.0 MASK 0.0.0.0 192.168.101.254 METRIC 25 SHELLend Starting: 1vagrant up Command Usage12345678910111213141516171819202122232425262728#cd vagrant#Remove boxsudo vagrant box remove centos7#Add boxsudo vagrant box add centos7 centos7-0.0.99.box#List boxsudo vagrant box list#sudo vagrant init centos7sudo vagrant up#sudo vagrant up node1sudo vagrant haltsudo vagrant reload#sudo vagrant destroysudo vagrant ssh#sudo vagrant ssh node1vagrant status#Export and use \"add box\" to import sudo vagrant package node1 --output node1.box#Snapshotsudo vagrant plugin install vagrant-vbox-snapshotsudo vagrant snapshot list node1#snapshot savesudo vagrant snapshot save node1 node1_snapshot#snapshot restoresudo vagrant snapshot restore node1 node1_snapshot Modifying the directory of VirtualBox: 123456789101112131415#Using root to handle:#https://www.jianshu.com/p/12cf1ecb224b#https://www.cnblogs.com/csliwei/p/5860005.htmlmv ~/.vagrant.d/ /data/vagrant/#vim /etc/profile.d/java.shexport VAGRANT_HOME='/data/vagrant/.vagrant.d'export VAGRANT_DISABLE_VBOXSYMLINKCREATE=1source /etc/profile#VBoxManage setproperty machinefolder /data/vagrant/sudo mkdir -p \"/data/vagrant/\"mv \"/root/VirtualBox VMs\" \"/data/vagrant/VirtualBox VMs\"sudo ln -s \"/data/vagrant/VirtualBox VMs\" \"/root/VirtualBox VMs\"#To relogin to take effect Resize Disk: 123456789vagrant plugin install vagrant-disksize#Edit the Vagrantfile:Vagrant.configure('2') do |config| ... config.vm.box = 'ubuntu/xenial64' config.disksize.size = '150GB' ...endvagrant halt &amp;&amp; vagrant up Note: this will not work with vagrant reload Nvidia Docker12345678910111213https://github.com/NVIDIA/nvidia-docker/issues/1034https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#nvidia-container-toolkitcurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.listsudo apt-get updatesudo apt-get install -y nvidia-container-toolkitdocker run --rm --name dave-nvidia --gpus all nvidia/cuda:12.3.2-devel-ubuntu22.04 nvidia-smi vim Vagrantfile: 123456789101112131415161718192021222324252627# -*- mode: ruby -*-# vi: set ft=ruby :# All Vagrant configuration is done below. The \"2\" in Vagrant.configure# configures the configuration version (we support older styles for# backwards compatibility). Please don't change it unless you know what# you're doing.Vagrant.configure(\"2\") do |config| config.vm.network :public_network, ip: \"192.168.109.50\", netmask: \"255.255.255.0\", bridge: \"eno1\", docker_network__gateway: \"192.168.109.254\" config.vm.provider \"docker\" do |d| #d.image = \"registry.zerofinance.net/library/ubuntu:22.04\" d.image = \"registry.zerofinance.net/library/cuda:12.3.2-devel-ubuntu22.04\" #d.build_dir = \".\" #d.create_args = [\"--hostname=config\", \"-v\", \"/data/fisco:/data/fisco\", \"-v\", \"/data/vagrant/docker/fisco/shell:/data/shell\"] d.create_args = [\"--cpus=40\", \"--memory=64g\", \"--hostname=ubuntu-analysis\", \"--gpus=all\",\"-v\", \"/data/containerd:/var/lib/containerd\", \"-v\", \"/works/app/container_apps:/works/app\", \"-v\", \"/data/container_datas:/data\"] d.privileged = true d.cmd = [\"/sbin/init\"] end config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL sudo route del default gw 172.17.0.1 sudo route add default gw 192.168.109.254 SHELLend Refrence https://www.ityoudao.cn/posts/vagrant-network/ http://sunyongfeng.com/201703/programmer/tools/vagrant https://cn.opensuse.org/Virtualbox_Network_Bridging","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"},{"name":"Vagrant","slug":"Vagrant","permalink":"http://blog.gcalls.cn/tags/Vagrant/"}]},{"title":"Loki Log System","slug":"Loki-Log-System","date":"2022-03-09T08:04:26.000Z","updated":"2024-08-02T05:39:00.967Z","comments":true,"path":"/2022/03/Loki-Log-System.html","link":"","permalink":"http://blog.gcalls.cn/2022/03/Loki-Log-System.html","excerpt":"This article recorded how to install and configure Log System based on Loki developing by Grafana.","text":"This article recorded how to install and configure Log System based on Loki developing by Grafana. kafkaKakfa k8sIt’s complex, please refer to Kakfa Config: kafka.zip Kakfa without zookeeper: https://learnk8s.io/kafka-ha-kubernetes#deploying-a-3-node-kafka-cluster-on-kubernetes https://stackoverflow.com/questions/73380791/kafka-kraft-replication-factor-of-3 https://github.com/IBM/kraft-mode-kafka-on-kubernetes Dockerfile: 123456789101112131415161718FROM openjdk:17-bullseyeENV KAFKA_VERSION=3.3.2ENV SCALA_VERSION=2.13ENV KAFKA_HOME=/opt/kafkaENV PATH=$&#123;PATH&#125;:$&#123;KAFKA_HOME&#125;/binLABEL name=\"kafka\" version=$&#123;KAFKA_VERSION&#125;RUN wget -O /tmp/kafka_$&#123;SCALA_VERSION&#125;-$&#123;KAFKA_VERSION&#125;.tgz https://downloads.apache.org/kafka/$&#123;KAFKA_VERSION&#125;/kafka_$&#123;SCALA_VERSION&#125;-$&#123;KAFKA_VERSION&#125;.tgz \\ &amp;&amp; tar xfz /tmp/kafka_$&#123;SCALA_VERSION&#125;-$&#123;KAFKA_VERSION&#125;.tgz -C /opt \\ &amp;&amp; rm /tmp/kafka_$&#123;SCALA_VERSION&#125;-$&#123;KAFKA_VERSION&#125;.tgz \\ &amp;&amp; ln -s /opt/kafka_$&#123;SCALA_VERSION&#125;-$&#123;KAFKA_VERSION&#125; $&#123;KAFKA_HOME&#125; \\ &amp;&amp; rm -rf /tmp/kafka_$&#123;SCALA_VERSION&#125;-$&#123;KAFKA_VERSION&#125;.tgzCOPY ./entrypoint.sh /RUN [\"chmod\", \"+x\", \"/entrypoint.sh\"]ENTRYPOINT [\"/entrypoint.sh\"] entrypoint.sh: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#!/bin/bash#NODE_ID=$&#123;HOSTNAME:6&#125;NODE_ID=$(hostname | sed s/.*-//)LISTENERS=\"PLAINTEXT://:9092,CONTROLLER://:9093\"#ADVERTISED_LISTENERS=\"PLAINTEXT://kafka-$NODE_ID.$SERVICE.$NAMESPACE.svc.cluster.local:9092\"#ADVERTISED_LISTENERS=\"PLAINTEXT://kafka-$NODE_ID.$SERVICE:9092\"if [[ \"$K8S_EXPOSE_BROKERS\" == \"true\" ]]; then ADVERTISED_LISTENERS=\"PLAINTEXT://$HOST_IP:9092\"else ADVERTISED_LISTENERS=\"PLAINTEXT://kafka-$NODE_ID.$SERVICE:9092\"fiCONTROLLER_QUORUM_VOTERS=\"\"for i in $( seq 0 $REPLICAS); do if [[ $i != $REPLICAS ]]; then CONTROLLER_QUORUM_VOTERS=\"$CONTROLLER_QUORUM_VOTERS$i@kafka-$i.$SERVICE:9093,\" else CONTROLLER_QUORUM_VOTERS=$&#123;CONTROLLER_QUORUM_VOTERS::-1&#125; fidonemkdir -p $SHARE_DIR/$NODE_ID if [[ ! -f \"$SHARE_DIR/cluster_id\" &amp;&amp; \"$NODE_ID\" = \"0\" ]]; then CLUSTER_ID=$(kafka-storage.sh random-uuid) echo $CLUSTER_ID &gt; $SHARE_DIR/cluster_idelse CLUSTER_ID=$(cat $SHARE_DIR/cluster_id)fi# sed -e \"s+^node.id=.*+node.id=$NODE_ID+\" \\# -e \"s+^controller.quorum.voters=.*+controller.quorum.voters=$CONTROLLER_QUORUM_VOTERS+\" \\# -e \"s+^listeners=.*+listeners=$LISTENERS+\" \\# -e \"s+^advertised.listeners=.*+advertised.listeners=$ADVERTISED_LISTENERS+\" \\# -e \"s+^log.dirs=.*+log.dirs=$SHARE_DIR/$NODE_ID+\" \\# /opt/kafka/config/kraft/server.properties &gt; server.properties.updated \\# &amp;&amp; mv server.properties.updated /opt/kafka/config/kraft/server.propertiessed -e \"s+^node.id=.*+node.id=$NODE_ID+\" \\-e \"s+^controller.quorum.voters=.*+controller.quorum.voters=$CONTROLLER_QUORUM_VOTERS+\" \\-e \"s+^listeners=.*+listeners=$LISTENERS+\" \\-e \"s+^advertised.listeners=.*+advertised.listeners=$ADVERTISED_LISTENERS+\" \\-e \"s+^log.dirs=.*+log.dirs=$SHARE_DIR/$NODE_ID+\" \\/opt/kafka/config/kraft/server.properties &gt; server.properties.updatedcat &lt;&lt;EOF &gt;&gt; server.properties.updateddefault.replication.factor=$&#123;DEFAULT_REPLICATION_FACTOR:=3&#125;min.insync.replicas=$&#123;DEFAULT_MIN_INSYNC_REPLICAS:=2&#125;offsets.topic.replication.factor=$&#123;DEFAULT_REPLICATION_FACTOR:=3&#125;transaction.state.log.replication.factor=$&#123;DEFAULT_REPLICATION_FACTOR:=3&#125;transaction.state.log.min.isr=$&#123;DEFAULT_MIN_INSYNC_REPLICAS:=2&#125;auto.create.topics.enable=$&#123;KAFKA_AUTO_CREATE_TOPICS_ENABLE:=true&#125;EOF#sed -i \"\\$aauto.create.topics.enable=$KAFKA_AUTO_CREATE_TOPICS_ENABLE\" server.properties.updatedmv server.properties.updated /opt/kafka/config/kraft/server.propertieskafka-storage.sh format -t $CLUSTER_ID -c /opt/kafka/config/kraft/server.propertiesexec kafka-server-start.sh /opt/kafka/config/kraft/server.properties docker building: 123docker build -t \"registry.zerofinance.net/xpayappimage/kafka:3.3.2\" .#docker login registry.zerofinance.netdocker push \"registry.zerofinance.net/xpayappimage/kafka:3.3.2\" kafka-kraft.yml: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122#部署 Service Headless，用于Kafka间相互通信apiVersion: v1kind: Servicemetadata: name: kafka-svc labels: app: kafkaspec: type: ClusterIP clusterIP: None ports: - name: '9092' port: 9092 protocol: TCP targetPort: 9092 - name: '9093' port: 9093 protocol: TCP targetPort: 9093 selector: app: kafka---#部署 Service，用于外部访问 KafkaapiVersion: v1kind: Servicemetadata: annotations: #service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: \"intranet\" #service.beta.kubernetes.io/alibaba-cloud-loadbalancer-vswitch-id: \"vsw-j6c8okcv03ah1uvu31tbm\" service.beta.kubernetes.io/alibaba-cloud-loadbalancer-id: \"lb-3nsgew8gt6lzmtzc0vn93\" service.beta.kubernetes.io/alibaba-cloud-loadbalancer-force-override-listeners: \"true\" name: kafka-broker labels: app: kafkaspec: type: LoadBalancer ports: - name: '9092' port: 9092 protocol: TCP targetPort: 9092 selector: app: kafka---apiVersion: apps/v1kind: StatefulSetmetadata: name: \"kafka\" labels: app: kafkaspec: selector: matchLabels: app: kafka serviceName: kafka-svc podManagementPolicy: \"OrderedReady\" replicas: 3 updateStrategy: type: \"RollingUpdate\" template: metadata: name: \"kafka\" labels: app: kafka spec: #securityContext: # fsGroup: 1001 nodeSelector: xpay-env: logs tolerations: - key: \"xpay-env\" operator: \"Equal\" value: \"logs\" effect: \"NoSchedule\" containers: - name: kafka image: \"registry.zerofinance.net/xpayappimage/kafka:3.3.2\" imagePullPolicy: \"Always\" #securityContext: # runAsNonRoot: true # runAsUser: 1001 env: - name: REPLICAS value: '3' - name: SERVICE value: kafka-svc - name: SHARE_DIR value: /mnt/kafka - name: CLUSTER_ID value: LelM2dIFQkiUFvXCEcqRWA - name: DEFAULT_REPLICATION_FACTOR value: '3' - name: DEFAULT_MIN_INSYNC_REPLICAS value: '2' ports: - containerPort: 9092 - containerPort: 9093 livenessProbe: tcpSocket: port: 9092 initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 2 readinessProbe: tcpSocket: port: 9092 initialDelaySeconds: 5 periodSeconds: 10 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 6 volumeMounts: - name: data mountPath: /mnt/kafka volumes: - name: data hostPath: path: \"/data/data/kafka-k8s\" kafka-ui.yml: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566apiVersion: v1kind: Servicemetadata: name: kafka-ui labels: app: kafka-uispec: #type: NodePort ports: - name: kafka port: 8080 targetPort: 8080 #nodePort: 30900 selector: app: kafka-ui---apiVersion: apps/v1kind: Deploymentmetadata: name: kafka-ui labels: app: kafka-uispec: replicas: 1 selector: matchLabels: app: kafka-ui template: metadata: labels: app: kafka-ui spec: containers: - name: kafka-ui image: provectuslabs/kafka-ui:latest imagePullPolicy: IfNotPresent ports: - name: kafka-ui containerPort: 8080 protocol: TCP env: - name: DYNAMIC_CONFIG_ENABLED value: \"true\" livenessProbe: httpGet: path: /actuator/health port: kafka-ui readinessProbe: httpGet: path: /actuator/info port: kafka-ui---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: kafka-uispec: tls: [] rules: - host: kafka-ui-test.zerofinance.net http: paths: - backend: serviceName: kafka-ui servicePort: 8080 Test: 12345678910&gt; kubectl -n zero-logs run kafka-client --rm -ti --image bitnami/kafka:3.1.0 -- bashkafka-topics.sh --create --bootstrap-server kafka-svc:9092 --replication-factor 2 --partitions 3 --topic testkafka-topics.sh --bootstrap-server kafka-svc:9092 --listkafka-console-producer.sh --broker-list kafka-svc:9092 --topic testkafka-console-consumer.sh --bootstrap-server kafka-svc:9092 --topic test --from-beginningkafka-topics.sh --create --bootstrap-server kafka-broker-test.zerofinance.net:9092 --replication-factor 2 --partitions 3 --topic testkafka-topics.sh --bootstrap-server kafka-broker-test.zerofinance.net:9092 --listkafka-console-producer.sh --broker-list kafka-broker-test.zerofinance.net:9092 --topic testkafka-console-consumer.sh --bootstrap-server kafka-broker-test.zerofinance.net:9092 --topic test --from-beginning Zookeeper https://www.qikqiak.com/k8strain/controller/statefulset/https://www.jianshu.com/p/f0b0fc3d192fhttps://itopic.org/kafka-in-k8s.htmlhttps://itopic.org/zookeeper-in-k8s.html Need to modify resource from: https://github.com/31z4/zookeeper-docker/tree/master/3.8.1 docker-entrypoint.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#!/bin/bashset -eZOO_MY_ID=$(($(hostname | sed s/.*-//) + 1))# Allow the container to be started with `--user`if [[ \"$1\" = 'zkServer.sh' &amp;&amp; \"$(id -u)\" = '0' ]]; then chown -R zookeeper \"$ZOO_DATA_DIR\" \"$ZOO_DATA_LOG_DIR\" \"$ZOO_LOG_DIR\" exec gosu zookeeper \"$0\" \"$@\"fimkdir -p $ZOO_DATA_DIR/$ZOO_MY_ID $ZOO_DATA_LOG_DIR/$ZOO_MY_ID# Generate the config only if it doesn't existif [[ ! -f \"$ZOO_CONF_DIR/zoo.cfg\" ]]; then CONFIG=\"$ZOO_CONF_DIR/zoo.cfg\" &#123; echo \"dataDir=$ZOO_DATA_DIR/$ZOO_MY_ID\" echo \"dataLogDir=$ZOO_DATA_LOG_DIR/$ZOO_MY_ID\" echo \"tickTime=$ZOO_TICK_TIME\" echo \"initLimit=$ZOO_INIT_LIMIT\" echo \"syncLimit=$ZOO_SYNC_LIMIT\" echo \"autopurge.snapRetainCount=$ZOO_AUTOPURGE_SNAPRETAINCOUNT\" echo \"autopurge.purgeInterval=$ZOO_AUTOPURGE_PURGEINTERVAL\" echo \"maxClientCnxns=$ZOO_MAX_CLIENT_CNXNS\" echo \"standaloneEnabled=$ZOO_STANDALONE_ENABLED\" echo \"admin.enableServer=$ZOO_ADMINSERVER_ENABLED\" &#125; &gt;&gt; \"$CONFIG\" if [[ -z $ZOO_SERVERS ]]; then ZOO_SERVERS=\"server.1=localhost:2888:3888;2181\" fi for server in $ZOO_SERVERS; do echo \"$server\" &gt;&gt; \"$CONFIG\" done if [[ -n $ZOO_4LW_COMMANDS_WHITELIST ]]; then echo \"4lw.commands.whitelist=$ZOO_4LW_COMMANDS_WHITELIST\" &gt;&gt; \"$CONFIG\" fi for cfg_extra_entry in $ZOO_CFG_EXTRA; do echo \"$cfg_extra_entry\" &gt;&gt; \"$CONFIG\" donefi# Write myid only if it doesn't existif [[ ! -f \"$ZOO_DATA_DIR/$ZOO_MY_ID/myid\" ]]; then echo \"$&#123;ZOO_MY_ID:-1&#125;\" &gt; \"$ZOO_DATA_DIR/$ZOO_MY_ID/myid\"fiexec \"$@\" builds images: 12docker build -t \"registry.zerofinance.net/xpayappimage/zookeeper:3.8.1\" .docker push registry.zerofinance.net/xpayappimage/zookeeper:3.8.1 LokiWhat’s the Grafana Loki? Loki is a log aggregation system designed to store and query logs from all your applications and infrastructure. Documents located in: https://grafana.com/docs/loki/latest/ Configurations of loki k8s: loki-k8s.zip Installationhttps://grafana.com/docs/loki/latest/fundamentals/overview/#overview There are losts of way to install Loki, here show it by docker. the other ways please refer to: https://grafana.com/docs/loki/latest/installation/ DockerIf you clients are distributed on individual machines, you can use docker: Installing: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#lokimkdir -p /data/loki/data/ /works/conf/loki/#Getting the loki id from the following command:#docker exec loki id#Like: uid=10001(loki) gid=10001(loki) groups=10001(loki)chown -R 10001.10001 /data/loki/data/ /works/conf/loki/#Creating the container:docker run -d --name loki --restart=always \\-v /etc/localtime:/etc/localtime:ro \\-v /data/loki/data:/loki/data \\-v /works/conf/loki:/mnt/config \\-p 3100:3100 -p 7946:7946 -p 9096:9096 grafana/loki:2.8.0 \\-config.file=/mnt/config/loki-config.yaml#grafana# docker run -d --name grafana \\# -v /etc/localtime:/etc/localtime:ro \\# -e \"GF_SMTP_ENABLED=true\" \\# -e \"GF_SMTP_HOST=smtp.example.com\" \\# -e \"GF_SMTP_USER=myuser\" \\# -e \"GF_SMTP_PASSWORD=mysecret\" \\# -p 3000:3000 grafana/grafana:latestchown -R 10001.10001 /data/grafana/docker run -d --name grafana9 --restart=always \\-v /etc/localtime:/etc/localtime:ro \\-v /data/grafana/:/var/lib/grafana \\-v /data/grafana/grafana.ini:/etc/grafana/grafana.ini \\--user 10001:10001 \\-p 3000:3000 grafana/grafana-oss:9.3.1#promtaildocker run -d --name promtail --restart=always \\-v /etc/localtime:/etc/localtime:ro \\-v /works/conf/promtail:/mnt/config \\-v /works/log:/works/log \\grafana/promtail:2.8.0 \\-config.file=/mnt/config/promtail-config.yaml \\-client.external-labels=hostname=$&#123;HOSTNAME&#125;docker run -d --name promtail-monitor --restart=always \\-v /etc/localtime:/etc/localtime:ro \\-v /works/conf/promtail/biz:/mnt/config \\grafana/promtail:2.8.0 \\-config.file=/mnt/config/promtail-config.yaml \\-client.external-labels=hostname=$&#123;HOSTNAME&#125;#fluent-bitdocker run --name fluent-bit --restart=always --network host -d \\-v /data/fluent-bit/:/fluent-bit/etc \\-v /works/log:/works/log \\fluent/fluent-bit:2.0.8#kafkadocker-compose up -d#Testdocker exec -it kafka-kafka9094-1 sh运行消费者,进行消息的监听kafka-console-consumer.sh --bootstrap-server 192.168.102.82:9092 --topic account --from-beginning#docker exec -it kafka_kafka9094_1 kafka-console-consumer.sh --bootstrap-server 192.168.101.82:9092 --topic dev --from-beginning#docker exec -it kafka_kafka9094_1 kafka-topics.sh --create --bootstrap-server 192.168.101.82:9092 --replication-factor 1 --partitions 3 --topic sandboxdocker exec -it kafka-kafka9094-1 sh打开一个新的ssh窗口,同样进入kafka的容器中,执行下面这条命令生产消息kafka-console-producer.sh --broker-list 192.168.102.82:9092 --topic account#alertmanagermkdir -p /data/alertmanager/chown -R 65534:65534 /data/alertmanager/docker run -d --name alertmanager --restart=always \\-v /data/alertmanager:/etc/alertmanager \\-p 9093:9093 prom/alertmanager:v0.24.0 \\--config.file=/etc/alertmanager/alertmanager-config.yaml \\--web.external-url=http://192.168.80.98:9093 \\--cluster.advertise-address=0.0.0.0:9093 \\--log.level=debug reload alertmanager: curl -XPOST http://am-test.zerofinance.net/-/reload Cluster Installation: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143#Loki(multiple machines):mkdir -p /data/loki/data/ /works/conf/loki/curl -O -L \"https://github.com/grafana/loki/releases/download/v2.8.0/loki-linux-amd64.zip\"# extract the binaryunzip \"loki-linux-amd64.zip\"# make sure it is executablechmod a+x \"loki-linux-amd64\"./loki-linux-amd64 -config.file=loki-config.yamlloki-config.yaml:auth_enabled: falseserver: log_level: info http_listen_port: 3100 grpc_listen_port: 9096 grpc_server_max_recv_msg_size: 1572864000 grpc_server_max_send_msg_size: 1572864000memberlist: join_members: [\"192.168.101.82\",\"192.168.80.196\"] dead_node_reclaim_time: 30s gossip_to_dead_nodes_time: 15s left_ingesters_timeout: 30s bind_addr: ['0.0.0.0'] bind_port: 7946 gossip_interval: 2s#https://grafana.com/blog/2021/02/16/the-essential-config-settings-you-should-use-so-you-wont-drop-logs-in-loki/#https://mpolinowski.github.io/docs/DevOps/Provisioning/2021-04-07--loki-prometheus-grafana/2021-04-07/ingester: lifecycler: join_after: 10s observe_period: 5s ring: replication_factor: 2 kvstore: store: memberlist # Duration to sleep before exiting to ensure metrics are scraped #final_sleep: 0s wal: enabled: true dir: /loki/data/wal # All chunks will be flushed when they hit this age, default is 1h max_chunk_age: 1h # Any chunk not receiving new logs in this time will be flushed chunk_idle_period: 1h # Must be greater than index read cache TTL if using an index cache (Default index read cache TTL is 5m) chunk_retain_period: 30s chunk_encoding: snappy # Loki will attempt to build chunks up to 1.5MB, flushing first if chunk_idle_period or max_chunk_age is reached first chunk_target_size: 1572864schema_config: configs: - from: 2023-04-01 object_store: aws schema: v11 store: boltdb-shipper index: period: 24h prefix: index_storage_config: boltdb_shipper: active_index_directory: /loki/data/boltdb-shipper-active cache_location: /loki/data/boltdb-shipper-cache # Can be increased for faster performance over longer query periods, uses more disk space cache_ttl: 24h shared_store: s3 aws: s3forcepathstyle: false bucketnames: loki-files endpoint: https://oss-cn-hongkong.aliyuncs.com access_key_id: LTA11111111111 secret_access_key: unseba111111111111111111 insecure: true index_queries_cache_config: redis: endpoint: r-111111111.redis.rds.aliyuncs.com:6379 password: 111111111 expiration: 1h chunk_store_config: chunk_cache_config: redis: endpoint: r-111111111.redis.rds.aliyuncs.com:6379 password: 111111111 expiration: 1h write_dedupe_cache_config: redis: endpoint: r-111111111.redis.rds.aliyuncs.com:6379 password: 111111111 expiration: 1hquery_range: results_cache: cache: redis: endpoint: r-111111111.redis.rds.aliyuncs.com:6379 password: 111111111 expiration: 1h cache_results: truecompactor: working_directory: /loki/data/boltdb-shipper-compactor shared_store: s3limits_config: ingestion_rate_mb: 2 ingestion_burst_size_mb: 4 max_streams_per_user: 0 max_global_streams_per_user: 0 enforce_metric_name: false reject_old_samples: true reject_old_samples_max_age: 168htable_manager: retention_deletes_enabled: true retention_period: 168h#https://grafana.com/docs/loki/latest/configuration/ruler: storage: type: s3 s3: s3forcepathstyle: false bucketnames: loki-files endpoint: https://oss-cn-hongkong.aliyuncs.com access_key_id: LTA11111111111 secret_access_key: unseba111111111111111111 insecure: true #rule_path: /loki/data/rules-temp alertmanager_url: http://192.168.101.82:9093 # How frequently to evaluate rules. evaluation_interval: 5s # How frequently to poll for rule changes. poll_interval: 5s ring: kvstore: store: memberlist enable_api: true Uninstalling: 12345678910111213#lokidocker rm -vf loki/bin/rm -fr /data/loki/data/*mkdir -p /data/loki/data/#docker exec loki id#uid=10001(loki) gid=10001(loki) groups=10001(loki)chown -R 10001.10001 /data/loki/data/#promtaildocker rm -vf promtail#grafana#docker rm -vf grafana Configuration: loki-config.yaml: For local file: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980auth_enabled: falseserver: log_level: error http_listen_port: 3100 grpc_listen_port: 9096 grpc_server_max_recv_msg_size: 1572864000 grpc_server_max_send_msg_size: 1572864000ingester: wal: enabled: true dir: /loki/data/wal replay_memory_ceiling: 10G lifecycler: address: 127.0.0.1 ring: kvstore: store: inmemory replication_factor: 1 heartbeat_timeout: 10m final_sleep: 0s chunk_idle_period: 1h max_chunk_age: 2h chunk_retain_period: 30s chunk_target_size: 1572864schema_config: configs: - from: 2023-04-01 store: boltdb-shipper object_store: filesystem schema: v11 index: prefix: index_ period: 24hstorage_config: boltdb_shipper: active_index_directory: /loki/data/index cache_location: /loki/data/boltdb-shipper-cache cache_ttl: 24h shared_store: filesystem filesystem: directory: /loki/data/chunkscompactor: working_directory: /loki/data/boltdb-shipper-compactor shared_store: filesystemlimits_config: ingestion_rate_mb: 50 ingestion_burst_size_mb: 100 max_streams_per_user: 0 max_global_streams_per_user: 0 enforce_metric_name: false reject_old_samples: true reject_old_samples_max_age: 168htable_manager: retention_deletes_enabled: true retention_period: 168h#https://grafana.com/docs/loki/latest/configuration/ruler: storage: type: local local: directory: /mnt/config/rules rule_path: /loki/data/rules-temp alertmanager_url: http://192.168.101.82:9093 # How frequently to evaluate rules. evaluation_interval: 5s # How frequently to poll for rule changes. poll_interval: 5s ring: kvstore: store: inmemory enable_api: true For Aliyun OSS: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182auth_enabled: falseserver: log_level: error http_listen_port: 3100 grpc_listen_port: 9096 grpc_server_max_recv_msg_size: 1572864000 grpc_server_max_send_msg_size: 1572864000ingester: wal: enabled: true dir: /loki/data/wal replay_memory_ceiling: 10G lifecycler: address: 127.0.0.1 ring: kvstore: store: inmemory replication_factor: 1 heartbeat_timeout: 10m final_sleep: 0s # chunk_idle_period: 1h # max_chunk_age: 2h # chunk_retain_period: 30s # chunk_target_size: 1572864schema_config: configs: - from: 2022-01-01 index: period: 24h prefix: index_ object_store: aws schema: v11 store: boltdb-shipperstorage_config: boltdb_shipper: active_index_directory: /loki/data/boltdb-shipper-active cache_location: /loki/data/boltdb-shipper-cache cache_ttl: 24h shared_store: s3 aws: s3forcepathstyle: false bucketnames: loki-files endpoint: https://oss-cn-shenzhen.aliyuncs.com access_key_id: xxxx secret_access_key: xxxx insecure: trueanalytics: reporting_enabled: falsecompactor: working_directory: /loki/data/boltdb-shipper-compactor shared_store: s3# table_manager:# retention_deletes_enabled: true# retention_period: 336hlimits_config: ingestion_rate_mb: 50 ingestion_burst_size_mb: 100 max_streams_per_user: 0 max_global_streams_per_user: 0 enforce_metric_name: false reject_old_samples: true reject_old_samples_max_age: 168hruler: storage: type: local local: directory: /mnt/config/rules rule_path: /loki/data/rules-temp alertmanager_url: http://192.168.102.82:9093 ring: kvstore: store: inmemory enable_api: true promtail-config.yaml: For log files: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950server: http_listen_port: 9080 grpc_listen_port: 0positions: filename: /tmp/positions.yamlclients: - url: http://192.168.80.196:3100/loki/api/v1/pushscrape_configs:# - job_name: service_log# file_sd_configs:# - files:# - ./config/*.yaml #从config目录下加载配置文件# refresh_interval: 1m- job_name: company-job pipeline_stages: - match: selector: '&#123;belongs=\"company\", filename=~\".*(?:error|tmlog).*\"&#125;' action: drop drop_counter_reason: promtail_noisy_error - match: selector: '&#123;belongs=\"company\"&#125;' stages: - regex: source: filename expression: \"^/works/log/(?P&lt;org&gt;.+?)/(?P&lt;env&gt;.+?)/(?P&lt;app_name&gt;.+?)/.+\\\\.log$\" - labels: org: env: app_name: - match: selector: '&#123;org=~\".+\"&#125;' stages: - multiline: firstline: '^\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125; \\d&#123;1,2&#125;:\\d&#123;2&#125;:\\d&#123;2&#125;' max_lines: 500 - regex: expression: \"^(?P&lt;time&gt;\\\\d&#123;4&#125;\\\\-\\\\d&#123;2&#125;\\\\-\\\\d&#123;2&#125; \\\\d&#123;1,2&#125;\\\\:\\\\d&#123;2&#125;\\\\:\\\\d&#123;2&#125;).*\" - timestamp: source: time format: '2006-01-02 15:04:05' location: Asia/Shanghai static_configs: - targets: - localhost labels: belongs: company __path__: /works/log/**/*.log Recoverying local files automatically: 123456789...scrape_configs:- job_name: service_log file_sd_configs: - files: - ./config/*.yaml #从config目录下加载配置文件 refresh_interval: 1m- job_name: company-job ... config/pipeline_stages.yaml 123456789- targets: - localhost labels: belongs: company __path__: /works/log/**/*.log #env: &#123;&#123;ENV&#125;&#125; #hostname: &#123;&#123;BINDIP&#125;&#125; # service_name: var-log-messages # log_type: var-log-messages For Kafka: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247server: http_listen_port: 9080 grpc_listen_port: 0positions: filename: /tmp/positions.yamlclients: - url: http://192.168.101.82:3100/loki/api/v1/pushscrape_configs: - job_name: kafka # file_sd_configs: # - files: # - /mnt/config/config/*.yaml # refresh_interval: 1m kafka: brokers: - 192.168.80.98:9092 topics: - dev - test - uat group_id: promtail_davetest labels: job: kafka relabel_configs: - action: replace source_labels: - __meta_kafka_topic target_label: topic - action: replace source_labels: - __meta_kafka_partition target_label: partition - action: replace source_labels: - __meta_kafka_group_id target_label: group - action: replace source_labels: - __meta_kafka_message_key target_label: message_key pipeline_stages: - match: selector: '&#123;job=\"kafka\"&#125;' stages: - json: expressions: log: log filename: filename - labels: filename: - match: selector: '&#123;job=\"kafka\", filename=~\".*(?:error|tmlog).*\"&#125;' action: drop drop_counter_reason: promtail_noisy_error - match: selector: '&#123;job=\"kafka\"&#125;' stages: - regex: source: filename expression: \"^/works/log/(?P&lt;org&gt;.+?)/(?P&lt;env&gt;.+?)/(?P&lt;app_name&gt;.+?)/.+\\\\.log$\" - labels: org: env: app_name: - output: source: log - match: selector: '&#123;org=~\".+\"&#125;' stages: #- multiline: # firstline: '^\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125; \\d&#123;1,2&#125;:\\d&#123;2&#125;:\\d&#123;2&#125;' # max_lines: 500 - regex: #expression: \"^(?P&lt;time&gt;\\\\d&#123;4&#125;\\\\-\\\\d&#123;2&#125;\\\\-\\\\d&#123;2&#125; \\\\d&#123;1,2&#125;\\\\:\\\\d&#123;2&#125;\\\\:\\\\d&#123;2&#125;).+?\\\\[bizKey=(?P&lt;biz_key&gt;.*?)\\\\,bizValue=(?P&lt;biz_value&gt;.*?)\\\\].*\" expression: \"^(?P&lt;time&gt;\\\\d&#123;4&#125;\\\\-\\\\d&#123;2&#125;\\\\-\\\\d&#123;2&#125; \\\\d&#123;1,2&#125;\\\\:\\\\d&#123;2&#125;\\\\:\\\\d&#123;2&#125;).*\" #- pack: # labels: # - time # - biz_key #- labels: # biz_key: # biz_value: - timestamp: source: times format: '2006-01-02 15:04:05' location: Asia/Shanghai - job_name: kakfka_payinfo # file_sd_configs: # - files: # - /mnt/config/config/*.yaml # refresh_interval: 1m kafka: brokers: - 192.168.80.98:9092 topics: - dev - test - uat group_id: promtail_payinfo_davetest labels: job: kakfka_payinfo relabel_configs: - action: replace source_labels: - __meta_kafka_topic target_label: topic - action: replace source_labels: - __meta_kafka_partition target_label: partition - action: replace source_labels: - __meta_kafka_group_id target_label: group - action: replace source_labels: - __meta_kafka_message_key target_label: message_key pipeline_stages: - match: selector: '&#123;job=\"kakfka_payinfo\"&#125; !~ \".*(PAYMENT_REFERENCE_LOG|CHECKOUT_PAYMENT_LOG).*\"' action: drop drop_counter_reason: promtail_noisy_error - match: selector: '&#123;job=\"kakfka_payinfo\"&#125;' stages: - json: expressions: log: log filename: filename - labels: filename: - match: selector: '&#123;job=\"kakfka_payinfo\"&#125;' stages: - regex: source: filename expression: \"^/works/log/(?P&lt;org&gt;.+?)/(?P&lt;env&gt;.+?)/(?P&lt;app_name&gt;.+?)/.+\\\\.log$\" - labels: org: env: app_name: payinfo: - output: source: log - match: selector: '&#123;job=\"kakfka_payinfo\", app_name=\"payment-server\"&#125; |~ \"PAYMENT_REFERENCE_LOG|CHECKOUT_PAYMENT_LOG\"' stages: #- multiline: # firstline: '^\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125; \\d&#123;1,2&#125;:\\d&#123;2&#125;:\\d&#123;2&#125;' # max_lines: 500 - regex: expression: \"^(?P&lt;time&gt;\\\\d&#123;4&#125;\\\\-\\\\d&#123;2&#125;\\\\-\\\\d&#123;2&#125; \\\\d&#123;1,2&#125;\\\\:\\\\d&#123;2&#125;\\\\:\\\\d&#123;2&#125;).*\" # - pack: # labels: # - time # - labels: # time: - timestamp: source: times format: '2006-01-02 15:04:05' location: Asia/Shanghai - job_name: kafka_monitor # file_sd_configs: # - files: # - /mnt/config/config/*.yaml # refresh_interval: 1m kafka: brokers: - 192.168.80.98:9092 topics: - dev - test - uat group_id: promtail_monitor_davetest labels: job: kafka_monitor relabel_configs: - action: replace source_labels: - __meta_kafka_topic target_label: topic - action: replace source_labels: - __meta_kafka_partition target_label: partition - action: replace source_labels: - __meta_kafka_group_id target_label: group - action: replace source_labels: - __meta_kafka_message_key target_label: message_key pipeline_stages: - match: selector: '&#123;job=\"kafka_monitor\"&#125;' stages: - json: expressions: log: log filename: filename - labels: filename: - match: selector: '&#123;job=\"kafka_monitor\", filename=~\".*(?:error|tmlog).*\"&#125;' action: drop drop_counter_reason: promtail_noisy_error - match: selector: '&#123;job=\"kafka_monitor\"&#125;' stages: - regex: source: filename expression: \"^/works/log/(?P&lt;org&gt;.+?)/(?P&lt;env&gt;.+?)/(?P&lt;app_name&gt;.+?)/.+\\\\.log$\" - labels: org: env: app_name: - output: source: log - match: selector: '&#123;org=~\".+\"&#125;' stages: #- multiline: # firstline: '^\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125; \\d&#123;1,2&#125;:\\d&#123;2&#125;:\\d&#123;2&#125;' # max_lines: 500 - regex: #[bizGroup=default, bizKey=TRANSFER_MONEY_FAILED, bizDesc=交易异常-订单超时-转账, bizValue=&#123;\"orderId\":\"orderNo1234567\",\"time\":\"2023-04-07 12:40:37\"&#125;] expression: \"^(?P&lt;time&gt;\\\\d&#123;4&#125;\\\\-\\\\d&#123;2&#125;\\\\-\\\\d&#123;2&#125; \\\\d&#123;1,2&#125;\\\\:\\\\d&#123;2&#125;\\\\:\\\\d&#123;2&#125;).+?\\\\[bizGroup=(?P&lt;biz_group&gt;.*?)\\\\,\\\\s*bizKey=(?P&lt;biz_key&gt;.*?)\\\\,\\\\s*bizDesc=(?P&lt;biz_desc&gt;.*?)\\\\,\\\\s*bizValue=(?P&lt;biz_value&gt;.*?)\\\\].*\" #- pack: # labels: # - time # - biz_key - labels: biz_group: biz_key: biz_desc: biz_value: - timestamp: source: times format: '2006-01-02 15:04:05' location: Asia/Shanghai /etc/grafana/grafana.ini 1234567891011121314151617181920...domain = logs.company.netroot_url = https://%(domain)s/[smtp]enabled = truehost = smtp.exmail.qq.com:465user = # If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"password = ;cert_file =;;key_file =;skip_verify = truefrom_address = from_name = Grafana;# EHLO identity in SMTP dialog (defaults to instance_name);;ehlo_identity = dashboard.example.com# SMTP startTLS policy (defaults to 'OpportunisticStartTLS')startTLS_policy = StartTLS fluent-bit/fluent-bit.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103[SERVICE] #Parsers_File parser.conf # 解析文件位置 Flush 5 # 5秒写入一次ES Daemon Off Log_Level warn parsers_file parsers_multiline.conf[INPUT] Name tail Tag dev path_key filename #read_from_head true multiline.parser multiline-regex Exclude_Path /works/log/*/dev/**/*-error.log Path /works/log/*/dev/**/*.log Buffer_Chunk_Size 4096KB Buffer_Max_Size 10240KB[INPUT] Name tail Tag test path_key filename #read_from_head true multiline.parser multiline-regex Exclude_Path /works/log/*/test/**/*-error.log Path /works/log/*/test/**/*.log Buffer_Chunk_Size 4096KB Buffer_Max_Size 10240KB[INPUT] Name tail Tag selftest path_key filename #read_from_head true multiline.parser multiline-regex Exclude_Path /works/log/*/selftest/**/*-error.log Path /works/log/*/selftest/**/*.log Buffer_Chunk_Size 4096KB Buffer_Max_Size 10240KB[INPUT] Name tail Tag sandbox path_key filename #read_from_head true multiline.parser multiline-regex Exclude_Path /works/log/*/sandbox/**/*-error.log Path /works/log/*/sandbox/**/*.log Buffer_Chunk_Size 4096KB Buffer_Max_Size 10240KB[INPUT] Name tail Tag uat path_key filename #read_from_head true multiline.parser multiline-regex Exclude_Path /works/log/*/uat/**/*-error.log Path /works/log/*/uat/**/*.log Buffer_Chunk_Size 4096KB Buffer_Max_Size 10240KB# [FILTER]# name parser# match *# key_name log# parser named-capture-test# [FILTER]# Name grep# Match configuration# #Exclude log level=INFO # Regex log =WARN[OUTPUT] Name kafka Match dev Brokers 192.168.80.98:9092 Topics dev[OUTPUT] Name kafka Match test Brokers 192.168.80.98:9092 Topics test[OUTPUT] Name kafka Match selftest Brokers 192.168.80.98:9092 Topics selftest[OUTPUT] Name kafka Match sandbox Brokers 192.168.80.98:9092 Topics sandbox[OUTPUT] Name kafka Match uat Brokers 192.168.80.98:9092 Topics uat fluent-bit/parsers_multiline.conf(if need) 12345678910111213141516171819202122232425[MULTILINE_PARSER] name multiline-regex type regex flush_timeout 3000 # # Regex rules for multiline parsing # --------------------------------- # # configuration hints: # # - first state always has the name: start_state # - every field in the rule must be inside double quotes # # rules | state name | regex pattern | next state # ------|---------------|-------------------------------------------- rule \"start_state\" \"/^\\d&#123;4&#125;\\-\\d&#123;2&#125;\\-\\d&#123;2&#125; \\d&#123;1,2&#125;\\:\\d&#123;2&#125;\\:\\d&#123;2&#125;.*/\" \"cont\" #rule \"cont\" \"/^([a-zA-Z]|\\s)+.*/\" \"cont\" rule \"cont\" \"/^(?!\\d&#123;4&#125;\\-\\d&#123;2&#125;\\-\\d&#123;2&#125;).*/\" \"cont\"#[PARSER]# Name named-capture-test# Format regex# Regex /^(?&lt;date&gt;\\d&#123;4&#125;\\-\\d&#123;2&#125;\\-\\d&#123;2&#125; \\d&#123;1,2&#125;\\:\\d&#123;2&#125;\\:\\d&#123;2&#125;)\\.\\d&#123;3&#125;\\s+(?&lt;message&gt;.*)/m Kakfa docker-compose.yml 12345678910111213141516171819202122232425#https://segmentfault.com/a/1190000021746086#https://github.com/wurstmeister/kafka-dockerversion: '2'services: zookeeper: image: wurstmeister/zookeeper volumes: - ./data:/data ports: - 2182:2181 kafka9094: image: wurstmeister/kafka ports: - 9092:9092 environment: KAFKA_BROKER_ID: 0 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://192.168.80.98:9092 KAFKA_CREATE_TOPICS: \"dev:3:1,test:3:1,selftest:3:1,uat:3:1,sandbox:3:1\" #kafka启动后初始化一个有3个partition(分区)1个副本名的topic KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092 volumes: - ./kafka-logs:/kafka depends_on: - zookeeper alertmanager-config.yaml https://yunlzheng.gitbook.io/prometheus-book/parti-prometheus-ji-chu/alert/alert-manager-config 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105global: smtp_smarthost: 'smtp.exmail.qq.com:25' smtp_from: 'xxx@xxx.com' smtp_auth_username: 'xxx@xxx.com' smtp_auth_password: 'xxx' smtp_require_tls: false #该参数定义了当Alertmanager持续多长时间未接收到告警后标记告警状态为resolved（已解决）。该参数的定义可能会影响到告警恢复通知的接收时间，读者可根据自己的实际场景进行定义，其默认值为5分钟 resolve_timeout: 10mtemplates:- '/etc/alertmanager/config/*-resolved.tmpl'- # 路由分组#https://blog.csdn.net/bluuusea/article/details/104619235#https://github.com/prometheus/alertmanager/blob/main/doc/examples/simple.yml#https://kebingzao.com/2022/11/29/prometheus-4-alertmanager/#https://zhuanlan.zhihu.com/p/63270049#https://blog.csdn.net/qq_37843943/article/details/120665690#https://blog.51cto.com/u_14205795/4561323route: # 该节点中的警报会按'env', 'alertname', 'biz_group', 'biz_key'做 Group，每个分组中最多每group_interval发送一条警报，同样的警报最多repeat_interval发送一次 # 分组规则，如果满足group_by中包含的标签，则这些报警会合并为一个通知发给receiver group_by: ['env', 'alertname', 'biz_group', 'biz_key'] # 设置等待时间，在此等待时间内如果接收到多个报警，则会合并成一个通知发送给receiver group_wait: 30s # 收到相同的分组告警通知的时间间隔(上下两组发送告警的间隔时间)，如果满足，则再会查找是否已经满足repeat_interval，如果满足，则会再次发送 # https://www.dianbanjiu.com/post/alertmanager-%E4%B8%AD%E4%B8%89%E4%B8%AA%E6%97%B6%E9%97%B4%E5%8F%82%E6%95%B0%E4%B8%8A%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/ # 再次发送时间在(group_interval+repeat_interval)左右 group_interval: 5m # 发送相同告警的时间间隔，如：4h，表示4小时内不会发送相同的报警 repeat_interval: 4h # 顶级路由配置的接收者（匹配不到子级路由，会使用根路由发送报警） receiver: 'emailreceivers' # 上面所有的属性都由所有子路由继承，并且可以在每个子路由上进行覆盖。 routes: #用于系统默认BaseException的异常 - receiver: emailreceivers group_by: ['env', 'alertname', 'biz_group', 'biz_key', 'biz_value'] group_wait: 10s group_interval: 1m repeat_interval: 3m #默认为false。false：配置到满足条件的子节点点后直接返回，true：匹配到子节点后还会继续遍历后续子节点 continue: false matchers: - biz_group=\"XPAY-SYSTEM-ERROR\" #用于业务告警 - receiver: alertmanager-webhook #- receiver: emailreceivers #- receiver: wecomreceivers group_by: ['env', 'alertname', 'biz_group', 'biz_key', 'biz_value'] group_wait: 0s group_interval: 1m repeat_interval: 2m #默认为false。false：配置到满足条件的子节点点后直接返回，true：匹配到子节点后还会继续遍历后续子节点 continue: false matchers: - biz_group!~\"XPAY-SYSTEM-ERROR\"#定义所有接收者receivers: - name: 'alertmanager-webhook' webhook_configs: - url: 'http://192.168.101.82:8088/alert' send_resolved: true - name: 'emailreceivers' email_configs: - to: 'xxx@xxx.com' html: '&#123;&#123; template \"email.to.html\" . &#125;&#125;' headers: #subject: ' &#123;&#123; .CommonAnnotations.summary &#125;&#125; &#123;&#123; if eq .Status \"firing\" &#125;&#125; DOWN &#123;&#123; else if eq .Status \"resolved\" &#125;&#125; UP &#123;&#123;end&#125;&#125;' subject: '[&#123;&#123; .Status &#125;&#125;]&#123;&#123; .CommonAnnotations.summary &#125;&#125;' #subject: '预警通知' send_resolved: true - name: 'wecomreceivers' wechat_configs: - send_resolved: true corp_id: 'xxx' to_user: 'SZ122' #to_party: 'SZ122 | SZ097' message: '&#123;&#123; template \"wechat.default.message\" . &#125;&#125;' agent_id: 'xxx' api_secret: 'xxx'# 抑制器配置：抑制是指当某以此告警发出后，可以停止重复发送由此告警引发的其他告警的机制# https://blog.csdn.net/qq_42883074/article/details/115544031#当我们前面已经有一个告警了，那么后面的告警规则在触发的时候会先翻一下前面的已经触发的告警，去查看是否有severity: 'critical'的标签#如果有了，那么去对比['alertname', 'biz_group', 'biz_key']标签是不是相同，如果是的话，#那么去查看一下自己准备发的告警里标签是否存在severity: 'warning'，如果是，就不告警了inhibit_rules: # 源标签警报触发时抑制含有目标标签的警报 - source_match: # 此处的抑制匹配一定在最上面的route中配置不然，会提示找不key。 # 前一个告警规则的标签 severity: 'critical' target_match: # 目标标签值正则匹配，可以是正则表达式如: \".*MySQL.*\" # 后面触发告警规则的标签 severity: 'High' # 确保这个配置下的标签内容相同才会抑制，也就是说警报中必须有这三个标签值才会被抑制。 equal: ['env', 'alertname', 'biz_group', 'biz_key'] loki/rules/fake/rules.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167groups: - name: 系统日志 rules: - alert: 系统日志-系统统一错误日志 #=: exactly equal #!=: not equal #=~: regex matches #!~: regex does not match #expr: sum by (env, app_name) (count_over_time(&#123;env=~\"dev\", app_name=\"account-server\"&#125;|unpack|bizKey=~\"\\\\w+\"[1m]) &gt;=1) #必须大于loki-config.yaml中的\"evaluation_interval: 3s\"的值 expr: sum by (env, biz_group, biz_key, biz_desc, biz_value) (count_over_time(&#123;biz_group=\"XPAY-SYSTEM-ERROR\",biz_key=~\".+\",biz_desc=~\".+\",biz_value=~\".+\"&#125;[10s]) &gt;=1) for: 1s labels: severity: High annotations: silenceResolved: \"true\" #emails: \"xxx@xxx.com\" #emailTemplate: \"email1\" #smsPhones: \"11111111111,22222222222\" #ttsPhones: \"11111111111,22222222222\" #wecomUrl: \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=111122223333\" #wecomTemplate: \"wecom1\" #只要捕捉到异常后，直接邮件通知相关对象通知一次 emails: \"xxx@xxx.com\" emailTemplate: \"email1\" summary: \"&#123;&#123; $labels.biz_desc &#125;&#125;\" description: \"&#123;&#123; $labels.biz_value &#125;&#125;\" count: \"&#123;&#123; $value &#125;&#125;\" - name: 支付 rules: - alert: 交易异常 expr: sum by (env, biz_group, biz_key, biz_desc, biz_value) (count_over_time(&#123;biz_group=\"XPAY-PAYMENT\",biz_key=~\".+\",biz_desc=~\".+\",biz_value=~\".+\"&#125;[5m]) &gt;=10) for: 1s labels: severity: High annotations: silenceResolved: \"true\" #在5min内出现10笔同类型的超时，则直接电话+企业微信通知相关对象按照0min，3min时间间隔通知二次 ttsPhones: \"11111111111,22222222222\" wecomUrl: \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=111122223333\" wecomTemplate: \"wecom1\" summary: \"&#123;&#123; $labels.biz_desc &#125;&#125;\" description: \"&#123;&#123; $labels.biz_value &#125;&#125;\" count: \"&#123;&#123; $value &#125;&#125;\" - name: 对账 rules: - alert: 外部对账失败 expr: sum by (env, biz_group, biz_key, biz_desc, biz_value) (count_over_time(&#123;biz_group=\"XPAY-RECONCILICATION\",biz_key=~\".+\",biz_desc=~\".+\",biz_value=~\".+\"&#125;[10s]) &gt;=1) for: 1s labels: severity: High annotations: silenceResolved: \"true\" #只要捕捉到异常后，直接企业微信通知相关对象通知一次 wecomUrl: \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=111122223333\" wecomTemplate: \"wecom1\" summary: \"&#123;&#123; $labels.biz_desc &#125;&#125;\" description: \"&#123;&#123; $labels.biz_value &#125;&#125;\" count: \"&#123;&#123; $value &#125;&#125;\" - alert: 内部对账失败 expr: sum by (env, biz_group, biz_key, biz_desc, biz_value) (count_over_time(&#123;biz_group=\"XPAY-RECONCILICATION\",biz_key=~\".+\",biz_desc=~\".+\",biz_value=~\".+\"&#125;[10s]) &gt;=1) for: 1s labels: severity: High annotations: silenceResolved: \"true\" #只要捕捉到异常后，直接企业微信通知相关对象通知一次 wecomUrl: \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=111122223333\" wecomTemplate: \"wecom1\" summary: \"&#123;&#123; $labels.biz_desc &#125;&#125;\" description: \"&#123;&#123; $labels.biz_value &#125;&#125;\" count: \"&#123;&#123; $value &#125;&#125;\" - name: 结算 rules: - alert: 渠道结算失败 expr: sum by (env, biz_group, biz_key, biz_desc, biz_value) (count_over_time(&#123;biz_group=\"XPAY-SETTLEMENT\",biz_key=~\".+\",biz_desc=~\".+\",biz_value=~\".+\"&#125;[10s]) &gt;=1) for: 1s labels: severity: High annotations: silenceResolved: \"true\" #只要捕捉到异常后，直接企业微信通知相关对象通知一次 wecomUrl: \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=111122223333\" wecomTemplate: \"wecom1\" summary: \"&#123;&#123; $labels.biz_desc &#125;&#125;\" description: \"&#123;&#123; $labels.biz_value &#125;&#125;\" count: \"&#123;&#123; $value &#125;&#125;\" - alert: 商户结算失败 expr: sum by (env, biz_group, biz_key, biz_desc, biz_value) (count_over_time(&#123;biz_group=\"XPAY-SETTLEMENT\",biz_key=~\".+\",biz_desc=~\".+\",biz_value=~\".+\"&#125;[10s]) &gt;=1) for: 1s labels: severity: High annotations: silenceResolved: \"true\" #只要捕捉到异常后，直接企业微信通知相关对象通知一次 wecomUrl: \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=111122223333\" wecomTemplate: \"wecom1\" summary: \"&#123;&#123; $labels.biz_desc &#125;&#125;\" description: \"&#123;&#123; $labels.biz_value &#125;&#125;\" count: \"&#123;&#123; $value &#125;&#125;\" - alert: 服务商结算失败 expr: sum by (env, biz_group, biz_key, biz_desc, biz_value) (count_over_time(&#123;biz_group=\"XPAY-SETTLEMENT\",biz_key=~\".+\",biz_desc=~\".+\",biz_value=~\".+\"&#125;[10s]) &gt;=1) for: 1s labels: severity: High annotations: silenceResolved: \"true\" #只要捕捉到异常后，直接企业微信通知相关对象通知一次 wecomUrl: \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=111122223333\" wecomTemplate: \"wecom1\" summary: \"&#123;&#123; $labels.biz_desc &#125;&#125;\" description: \"&#123;&#123; $labels.biz_value &#125;&#125;\" count: \"&#123;&#123; $value &#125;&#125;\" - name: 审批 rules: - alert: 审批失败 expr: sum by (env, biz_group, biz_key, biz_desc, biz_value) (count_over_time(&#123;biz_group=\"XPAY-APPROVAL\",biz_key=~\".+\",biz_desc=~\".+\",biz_value=~\".+\"&#125;[10s]) &gt;=1) for: 1s labels: severity: High annotations: silenceResolved: \"true\" #只要捕捉到异常后，直接企业微信通知相关对象通知一次 wecomUrl: \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=111122223333\" wecomTemplate: \"wecom1\" summary: \"&#123;&#123; $labels.biz_desc &#125;&#125;\" description: \"&#123;&#123; $labels.biz_value &#125;&#125;\" count: \"&#123;&#123; $value &#125;&#125;\" - name: 业务阻断 rules: - alert: 业务阻断 expr: sum by (env, biz_group, biz_key, biz_desc, biz_value) (count_over_time(&#123;biz_group=\"XPAY-BIZ-BLOCK\",biz_key=~\".+\",biz_desc=~\".+\",biz_value=~\".+\"&#125;[10s]) &gt;=1) for: 1s labels: severity: High annotations: silenceResolved: \"true\" #只要捕捉到异常后，直接电话+企业微信通知相关对象 wecomUrl: \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=111122223333\" wecomTemplate: \"wecom1\" summary: \"&#123;&#123; $labels.biz_desc &#125;&#125;\" description: \"&#123;&#123; $labels.biz_value &#125;&#125;\" count: \"&#123;&#123; $value &#125;&#125;\" - name: 会计 rules: - alert: 会计异常 expr: sum by (env, biz_group, biz_key, biz_desc, biz_value) (count_over_time(&#123;biz_group=\"XPAY-ACCOUNTING\",biz_key=~\".+\",biz_desc=~\".+\",biz_value=~\".+\"&#125;[10s]) &gt;=1) for: 1s labels: severity: High annotations: silenceResolved: \"true\" #只要捕捉到异常后，直接企业微信通知相关对象通知一次 wecomUrl: \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=111122223333\" wecomTemplate: \"wecom1\" summary: \"&#123;&#123; $labels.biz_desc &#125;&#125;\" description: \"&#123;&#123; $labels.biz_value &#125;&#125;\" count: \"&#123;&#123; $value &#125;&#125;\" config/WebCom-resolved.tmpl 12345678910111213141516171819202122232425262728293031323334&#123;&#123; define \"wechat.default.message\" &#125;&#125;&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;&#123;&#123;- if eq $index 0 -&#125;&#125;**********[&#123;&#123; $alert.Status &#125;&#125;]告警通知**********模块名称: &#123;&#123; $alert.Labels.alertname &#125;&#125;预警级别: &#123;&#123; $alert.Labels.severity &#125;&#125;故障业务: &#123;&#123; $alert.Labels.biz_key &#125;&#125;&#123;&#123;- end &#125;&#125;=====================预警名称: &#123;&#123; $alert.Annotations.summary &#125;&#125;预警警详情: &#123;&#123; $alert.Annotations.description &#125;&#125;错误次数: &#123;&#123; $alert.Annotations.count &#125;&#125;次故障时间: &#123;&#123; $alert.StartsAt.Local.Format \"2006-01-02 15:04:05\" &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;&#123;&#123;- if eq $index 0 -&#125;&#125;**********恢复通知**********模块名称: &#123;&#123; $alert.Labels.alertname &#125;&#125;预警级别: &#123;&#123; $alert.Labels.severity &#125;&#125;故障业务: &#123;&#123; $alert.Labels.biz_key &#125;&#125;&#123;&#123;- end &#125;&#125;=====================预警名称: &#123;&#123; $alert.Annotations.summary &#125;&#125;预警警详情: &#123;&#123; $alert.Annotations.description &#125;&#125;错误次数: &#123;&#123; $alert.Annotations.count &#125;&#125;次故障时间: &#123;&#123; $alert.StartsAt.Local.Format \"2006-01-02 15:04:05\" &#125;&#125;恢复时间: &#123;&#123; $alert.EndsAt.Local.Format \"2006-01-02 15:04:05\" &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125; config/WebCom.tmpl 1234567891011121314151617&#123;&#123; define \"wechat.default.message\" &#125;&#125;&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;&#123;&#123;- if eq $index 0 -&#125;&#125;**********[&#123;&#123; $alert.Status &#125;&#125;]告警通知**********模块名称: &#123;&#123; $alert.Labels.alertname &#125;&#125;预警级别: &#123;&#123; $alert.Labels.severity &#125;&#125;故障业务: &#123;&#123; $alert.Labels.biz_key &#125;&#125;&#123;&#123;- end &#125;&#125;=====================预警名称: &#123;&#123; $alert.Annotations.summary &#125;&#125;预警警详情: &#123;&#123; $alert.Annotations.description &#125;&#125;错误次数: &#123;&#123; $alert.Annotations.count &#125;&#125;次故障时间: &#123;&#123; $alert.StartsAt.Local.Format \"2006-01-02 15:04:05\" &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125; config/Email-resolved.tmpl 12345678910111213141516171819202122232425262728293031&#123;&#123; define \"email.to.html\" &#125;&#125;&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;========= ERROR ==========&lt;br&gt;模块名称: &#123;&#123; .Labels.alertname &#125;&#125;&lt;br&gt;预警级别: &#123;&#123; .Labels.severity &#125;&#125;&lt;br&gt;故障业务: &#123;&#123; .Labels.biz_key &#125;&#125;&lt;br&gt;=====================&lt;br/&gt;预警名称: &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt;预警详情: &#123;&#123; .Annotations.description &#125;&#125;&lt;br&gt;错误次数: &#123;&#123; .Annotations.count &#125;&#125;次&lt;br&gt;故障时间: &#123;&#123; .StartsAt.Format \"2020-01-02 15:04:05\"&#125;&#125; &lt;br&gt;========= END ==========&lt;br&gt;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;========= INFO ==========&lt;br&gt;模块名称: &#123;&#123; .Labels.alertname &#125;&#125;&lt;br&gt;预警级别: &#123;&#123; .Labels.severity &#125;&#125;&lt;br&gt;故障业务: &#123;&#123; .Labels.biz_key &#125;&#125;&lt;br&gt;=====================&lt;br/&gt;预警名称: &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt;预警详情: &#123;&#123; .Annotations.description &#125;&#125;&lt;br&gt;错误次数: &#123;&#123; .Annotations.count &#125;&#125;次&lt;br&gt;故障时间: &#123;&#123; .StartsAt.Format \"2020-01-02 15:04:05\"&#125;&#125; &lt;br&gt;恢复时间：&#123;&#123; .EndsAt.Format \"2006-01-02 15:04:05\" &#125;&#125;&lt;br&gt;========= END ==========&lt;br&gt;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125; config/Email.tmpl 123456789101112&#123;&#123; define \"email.to.html\" &#125;&#125;&#123;&#123; range .Alerts &#125;&#125;模块名称: &#123;&#123; .Labels.alertname &#125;&#125;&lt;br&gt;预警级别: &#123;&#123; .Labels.severity &#125;&#125;&lt;br&gt;故障业务: &#123;&#123; .Labels.biz_key &#125;&#125;&lt;br&gt;=====================&lt;br/&gt;预警名称: &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt;预警详情: &#123;&#123; .Annotations.description &#125;&#125;&lt;br&gt;错误次数: &#123;&#123; .Annotations.count &#125;&#125;次&lt;br&gt;故障时间: &#123;&#123; .StartsAt.Format \"2020-01-02 15:04:05\"&#125;&#125; &lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; 迁移grafana: https://www.jianshu.com/p/bc37e2fc15e7 Collecting typeThere is 2 way to collect logs: 12341: fluent bit---&gt;kafka---&gt;promtail---&gt;loki ---&gt;logstash---&gt;ES2: promtail---&gt;loki3: fluent bit---&gt;loki Recommended: fluent bit—&gt;kafka—&gt;promtail—&gt;loki docker-composeNot recommend, just for local study. 1234#depend on Linux: https://grafana.com/docs/loki/latest/installation/docker/#Install with Docker Composewget https://raw.githubusercontent.com/grafana/loki/v2.7.0/production/docker-compose.yaml -O docker-compose.yamldocker-compose -f docker-compose.yaml up The modified docker-compose.yaml as follows: docker-compose.yaml: 12345678910111213141516171819202122232425262728293031323334353637version: \"3\"networks: loki:services: loki: image: grafana/loki:2.4.1 volumes: - /etc/localtime:/etc/localtime:ro - .:/mnt/config ports: - \"3100:3100\" command: -config.file=/mnt/config/loki-config.yaml networks: - loki promtail: image: grafana/promtail:2.4.1 volumes: - /etc/localtime:/etc/localtime:ro - .:/mnt/config - /mnt/d/works/log:/works/log command: -config.file=/mnt/config/promtail-config.yaml -client.external-labels=hostname=$&#123;HOSTNAME&#125; networks: - loki grafana: image: grafana/grafana:latest volumes: - /etc/localtime:/etc/localtime:ro - /works/loki/docker/grafana.ini:/etc/grafana/grafana.ini command: -config.file=/mnt/config/promtail-config.yaml ports: - \"3000:3000\" networks: - loki Starting: 12345#Starting:docker-compose -f docker-compose.yaml up#Deleting:docker-compose -f docker-compose.yaml rm -vf When it’s started, you can check the status using the following url: 12http://localhost:3100/readyhttp://localhost:3100/metrics Grafana URL is: http://localhost:3000/, default account is admin/admin Grafana Configuration1234567891011121314env: Query: label_values(env)system: Query: label_values(&#123;belongs=\"company\", filename=~\".*$&#123;env&#125;.*\"&#125;, filename) Regex: /works\\/log\\/.+?\\/.+?\\/(.+?)\\/.*/hostname: Query: label_values(&#123;belongs=\"company\", filename=~\".*$&#123;env&#125;/$&#123;system&#125;.*\"&#125;, hostname)filename: Query: label_values(&#123;belongs=\"company\", filename=~\".*$&#123;env&#125;/$&#123;system&#125;.*\", filename!~\".*(?:error|tmlog).*\"&#125;, filename) Regex: /.*\\/(.+\\.log)/search: Type: Text boxLog browser: &#123;env=\"$&#123;env&#125;\", app_name=\"$&#123;system&#125;\", hostname=~\".*$&#123;hostname&#125;.*\", filename=~\".*$&#123;filename&#125;.*\"&#125;|~\"(?i)$search\" promtailPromtail is an agent which ships the contents of local logs to a private Grafana Loki instance or Grafana Cloud. It is usually deployed to every machine that has applications needed to be monitored. More details: https://grafana.com/docs/loki/latest/clients/promtail/ ConfigurationPromtail ConfigAll of the rule of collecting logs will be configured in the “promtail-config.yaml” 1234567891011121314151617181920212223242526272829scrape_configs:- job_name: saas-tenant-management-system pipeline_stages: - match: selector: &apos;&#123;app_name=&quot;saas-tenant-management-system&quot;&#125;&apos; stages: #https://grafana.com/docs/loki/latest/clients/promtail/stages/multiline/ #Working on collecting the multiline, like exception logs - multiline: firstline: &apos;^\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125; \\d&#123;1,2&#125;:\\d&#123;2&#125;:\\d&#123;2&#125;&apos; max_lines: 500 #https://grafana.com/docs/loki/latest/clients/promtail/stages/regex/ - regex: expression: &quot;^(?P&lt;timestamp&gt;\\\\d&#123;4&#125;\\\\-\\\\d&#123;2&#125;\\\\-\\\\d&#123;2&#125; \\\\d&#123;1,2&#125;\\\\:\\\\d&#123;2&#125;\\\\:\\\\d&#123;2&#125;)\\\\.\\\\d+ .*$&quot; #https://grafana.com/docs/loki/latest/fundamentals/labels/ #Working for the variables of searching. #- labels: # time: - timestamp: format: RFC3339Nano source: timestamp static_configs: - targets: - localhost labels: app_name: saas-tenant-management-system belongs: alphatimes __path__: /works/log/alphatimes/**/saas-tenant-management-system/**/*.log...... Notice: If multline fields are configured, it won’t appear in the lables of seaching. it conflict witch regex stage. For example: “loglevel” field configured in regex stage, if you the “loglevel” contain multiline, you wan to search by: “{loglevel=”ERROR”}”, it won’t display the multiline logs, just single log, althought “loglevel” contain multiline logs. Grafana ConfigGrafana 6.0 and more recent versions have built-in support for Grafana Loki. Use Grafana 6.3 or a more recent version to take advantage of LogQL functionality. Log into your Grafana instance. If this is your first time running Grafana, the username and password are both defaulted to admin. In Grafana, go to Configuration &gt; Data Sources via the cog icon on the left sidebar.Click the big + Add data source button.Choose Loki from the list. The http URL field should be the address of your Loki server. For example, when running locally or with Docker using port mapping, the address is likely http://localhost:3100. When running with docker-compose or Kubernetes, the address is likely http://loki:3100. To see the logs, click Explore on the sidebar, select the Loki datasource in the top-left dropdown, and then choose a log stream using the Log labels button. VariablesCreating a new dashboard named “Loki”(just first time), entering “dashboards settings”(gear icon): Env:1Query: label_values(env) System:12Query: label_values(&#123;belongs=&quot;company&quot;, filename=~&quot;.*$&#123;env&#125;.*&quot;&#125;, filename)Regex: /works\\/log\\/.+?\\/.+?\\/(.+?)\\/.*/ Hostname:1Query: label_values(&#123;belongs=&quot;company&quot;, filename=~&quot;.*$&#123;env&#125;/$&#123;system&#125;.*&quot;&#125;, hostname) Filename:12Query: label_values(&#123;belongs=&quot;company&quot;, filename=~&quot;.*$&#123;env&#125;/$&#123;system&#125;.*&quot;, filename!~&quot;.*(?:error|tmlog).*&quot;&#125;, filename)Regex: /.*\\/(.+\\.log)/ Search: Log Panel Log browser:1&#123;env=&quot;$&#123;env&#125;&quot;, app_name=&quot;$&#123;system&#125;&quot;, hostname=~&quot;.*$&#123;hostname&#125;.*&quot;, filename=~&quot;.*$&#123;filename&#125;.*&quot;&#125;|~&quot;(?i)$search&quot; KubernetesUsing helm to install loki on the k8s environment easyly, but recommend it by customed congratulation: Notice: It’s weird the way of Kubernetes couldn’t collection the logs completely, finally I used the docker to deploy. HemlInstalling heml: 123456789#Linux:#https://helm.sh/docs/intro/install/#from-scriptcurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3chmod 700 get_helm.sh./get_helm.sh#Windows:#Members of the Helm community have contributed a Helm package build to Chocolatey. This package is generally up to date. run as administrator:choco install kubernetes-helm Pulling repositories: 12345678910111213#https://grafana.com/docs/loki/latest/installation/helm/helm repo add grafana https://grafana.github.io/helm-chartshelm repo updatecd /works/lokikubectl create ns loki#helm pull grafana/loki-stackhelm pull grafana/grafanahelm pull grafana/lokihelm pull grafana/promtailtar zxvf promtail-3.11.0.tgz Configure: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123#Create PersistentVolumecat PersistentVolume.yaml apiVersion: v1kind: PersistentVolumemetadata: name: loki-pv-volume labels: type: localspec: storageClassName: loki capacity: storage: 30Gi accessModes: - ReadWriteOnce hostPath: path: \"/data/data/loki-data\"kubectl create -f PersistentVolume.yaml#PersistentVolumeClaim.yamlcat PersistentVolumeClaim.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: loki-pv-claim namespace: lokispec: storageClassName: loki accessModes: - ReadWriteOnce resources: requests: storage: 30Gikubectl create -f PersistentVolumeClaim.yaml#Notice: Since the persistence volume based on local system, make sure hostPath.path is shared with multiple machines that loki may be deployed on. or you can use nfs model of persistence volume.#Making sure k8s has privilege to access hostPath.path. refer to the following configuration:securityContext.fsGroup: 10001 runAsGroup: 10001 runAsNonRoot: true runAsUser: 10001#chown hostPath.path:sudo chown -R 10001:10001 /data/data/loki-data/#configure lokivim loki/values.yaml#Enable persistencepersistence: enabled: true accessModes: - ReadWriteOnce size: 30Gi annotations: &#123;&#125; # selector: # matchLabels: # app.kubernetes.io/name: loki # subPath: \"\" existingClaim: loki-pv-claim#configure promtailvim promtail/values.yamlextraArgs: - -client.external-labels=hostname=$(HOSTNAME)config: ... lokiAddress: http://loki:3100/loki/api/v1/pushextraVolumes: - name: journal hostPath: path: /var/log/journal - name: logs hostPath: path: /works/logextraVolumeMounts: - name: journal mountPath: /var/log/journal readOnly: true - name: logs mountPath: /works/log readOnly: true extraScrapeConfigs: | - job_name: company-job pipeline_stages: - match: selector: '&#123;belongs=\"company\", filename=~\".*(?:error|tmlog).*\"&#125;' action: drop drop_counter_reason: promtail_noisy_error - match: selector: '&#123;belongs=\"company\"&#125;' stages: - regex: source: filename expression: \"^/works/log/(?P&lt;org&gt;.+?)/(?P&lt;env&gt;.+?)/(?P&lt;app_name&gt;.+?)/.+\\\\.log$\" - labels: org: env: app_name: - match: selector: '&#123;org=~\".+\"&#125;' stages: - multiline: firstline: '^\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125; \\d&#123;1,2&#125;:\\d&#123;2&#125;:\\d&#123;2&#125;' max_lines: 500 - regex: expression: \"^(?P&lt;time&gt;\\\\d&#123;4&#125;\\\\-\\\\d&#123;2&#125;\\\\-\\\\d&#123;2&#125; \\\\d&#123;1,2&#125;\\\\:\\\\d&#123;2&#125;\\\\:\\\\d&#123;2&#125;).*\" - timestamp: source: time format: '2006-01-02 15:04:05' location: Asia/Shanghai static_configs: - targets: - localhost labels: belongs: company __path__: /works/log/**/*.log InstallationInstalling the revlant components: 123456789101112131415161718192021222324252627282930313233343536373839404142cd /works/loki/helm upgrade --install loki-grafana grafana/ -n lokihelm upgrade --install loki loki/ -n loki#helm upgrade --install promtail promtail/ --set \"loki.serviceName=loki\" -n loki#If deploying a individual machine, don't need \"--set\" parameter#kubectl get nodes --show-labels#helm upgrade --install promtail promtail/ -n loki --set nodeSelector.\"kubernetes\\.io/hostname\"=192.168.80.201#helm upgrade --install promtail promtail/ -n loki --set nodeSelector.\"kubernetes\\.io/hostname\"=k8s-master-clusterhelm upgrade --install promtail promtail/ -n loki#Waiting all of the pods are ready:kubectl get pods -n loki -w#grafana#Getting the grafana password using this:kubectl get secret --namespace loki loki-grafana -o jsonpath=\"&#123;.data.admin-password&#125;\" | base64 --decode ; echo#Exposed 3000 port outsize so that you can access it on your browser:kubectl port-forward --namespace loki service/loki-grafana 3000:80 --address 0.0.0.0URL: http://192.168.80.98:3000/Datasource: http://loki:3100/#Configure grafana#Loki Kubernetes Logsk8s logs dashboard:https://grafana.com/grafana/dashboards/15141#company Logsenv: Query: label_values(env)system: Query: label_values(&#123;belongs=\"company\", filename=~\".*$&#123;env&#125;.*\"&#125;, filename) Regex: /works\\/log\\/.+?\\/.+?\\/(.+?)\\/.*/hostname: Query: label_values(&#123;belongs=\"company\", filename=~\".*$&#123;env&#125;/$&#123;system&#125;.*\"&#125;, hostname)filename: Query: label_values(&#123;belongs=\"company\", filename=~\".*$&#123;env&#125;/$&#123;system&#125;.*\", filename!~\".*(?:error|tmlog).*\"&#125;, filename) Regex: /.*\\/(.+\\.log)/search: Type: Text boxLog browser: &#123;env=\"$&#123;env&#125;\", app_name=\"$&#123;system&#125;\", hostname=~\".*$&#123;hostname&#125;.*\", filename=~\".*$&#123;filename&#125;.*\"&#125;|~\"(?i)$search\" UninstallationUninstalling the revlant components: 123456789helm uninstall loki -n loki#kubectl -n loki delete pvc storage-loki-0#rm -fr /data/data/loki-data/loki/helm uninstall promtail -n lokihelm uninstall loki-grafana -n loki#kubectl -n loki get pvckubectl -n loki delete pvc loki-pv-claim#kubectl -n loki get pvkubectl delete pv loki-pv-volume Optimizehttps://grafana.com/blog/2021/02/16/the-essential-config-settings-you-should-use-so-you-wont-drop-logs-in-loki/ Troubleshootingerror: code = ResourceExhausted desc = trying to send message larger than max https://blog.csdn.net/qq_41980563/article/details/122186703 429 Too Many Requests Ingestion rate limit exceeded https://www.codeleading.com/article/71834625328/ Maximum active stream limit exceeded https://izsk.me/2021/03/18/Loki-Prombles/ https://www.bboy.app/2020/07/08/%E4%BD%BF%E7%94%A8loki%E8%BF%9B%E8%A1%8C%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86/ Loki: Bad Request. 400. invalid query, through https://zhuanlan.zhihu.com/p/457985915 https://blog.csdn.net/u010948569/article/details/108387324 insane quantity of files in chunks directory https://github.com/grafana/loki/issues/1258 Searching data slowly This reason may occur by some inappropriate configured labels, using the following command to diagnose: 1logcli series --analyze-labels &apos;&#123;app_name=&quot;hkcash-server&quot;&#125;&apos; You can this article to see how to avoid this issue: https://grafana.com/docs/loki/latest/best-practices/ Alarmmanagerhttps://www.bilibili.com/read/cv17329220 Configuration backupLoki Config: loki.zip AlertManager Config: AlertManager.zip Grafana Config: grafana.tgz Reference https://grafana.com/docs/loki/latest/getting-started/get-logs-into-loki/ https://grafana.com/docs/loki/latest/fundamentals/labels/ https://grafana.com/docs/loki/latest/logql/log_queries/ https://grafana.com/docs/loki/latest/clients/promtail/stages/multiline/ https://grafana.com/docs/loki/latest/clients/promtail/stages/regex/ https://github.com/google/re2/wiki/Syntax https://grafana.com/docs/grafana/latest/variables/ https://grafana.com/docs/grafana/latest/datasources/loki/ https://www.jianshu.com/p/474a5034a501 https://www.jianshu.com/p/259a1d656745 https://www.jianshu.com/p/672173b609f7 https://www.cnblogs.com/ssgeek/p/11584870.html https://grafana.com/docs/loki/latest/installation/helm/ https://blog.csdn.net/weixin_49366475/article/details/114384817 https://blog.luxifan.com/blog/post/lucifer/1.%E5%88%9D%E8%AF%86Loki-%E4%B8%80 https://blog.csdn.net/bluuusea/article/details/104619235 https://blog.51cto.com/u_14205795/4561323 https://www.cnblogs.com/punchlinux/p/17035742.html https://kebingzao.com/2022/11/29/prometheus-4-alertmanager/ https://blog.csdn.net/wang7531838/article/details/107809870 https://blog.51cto.com/u_12965094/2690336 https://blog.csdn.net/qq_42883074/article/details/115544031 https://blog.csdn.net/bluuusea/article/details/104619235 http://www.mydlq.club/article/126/ https://www.orchome.com/10106 https://blog.51cto.com/u_14320361/2461666 https://chenzhonzhou.github.io/2020/07/17/alertmanager-de-gao-jing-mo-ban/ https://blog.csdn.net/weixin_44911287/article/details/124149964 https://blog.csdn.net/easylife206/article/details/127581630 kubectl create ns zero-lokikubectl -n zero-loki create configmap –from-file configmap/loki-config-cluster.yaml loki-configkubectl -n zero-loki create configmap –from-file configmap/rules.yaml loki-rules kubectl -n zero-loki describe configmap loki-configkubectl -n zero-loki describe configmap loki-rules kubectl -n zero-loki apply -f zero-loki.yml kubectl -n zero-loki get po,svc -owide #kubectl -n zero-loki logs -f loki-cluster-57777d6d6-vkbc5 #kubectl -n zero-loki describe po loki-cluster-57777d6d6-8tfgd kubectl.exe -n zero-loki exec -it kafka-0 bashkafka-topics.sh –create –zookeeper “zookeeper-headless:2181” –replication-factor 2 –partitions 3 –topic uatkafka-console-producer.sh –broker-list “192.168.80.99:9192,192.168.80.99:9292,192.168.80.99:9392” –topic uatkafka-console-consumer.sh –bootstrap-server “192.168.80.99:9192,192.168.80.99:9292,192.168.80.99:9392” –topic uat –from-beginningkafka-topics.sh –list –zookeeper “zookeeper-headless:2181” kafka-run-class.sh kafka.tools.GetOffsetShell –broker-list “192.168.80.99:9192,192.168.80.99:9292,192.168.80.99:9392” –topic uat kubectl -n xpay-logs run -ti –rm centos-test –image=centos:7 –overrides=’{“spec”: { “nodeSelector”: {“xpay-env”: “logs”}}}’","categories":[{"name":"Log","slug":"Log","permalink":"http://blog.gcalls.cn/categories/Log/"}],"tags":[{"name":"Log","slug":"Log","permalink":"http://blog.gcalls.cn/tags/Log/"}]},{"title":"istio学习总结","slug":"istio学习总结","date":"2021-09-22T07:41:46.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2021/09/istio学习总结.html","link":"","permalink":"http://blog.gcalls.cn/2021/09/istio学习总结.html","excerpt":"istio学习总结","text":"istio学习总结 安装不使用micro.k8s中的istio插件。 https://istio.io/latest/docs/setup/getting-started/ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#https://istio.io/latest/zh/docs/setup/install/istioctl/curl -L https://istio.io/downloadIstio | sh -cd istio-1.11.4#https://istio.io/latest/zh/docs/setup/getting-started/vim /etc/profile.d/istio.shexport PATH=\"$PATH:/works/istio/istio-1.11.4/bin\". /etc/profileistioctl install --set profile=demo -ykubectl label namespace default istio-injection=enabled#Example#https://blog.frognew.com/2021/07/learning-istio-1.10-01.htmlkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yamlkubectl get pods -wkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yamlkubectl get gatewayistioctl analyze✔ No validation issues found when analyzing namespace: default.kubectl exec \"$(kubectl get pod -l app=ratings -o jsonpath='&#123;.items[0].metadata.name&#125;')\" -c ratings -- curl -s productpage:9080/productpage | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"kubectl get svc istio-ingressgateway -n istio-systemexport INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='&#123;.items[0].status.hostIP&#125;')#export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name==\"http2\")].port&#125;')export INGRESS_PORT=$(kubectl get svc istio-ingressgateway -n istio-system|awk '&#123;print $5&#125;'|sed '1d'|awk -F ',' '&#123;print $2&#125;'|awk -F '/' '&#123;print $1&#125;'|awk -F ':' '&#123;print $2&#125;')export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORTecho \"$GATEWAY_URL\"echo \"http://$GATEWAY_URL/productpage\"#查看仪表板kubectl apply -f samples/addonskubectl rollout status deployment/kiali -n istio-systemistioctl dashboard --address 0.0.0.0 kiali#istioctl dashboard kialihttp://192.168.95.234:20001/kiali#Mocking some traffic data:for i in $(seq 1 100); do curl -s -o /dev/null \"http://$GATEWAY_URL/productpage\"; done#jinjecting sidecar with existing namespacehttps://istio.io/latest/docs/setup/additional-setup/sidecar-injection/kubectl label zerofinance-dev istio-injection=enabled#Injection occurs at pod creation time. Kill the running pod and verify a new pod is created with the injected sidecar. The original pod has 1/1 READY containers, and the pod with injected sidecar has 2/2 READY containers.kubectl -n zerofinance-dev delete po --all#Disable injection for the default namespace and verify new pods are created without the sidecar.kubectl label zerofinance-dev default istio-injection- 卸载12345678910111213141516171819#卸载applicationkubectl delete -f samples/addonsistioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f -kubectl delete namespace istio-systemkubectl label namespace default istio-injection-#or https://istio.io/latest/zh/docs/examples/bookinfo/#cleanupsamples/bookinfo/platform/kube/cleanup.shkubectl get virtualservices #-- there should be no virtual serviceskubectl get destinationrules #-- there should be no destination ruleskubectl get gateway #-- there should be no gatewaykubectl get pods #-- the Bookinfo pods should be deletedkubectl get se #-- ServerEntity#卸载istio#https://istio.io/latest/zh/docs/setup/install/istioctl/#uninstall#可选的 --purge 参数将删除所有 Istio 资源，包括可能被其他 Istio 控制平面共享的、集群范围的资源。istioctl x uninstall --purge bookinfo应用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329#https://blog.frognew.com/2021/07/learning-istio-1.10-03.htmlkubectl get deploy -l app=reviewskubectl get pod -l app=reviews#将请求路由到固定版本的微服务上kubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml#上面reviews微服务的destinationrule的subsets包含v1, v2, v3。#接下来使用下面的命令创建VirtualService:kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml#reviews微服务的VirtualService配置了路由的目标只到v1 subset，其他服务在请求reviews时将只被路由到reviews v1。 istio配置的传播是最终一致性的，等待几秒钟后，多次刷新/prodctpage页，确认书籍评不再不包含评分信息，说明请求一直被路由到v1版本的reviews服务。#基于用户身份进行请求路由kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml#请求头中end-user=jason的路由目标是reviews-v2，其他是reviews-v1。测试以jason登录(密码任意输入)，登录后多次刷新/productpage页面，书籍的评论都一直包含黑星评级，而以其他用户名登录或匿名不登录访问时书籍评论都不包含星级评论。 这说明已经成功配置按用户身份的路由控制。#还原kubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml#使用Istio对服务进行流量管理之故障注入#https://blog.frognew.com/2021/07/learning-istio-1.10-04.html#流量管理API资源对象VirtualService和DestinationRule#https://blog.frognew.com/2021/07/learning-istio-1.10-05.html#使用Istio对服务进行流量管理之流量转移#https://blog.frognew.com/2021/07/learning-istio-1.10-06.html#使用Istio Gateway将外部流量接入到服务网格#https://blog.frognew.com/2021/07/learning-istio-1.10-07.html#https://www.cnblogs.com/boshen-hzb/p/10679863.html#https://jonathangazeley.com/2020/12/30/load-balancing-ingress-with-metallb-on-microk8s/上面istio-ingressgateway的Service的Type是LoadBalancer, 它的EXTERNAL-IP处于pending状态， 这是因为我们目前的环境并没有可用于Istio Ingress Gateway外部的负载均衡器，为了使得可以从外部访问， 通过修改istio-ingressgateway这个Service的externalIps:kubectl edit service istio-ingressgateway -n istio-systemspec: externalIPs: - 192.168.95.234samples/bookinfo/networking/bookinfo-gateway.yaml#Istio流量管理之TCP流量转移#https://blog.frognew.com/2021/07/learning-istio-1.10-08.htmlkubectl create namespace istio-io-tcp-traffic-shiftingkubectl label namespace istio-io-tcp-traffic-shifting istio-injection=enabledkubectl apply -f samples/tcp-echo/tcp-echo-services.yaml -n istio-io-tcp-traffic-shiftingkubectl get svc istio-ingressgateway -n istio-system -o yaml#testfor i in &#123;1..10&#125;; do \\sh -c \"(date; sleep 1) | nc 192.168.95.234 31400\"; \\done10个请求中的输出结果都是one说明请求流量被100%的路由到了v1版本的服务。接下来通过以下命令，将20%流量从tcp-echo:v1迁移到tcp-echo:v2:kubectl apply -f samples/tcp-echo/tcp-echo-20-v2.yaml -n istio-io-tcp-traffic-shifting#testfor i in &#123;1..10&#125;; do \\sh -c \"(date; sleep 1) | nc 192.168.95.234 31400\"; \\done10个请求中差不多有20%的请求被路由到了版本v2的服务商。#Istio流量管理之设置请求超时和熔断#https://blog.frognew.com/2021/07/learning-istio-1.10-09.html#Istio流量管理之流量镜像#https://blog.frognew.com/2021/07/learning-istio-1.10-10.html#Istio流量管理之访问外部服务的三种方法#https://blog.frognew.com/2021/07/learning-istio-1.10-11.html使用ServiceEntry API资源对象将一个可访问的外部服务注册到服务网格中(推荐)kubectl get istiooperator installed-state -n istio-system -o jsonpath='&#123;.spec.meshConfig.outboundTrafficPolicy.mode&#125;'如果显示为空，则表示为ALLOW_ANY选项值为ALLOW_ANY，sidecar将允许调用未知的服务，调整为：REGISTRY_ONLYistioctl install --set profile=demo -y --set meshConfig.outboundTrafficPolicy.mode=REGISTRY_ONLY#test namespace可选kubectl apply -f - &lt;&lt;EOFapiVersion: networking.istio.io/v1alpha3kind: ServiceEntrymetadata: name: external-svc namespace: istio-systemspec: hosts: - \"baidu.com\" - \"www.baidu.com\" ports: - name: http number: 80 protocol: HTTP - name: https number: 443 protocol: HTTPS location: MESH_EXTERNALEOFkubectl -n istio-system get sekubectl run curl --image=radial/busyboxplus:curl -it#kubectl attach curl -c curl -i -tkubectl exec -it curl -c curl -- curl http://www.baidu.comkubectl exec -it curl -c curl -- curl https://www.baidu.comkubectl apply -f - &lt;&lt;EOFapiVersion: networking.istio.io/v1alpha3kind: ServiceEntrymetadata: name: httpbin-extspec: hosts: - httpbin.org ports: - number: 80 name: http protocol: HTTP - number: 443 name: https protocol: HTTPS resolution: DNS location: MESH_EXTERNALEOFkubectl run curl --image=radial/busyboxplus:curl -it#kubectl attach curl -c curl -i -tkubectl exec -it curl -c curl -- curl http://httpbin.org/headerskubectl exec -it curl -c curl -- curl -sSI https://httpbin.org/headers#Istio Egress Gateway及其使用场景#https://blog.frognew.com/2021/07/learning-istio-1.10-12.htmlkubectl exec -it curl -c curl -- curl -sSI https://httpbin.org/headers | grep \"HTTP/\"为httpbin.org的443端口创建Egress Gateway。并为指向Egress Gateway的流量创建一个目标规则DestinationRule。kubectl apply -f - &lt;&lt;EOFapiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: istio-egressgatewayspec: selector: istio: egressgateway servers: - port: number: 443 name: tls protocol: TLS hosts: - httpbin.org tls: mode: PASSTHROUGH---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: egressgateway-for-httpbin-extspec: host: istio-egressgateway.istio-system.svc.cluster.local subsets: - name: httpbin-extEOF创建一个虚拟服务VirtualService，将流量从 sidecar 引导至 egress gateway，再从 egress gateway 引导至外部服务：kubectl apply -f - &lt;&lt;EOFapiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: direct-httpbin-ext-through-egress-gatewayspec: hosts: - httpbin.org gateways: - mesh - istio-egressgateway tls: - match: - gateways: - mesh port: 443 sniHosts: - httpbin.org route: - destination: host: istio-egressgateway.istio-system.svc.cluster.local subset: httpbin-ext port: number: 443 - match: - gateways: - istio-egressgateway port: 443 sniHosts: - httpbin.org route: - destination: host: httpbin.org port: number: 443 weight: 100EOFkubectl exec -it curl -c curl -- curl -sSI https://httpbin.org/headers | grep \"HTTP/\"检查istio-egressgateway代理的日志：kubectl logs -l istio=egressgateway -n istio-system#使用认证策略设置双向TLS和基本的终端用户认证#https://blog.frognew.com/2021/07/learning-istio-1.10-13.htmlkubectl create ns fookubectl label namespace foo istio-injection=enabledkubectl create ns barkubectl label namespace bar istio-injection=enabledkubectl apply -f samples/httpbin/httpbin.yaml -n fookubectl run curl --image=radial/busyboxplus:curl -it -n fookubectl apply -f samples/httpbin/httpbin.yaml -n barkubectl run curl --image=radial/busyboxplus:curl -it -n barkubectl create ns legacykubectl apply -f samples/httpbin/httpbin.yaml -n legacykubectl run curl --image=radial/busyboxplus:curl -it -n legacy#Testfor from in \"foo\" \"bar\" \"legacy\"; do for to in \"foo\" \"bar\" \"legacy\"; do kubectl exec curl -c curl -n $&#123;from&#125; -- curl -s \"http://httpbin.$&#123;to&#125;:8000/ip\" -s -o /dev/null -w \"curl.$&#123;from&#125; to httpbin.$&#123;to&#125;: %&#123;http_code&#125;\\n\"; done; donekubectl get peerauthentication --all-namespacesNo resources found#配置全局严格模式启用istio双向TLS#前面学习了具有sidecar代理的工作负载之间将自动启用双向TLS认证，但工作负载仍然可以接收plain-text流量。可以通过将整个服务网格的对等认证策略(PeerAuthentication)设置为STRICT模式，以阻止整个网格的服务以非双向TLS通信。 如下所示，全局的对等认证策略是没有selector的，且它必须位于安装istio的根命名空间内(如istio-system)。kubectl apply -f - &lt;&lt;EOFapiVersion: security.istio.io/v1beta1kind: PeerAuthenticationmetadata: name: \"default\" namespace: \"istio-system\"spec: mtls: mode: STRICTEOF#Testfor from in \"foo\" \"bar\" \"legacy\"; do for to in \"foo\" \"bar\" \"legacy\"; do kubectl exec curl -c curl -n $&#123;from&#125; -- curl -s \"http://httpbin.$&#123;to&#125;:8000/ip\" -s -o /dev/null -w \"curl.$&#123;from&#125; to httpbin.$&#123;to&#125;: %&#123;http_code&#125;\\n\"; done; done会发现没有sidecar代理的curl.legacy到有sidecar代理的httpbin.foo和httpbin.bar的请求将会失败，因为全局的对等认证策略是严格模式，要求客户端与httpbin.foo和httpbin.bar之间的流量必须是双向TLS的。#终端用户认证kubectl -n foo exec -it curl -- curl http://httpbin.foo.svc.cluster.local:8000/headerskubectl apply -f - &lt;&lt;EOFapiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: httpbin-gatewayspec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \"httpbin.example.com\"---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbinspec: hosts: - \"httpbin.example.com\" gateways: - httpbin-gateway http: - route: - destination: host: httpbin.foo.svc.cluster.local port: number: 8000EOF配置要求必须提供有效的token，这样没有token的请求也会被拒绝:kubectl apply -f - &lt;&lt;EOFapiVersion: security.istio.io/v1beta1kind: AuthorizationPolicymetadata: name: \"frontend-ingress\" namespace: istio-systemspec: selector: matchLabels: istio: ingressgateway action: DENY rules: - from: - source: notRequestPrincipals: [\"*\"] to: - operation: hosts: [\"httpbin.example.com\"]EOFcurl http://httpbin.example.com/headers -s -o /dev/null -w \"%&#123;http_code&#125;\\n\"403TOKEN=$(curl https://raw.githubusercontent.com/istio/istio/release-1.10/security/tools/jwt/samples/demo.jwt -s)curl --header \"Authorization: Bearer $TOKEN\" http://httpbin.example.com/headers -s -o /dev/null -w \"%&#123;http_code&#125;\\n\"200配置按路由提供有效token，路径指host、path、或者method:kubectl apply -f - &lt;&lt;EOFapiVersion: security.istio.io/v1beta1kind: AuthorizationPolicymetadata: name: \"frontend-ingress\" namespace: istio-systemspec: selector: matchLabels: istio: ingressgateway action: DENY rules: - from: - source: notRequestPrincipals: [\"*\"] to: - operation: paths: [\"/headers\"] hosts: [\"httpbin.example.com\"]EOF不提供token访问https://httpbin.example.com/headers将被拒绝，但可以访问其他路径：curl http://httpbin.example.com/headers -s -o /dev/null -w \"%&#123;http_code&#125;\\n\"403curl http://httpbin.example.com/ip -s -o /dev/null -w \"%&#123;http_code&#125;\\n\"200curl --header \"Authorization: Bearer $TOKEN\" http://httpbin.example.com/headers -s -o /dev/null -w \"%&#123;http_code&#125;\\n\"200 使用 https://blog.frognew.com/2021/07/learning-istio-1.10-01.html https://blog.frognew.com/2021/07/learning-istio-1.10-03.html http://jartto.wang/2020/07/29/istio-1/ https://developer.51cto.com/art/202101/641511.htm https://www.cxymm.net/article/weixin_43188769/110915810 https://jimmysong.io/blog/what-is-a-service-mesh/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/tags/kubernetes/"}]},{"title":"iptables","slug":"iptables","date":"2021-03-02T09:36:00.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2021/03/iptables.html","link":"","permalink":"http://blog.gcalls.cn/2021/03/iptables.html","excerpt":"记录一下iptables相关的命令。","text":"记录一下iptables相关的命令。 查看12345678910111213141516171819202122232425262728293031#(n：不进行host反查，v：详细信息)iptables -nvLiptables -nL --line-number#Only for INPUTiptables -nL INPUT --line-number#Delete ruleiptables -D INPUT 2#Exampleiptables -t nat -nL V2RAYiptables -t nat -nL V2RAY --line-numberiptables -t nat -D V2RAY 11#插入到INPUT链中的第7行位置iptables -I INPUT 7 -i eth0 -p tcp -m tcp --dport 11111 -j ACCEPT #最前iptables -I#最后iptables -A#Deleteiptables -D INPUT -i eth0 -p udp -m udp --dport 11111 -j ACCEPT iptables -nL INPUT --line-numberiptables -D INPUT 7#Clean iptablesiptables -t nat -D OUTPUT -p tcp -j XRAY#iptables -t nat -F XRAYiptables -t nat -F #删除前需要清空策略，否则删除不掉 iptables -t nat -X XRAYiptables -t nat -Z XRAY 设定1234567891011121314151617181920212223-F：清除所有已定的规则-X：除掉所有用户“自定义”的链-Z：将所有的chain的计数与流量统计归零-A：插入规则的最后面-I：插入变为第一条规则-i：进入网络接口(eth0,lo等)-o：传出网络接口-p：协议(tcp、udp、icmp及all或者!ICMP=tcp/udp)-s：来源IP匹配：192.168.0.0/24或192.168.0.0/255.255.255.0--sport：来源端口-d：目标网络--dport：目的端口-j：操作：ACCEPT/DROP/REJECT/LOG-P：策略：INPUT/OUTPUT/FORWARD-m：模块：state/mac--state：状态 INVAID：无效 ESTABLISHED：联机成功的 NEW：新建的包 RELATED：成功发邮并返回的包注意：指定port时必须指定是udp还是tcpA PREROUTING -p tcp --dport 8022 -j DNAT --to-destination 192.168.0.133:22-A POSTROUTING -d 192.168.0.133 -p tcp --dport 22 -j SNAT --to 192.168.0.9 例子： 123456789101112131415#!/bin/shiptables -Fiptables -Xiptables -Ziptables -P INPUT DROPiptables -P OUTPUT ACCEPTiptables -P FORWARD ACCEPT#内部循环放行iptables -A INPUT -i lo -j ACCEPT#联机成功、并且成功发出与返回的包放行iptables -A INPUT -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPTiptables -A INPUT -i eth0 -s 192.168.0.5 -j ACCEPTiptables -A INPUT -i eth0 -p icmp --icmp-type any -j ACCEPT#只允许本机连接mysqliptables -A INPUT -s ! 127.0.0.1 -p tcp -m tcp --dport 3306 -j REJECT 备份与还原1iptables-save &gt; /etc/sysconfig/iptables.bak 格式与/etc/sysconfig/iptables中的内容一样 1iptables-restore &lt; /etc/sysconfig/iptables.bak 加载/etc/sysconfig/iptables.bak中的内容，不会写入/etc/sysconfig/iptables中 1/etc/init.d/iptables save 保存内容至/etc/sysconfig/iptables中，并可以使用service iptables start启动 NAT端口转发： 1.到本机端口：1iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 81 2.到其它IP端口：1iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j DNAT --to 192.168.0.10:81 3.修改流入包的信息：1iptables -t nat -A PREROUTING -d 183.63.2.202 -j DNAT --to-destination 92.168.0.10 4.修改流出包的信息：1iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -j SNAT --to-source 183.63.2.202 5.ip伪装（当为动态IP时使用ip装，当为静态IP时可用第4条）：1iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -o eth0 -j MASQUERADE 6.打开路由： 永久修改：/etc/sysctl.conf中的net.ipv4.ip_forward=1，生效：sysctl -p 临时修改：echo 1 &gt; /proc/sys/net/ipv4/ip_forward，重启后失效 7.屏蔽某个网站：1iptables -A FORWARD -m string --string &quot;qq.com&quot; --algo bm -j DROP –algo:bm|kmp：bm比kmp快 8.禁止某个IP段不能上网：1iptables -A FORWARD -m iprange --src-range 192.168.0.0-192.168.0.100 -j DROP 其中：–src-range：源地址 –dst-range：目的地址 9.多个端口：1-m --multport --sport 80,82 –sport 80:82 #80到82端口 10.时间限制：1-m time --time-start 00:00 --time-stop 13:00 11.限速：12iptables -A FORWARD -s 192.168.0.158 -m limit --limit 20/s -j ACCEPTiptables -A FORWARD -s 192.168.0.158 -j DROP 20/s表示限速30k/s： 先用ifconfig查看MUT(1500字节)，30k/1500B=20个MTU 12.禁止其他主机ping防火墙主机，但是允许从防火墙上ping其他主机: 123iptables -A INPUT -p icmp --icmp-type Echo-Request -j DROPiptables -A INPUT -p icmp --icmp-type Echo-Reply -j ACCEPTiptables -A INPUT -p icmp --icmp-type destination-Unreachable -j ACCEPT 13.开放vpn服务:1234iptables -A INPUT -p tcp -m multiport --destination-port 47,1723 -j ACCEPTiptables -A INPUT -p gre -j ACCEPTiptables -A OUTPUT -p tcp -m multiport --source-port 47,1723 -j ACCEPTiptables -A OUTPUT -p gre -j ACCEPT 实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# Generated by iptables-save v1.3.5 on Tue Jun 8 08:43:18 2010*nat:PREROUTING ACCEPT [9:721]:POSTROUTING ACCEPT [0:0]:OUTPUT ACCEPT [0:0]#-A PREROUTING -d 183.63.2.202 -j DNAT --to-destination 192.168.0.0/24#-A POSTROUTING -s 192.168.0.0/24 -j SNAT --to-source 183.63.2.202-A POSTROUTING -s 192.168.0.0/24 -j MASQUERADE#-A PREROUTING -s 192.168.0.0/24 -p tcp -m tcp --dport 81 -j DNAT --to 10.35.60.79:8443#-A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 81-A PREROUTING -p tcp -m tcp --dport 81 -j REDIRECT --to-ports 80#-A PREROUTING -p tcp -m tcp --dport 23 -j DNAT --to 192.168.0.167:22COMMIT# Completed on Tue Jun 8 08:43:18 2010# Generated by iptables-save v1.3.5 on Tue Jun 8 08:43:18 2010*filter:INPUT DROP [30:2480]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [104:11777]-A INPUT -i lo -j ACCEPT-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT#-A INPUT -p tcp -m tcp --sport 1:1023 --dport 1:1023 --tcp-flags FIN,SYN,RST,ACK SYN -j DROP#open ssh#-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 22 -j ACCEPT-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT#-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT#open smb-A INPUT -s 192.168.0.0/255.255.255.0 -p udp -m udp --dport 137:138 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 139 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 445 -j ACCEPT#open postfix-A INPUT -p tcp -m tcp --dport 25 -j ACCEPT-A INPUT -p tcp -m tcp --dport 110 -j ACCEPT-A INPUT -p tcp -m tcp --dport 143 -j ACCEPT-A INPUT -p tcp -m tcp --dport 53 -j ACCEPT#open mysql-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 3306 -j ACCEPT#-A INPUT --dport 3306 -j ACCEPT#open vnc-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 5801 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 5901 -j ACCEPT#open http-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT-A INPUT -p tcp -m tcp --dport 81 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 8080 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 8088 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 10000 -j ACCEPT#open socket-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 5555 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 5556 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 6666 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 6667 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 7777 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 7778 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 7788 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 7789 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 8888 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 8889 -j ACCEPT#open ping-A INPUT -s 192.168.0.0/255.255.255.0 -p icmp -m icmp --icmp-type any -j ACCEPT#dns-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 53 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p udp -m udp --dport 53 -j ACCEPT#vpn-A INPUT -p tcp -m multiport --destination-port 47,1723 -j ACCEPT-A INPUT -p udp -m multiport --destination-port 47,1723 -j ACCEPT#openvpn#-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m tcp --dport 444 -j ACCEPT#-A INPUT -p gre -j ACCEPT#nfs-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp -m multiport --dport 111,2049 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p udp -m multiport --dport 111,2049 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p tcp --dport 10001:10004 -j ACCEPT-A INPUT -s 192.168.0.0/255.255.255.0 -p udp --dport 10001:10004 -j ACCEPT#forward#-A FORWARD -s 192.168.0.0/255.255.255.0 -j ACCEPTCOMMIT# Completed on Tue Jun 8 08:43:18 2010# Generated by webmin*mangle:FORWARD ACCEPT [0:0]:INPUT ACCEPT [0:0]:OUTPUT ACCEPT [0:0]:PREROUTING ACCEPT [0:0]:POSTROUTING ACCEPT [0:0]COMMIT# Completed","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"Kubernetes Development Environment","slug":"Kubernetes-Development-Environment","date":"2020-12-19T01:27:52.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2020/12/Kubernetes-Development-Environment.html","link":"","permalink":"http://blog.gcalls.cn/2020/12/Kubernetes-Development-Environment.html","excerpt":"This article will teach us how to set up a development environment in you local machine, including java/k8s/spring cloud kubernetes, etc.","text":"This article will teach us how to set up a development environment in you local machine, including java/k8s/spring cloud kubernetes, etc. OSreference: https://docs.microsoft.com/en-us/windows/wsl/install-win10 I’d recommend you base on WSL system to develop, if you don’t know what’s the wsl, look at this article Installing WSL as below: 123456789101112#Step 1 - Enable WSL#Openning PowerShell as administrotor：dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart#Step 2 - Enable Virtual Machine #Openning PowerShell as administrotor：dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart#Step 3 - Downloading Linux Kernelwget http://aka.ms/wsl2kernelmsix64#Step 4 - Setting WSL 2 as defaultwsl --set-default-version 2#Step 5 - Installing linux Searching proper linux version in Microsoft Store Windows TerminalI’d highly recommend using Windows terminal in Windows 10, it’s pretty handly. Reference: https://docs.microsoft.com/en-us/windows/terminal/get-started Adding git-bash support： 12345678&#123; \"closeOnExit\" : true, \"commandline\" : \"D:\\\\Developer\\\\Git\\\\bin\\\\bash.exe --login -i\", \"guid\" : \"&#123;1d4e097e-fe87-4164-97d7-3ca794c316fd&#125;\", \"icon\" : \"D:\\\\Developer\\\\Git\\\\git-bash.png\", \"name\" : \"Bash\", \"startingDirectory\" : \"%USERPROFILE%\"&#125; Install Local Docker(Optional)If you want to use the remote development environment, don’t need it. Reference: https://docs.docker.com/docker-for-windows/wsl/ https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/ Just need to install Docker-Desktop for Windows, and select the ubuntu at “Setting-&gt;Resources/WSL Integration”, that’s all. Install Local Kubernetes(Optional)If you want to use the remote development environment, don’t need it. Just need to enable kubernetes in “Docker-Desktop Settings”, that’s all. But you need to set the proxy in the Docker Setting, or you will get the failure by Downloading google’s containers. 12http://192.168.101.175:1082127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net,*.aliyun.com,*.163.com,*.docker-cn.com,registry.gcalls.cn Enable Ingress Addon: Reference：https://github.com/docker/for-win/issues/7094 1234567#https://kubernetes.github.io/ingress-nginx/deploy/#docker-for-mac #kubectl.exe create -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-0.32.0/deploy/static/provider/cloud/deploy.yaml #Resolved: Unable to connect to the server proxy_onwget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.41.2/deploy/static/provider/cloud/deploy.yamlproxy_off kubectl apply -f deploy.yaml demo.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162kind: ServiceapiVersion: v1metadata: name: hello labels: app: hellospec: #type: NodePort ports: - protocol: TCP name: http port: 8080 targetPort: 8080 selector: app: hello---kind: DeploymentapiVersion: apps/v1metadata: name: hello labels: app: hellospec: replicas: 1 selector: matchLabels: app: hello template: metadata: labels: app: hello spec: containers: - name: hello # image: paulbouwer/hello-kubernetes:1.8 image: gcr.io/google-samples/hello-app:1.0 ports: - containerPort: 8080---apiVersion: networking.k8s.io/v1 # for versions before 1.14 use extensions/v1beta1kind: Ingressmetadata: name: hello # annotations: # nginx.ingress.kubernetes.io/rewrite-target: /$1 # nginx.ingress.kubernetes.io/whitelist-source-range: 192.168.147.174/32spec: rules: - host: hello.info http: paths: - path: / pathType: Prefix backend: service: name: hello port: number: 8080 Remote Kubernetes EnvironmentI’d recommend installing docker and kubernetes on the remote machine, and all of developers can share it and save some of local resources. DockerUbuntu Reference: http://blog.gcalls.cn/blog/2018/12/ubuntu-os.html#Docker For CentOS: 1234567#https://www.cnblogs.com/763977251-sg/p/11837130.html#Docker installation#https://aka.ms/vscode-remote/samples/docker-from-dockersudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposudo yum makecache fastsudo yum -y install docker-ce KubernetesKubectlFollowing the instructions to install the k8s client tools: 123456789101112131415161718192021222324252627282930313233343536373839#For MAC#https://kubernetes.io/zh/docs/tasks/tools/install-kubectl-macos/#curl -LO \"https://dl.k8s.io/release/v1.18.18/bin/darwin/amd64/kubectl\"#chmod +x ./kubectl#sudo mv ./kubectl /usr/local/bin/kubectl#sudo chown root: /usr/local/bin/kubectlbrew install kubectlmkdir .kube#the \"config\" file located in the root of projectcp config ~/.kube/kubectl config use microk8s-0#For WindowsStep 1:Downloading https://dl.k8s.io/release/v1.18.18/bin/windows/amd64/kubectl.exe, and put it to a executable environment path(I'd recommend putting it to the folder of GIT_HOME/bin), and make sure $GIT_HOME is in the PATH of your environment.Step 2: Open the terminal of CMD, executing the following command(Press Win+R, and input \"cmd\"):cd %HOMEPATH%mkdir .kubeStep 3: Copying the \"config\" file located in the root folder of project to the \".kube\" folder located in current user home(like: C:\\Users\\YourName\\.kube).Step 4:Open the terminal of CMD again, executing the following command:kubectl config use microk8s-0Congratulations, That's all done.#For Linux#https://kubernetes.io/zh/docs/tasks/tools/install-kubectl-linux/curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectlkubectl version --clientmkdir ~/.kube#the \"config\" file located in the root of projectcp config ~/.kube/kubectl config use microk8s-0 microk8sRecommend using microk8s on Linux. It’s the best performance. Reference： https://jiajunhuang.com/articles/2019_11_17-microk8s.md.htmlhttps://microk8s.io/#quick-starthttps://microk8s.io/docshttps://www.cnblogs.com/xiao987334176/p/10931290.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#For ubuntu:#https://blog.flyfox.top/2020/04/03/microk8s%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/#https://microk8s.io/#install-microk8s#Ubuntu 20.04 has been installed with snap pre-installedsudo snap install microk8s --classicapt install bash-completion -ysource /usr/share/bash-completion/bash_completion#For centos7:#sudo yum install epel-releasesudo su - devsudo yum install snapdsudo systemctl enable --now snapd.socketsudo ln -s /var/lib/snapd/snap /snapsudo snap install microk8s --classicyum install bash-completion -ysource /usr/share/bash-completion/bash_completion#Notice: microk8s is using containerd, not docker any more.#Either log out and back in again or restart your system to ensure sudo vim /var/snap/microk8s/current/args/containerd-envHTTP_PROXY=\"http://192.168.101.175:1082\"HTTPS_PROXY=\"http://192.168.101.175:1082\"NO_PROXY=\"127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net,*.aliyun.com,*.163.com,*.docker-cn.com,registry.gcalls.cn\"sudo systemctl list-unit-files |grep -i microk8ssudo systemctl restart snap.microk8s.daemon-containerd.servicemicrok8s.start#Addons: https://microk8s.io/docs/addons#heading--list#microk8s.enable dashboard dns ingress istio registry storage rbacmicrok8s.enable dashboard dns ingress hostpath-storagemicrok8s status --wait-ready#list all of enabled addonsmicrok8s statusmicrok8s.kubectl describe pods -Amicrok8s.inspectkubectl cluster-infoKubernetes master is running at https://192.168.95.234:16443Metrics-server is running at https://192.168.95.234:16443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxyCoreDNS is running at https://192.168.95.234:16443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.#bashboard:#https://medium.com/@junya.kaneko/quick-way-of-using-kubernetes-dashboard-on-microk8s-9c7b0e26be02#Skip loginmicrok8s kubectl edit deployment/kubernetes-dashboard -n kube-systemspec: containers: - args: - --auto-generate-certificates - --namespace=kube-system - --enable-skip-loginmicrok8s kubectl create clusterrolebinding kubernertes-dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboardmicrok8s kubectl proxy --accept-hosts=.* --address=0.0.0.0http://192.168.80.98:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#/logintoken=$(microk8s kubectl -n kube-system get secret | grep default-token | cut -d \" \" -f1)echo $tokenmicrok8s kubectl -n kube-system describe secret $token#uninstall microk8ssudo snap remove microk8srm -fr /root/snap/microk8s /home/dev/snap/microk8s#https://microk8s.io/docs/working-with-kubectl#Export the config for clientscd $HOMEmkdir .kubecd .kubemicrok8s config &gt; config#https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/#kubectl context configkubectl config get-contexts#current context configkubectl config current-context#switch config as alik8s-0kubectl config use-context alik8s-0#~/.bash_profile#yum install bash-completion -y ~/.bash_profilealias k=kubectlsource &lt;(kubectl completion bash | sed s/kubectl/k/g)#source /usr/share/bash-completion/bash_completion#Kubectl installation:curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kubectlchmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/#OR#Kubectl For CentOS#https://blog.csdn.net/nklinsirui/article/details/80581286cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubectl#Kubectl For Ubuntu#https://blog.csdn.net/nklinsirui/article/details/80581286curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - echo \"deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\" &gt;&gt; /etc/apt/sources.listapt-get updateapt-get install -y kubectl HarborHarbor is an open source trusted cloud native registry project that stores, signs, and scans content. Harbor extends the open source Docker Distribution by adding the functionalities usually required by users such as security, identity and management. Having a registry closer to the build and run environment can improve the image transfer efficiency. Harbor supports replication of images between registries, and also offers advanced security features such as user management, access control and activity auditing. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#Habor:#Using root enviroment:sudo yum install python3-pip#rootpip3 install -U docker-compose#non-root#pip3 install --user docker-compose#https://goharbor.io/docs/2.1.0/install-config/configure-https/#Generating the certificate:#Using dev enviroment:sudo su - devexport DP_Id=\"\"export DP_Key=\"\"acme.sh --issue --dns dns_dp -d gcalls.cn -d *.gcalls.cn#acme.sh --issue --dns dns_dp -d registry.gcalls.cn --keylength ec-256Your cert is in /home/dev/.acme.sh/gcalls.cn/gcalls.cn.cer Your cert key is in /home/dev/.acme.sh/gcalls.cn/gcalls.cn.key The intermediate CA cert is in /home/dev/.acme.sh/gcalls.cn/ca.cer The full chain certs is there: /home/dev/.acme.sh/gcalls.cn/fullchain.cersudo mkdir -p /etc/docker/certs.d/registry.gcalls.cnsudo cp /home/dev/.acme.sh/gcalls.cn/gcalls.cn.cer /etc/docker/certs.d/registry.gcalls.cn/#must be gcalls.cn.certsudo cp /home/dev/.acme.sh/gcalls.cn/fullchain.cer /etc/docker/certs.d/registry.gcalls.cn/gcalls.cn.certsudo cp /home/dev/.acme.sh/gcalls.cn/gcalls.cn.key /etc/docker/certs.d/registry.gcalls.cn/sudo cp /home/dev/.acme.sh/gcalls.cn/ca.cer /etc/docker/certs.d/registry.gcalls.cn/cp -a harbor-offline-installer-v2.1.2.tgz /works/k8s/cd /works/k8s/tar zxvf harbor-offline-installer-v2.1.2.tgzsudo chown -R dev.dev /works/k8s/harbor#https://goharbor.io/docs/2.1.0/install-config/configure-yml-file/#Modifying the harbor.ymlcd /works/k8s/harborcp -a harbor.yml.tmpl harbor.ymlvim harbor.yml:hostname: registry.gcalls.cnhttps: # The path of cert and key files for nginx #certificate: /home/dev/.acme.sh/gcalls.cn/gcalls.cn.cer certificate: /etc/docker/certs.d/registry.gcalls.cn/gcalls.cn.cert private_key: /home/dev/.acme.sh/gcalls.cn/gcalls.cn.key #Execting the script ./preparesudo su - rootcd /works/k8s/harbor#https://goharbor.io/docs/2.1.0/install-config/run-installer-script/./install.sh #If Harbor is running, stop and remove the existing instance.Your image data remains in the file system, so no data is lost.#docker-compose down -v#Restartingdocker-compose stopdocker-compose up -d#Open a browser and enter https://yourdomain.com. It should display the Harbor interfacehttps://registry.gcalls.cnadmin/Harbor12345docker login registry.gcalls.cn#troubleshootingGet https://registry.gcalls.cn/v2/: net/http: TLS handshake timeoutIf you got the error above, it seems you are using the proxy, try to exclude \"registry.gcalls.cn\" in the \"NO_PROXY\", the file is located:/etc/systemd/system/docker.service.d/http-proxy.confEnvironment=\"HTTP_PROXY=http://192.168.101.175:1082\"Environment=\"HTTPS_PROXY=http://192.168.101.175:1082\"Environment=\"NO_PROXY=127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net,*.aliyun.com,*.163.com,*.docker-cn.com,registry.gcalls.cn\"systemctl daemon-reload &amp;&amp; systemctl restart docker#Testdocker pull hello-world#must include the project name, like xwallet, and created the project beforehand:docker tag hello-world registry.gcalls.cn/xwallet/hello-worlddocker push registry.gcalls.cn/xwallet/hello-world#If the registry work without http, need to add the following(https don't do it):#server-side#vim /etc/docker/daemon.json#\"insecure-registries\" : [\"localhost:32000\", \"192.168.95.233:32000\"]#client-side#\"insecure-registries\" : [\"192.168.95.233:32000\"]#Don't forget rebooting docker docker registry21234567891011121314151617181920212223sudo su - dev#sslacme.sh --issue --dns dns_dp -d registry.gcalls.cn#installmkdir -p /works/docker/registrydocker run -d \\ --name private_registry --restart=always \\ -e SETTINGS_FLAVOUR=dev \\ -e STORAGE_PATH=/registry-storage \\ -v /works/docker/registry:/var/lib/registry \\ -u root \\ -p 5000:5000 \\ -v /home/dev/.acme.sh/registry.gcalls.cn:/certs \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/fullchain.cer \\ -e REGISTRY_HTTP_TLS_KEY=/certs/registry.gcalls.cn.key \\ registry:2#testdocker pull hello-worlddocker tag hello-world registry.gcalls.cn:5000/hello-worlddocker push registry.gcalls.cn:5000/hello-world kindNot Recommend. Reference：https://kind.sigs.k8s.io/docs/user/quick-start/ 123456789101112131415161718192021# Download the latest version of Kindcurl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/v0.9.0/kind-$(uname)-amd64# Make the binary executablechmod +x ./kind# Move the binary to your executable pathsudo mv ./kind /usr/local/bin/# Check if the KUBECONFIG is not setecho $KUBECONFIG# Check if the .kube directory is created &gt; if not, no need to create itls $HOME/.kube# Create the cluster and give it a name (optional)export http_proxy=\"http://192.168.101.175:1082\"export https_proxy=$http_proxyexport no_proxy=\"127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net,*.aliyun.com,*.163.com,*.docker-cn.com,registry.gcalls.cn\"kind create cluster --name wslkindkind delete cluster --name wslkindkind get clusters# Check if the .kube has been created and populated with filesls $HOME/.kubekubectl get nodes Notice: Kind clusters based on docker, cannot communicate with the internal docker container. Adding the extraPortMappings: Reference：https://kind.sigs.k8s.io/docs/user/using-wsl2/ 12345678910111213141516171819202122# cluster-config.ymlkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4nodes:- role: control-plane extraPortMappings: - containerPort: 8080 hostPort: 8080 protocol: TCP - containerPort: 30000 hostPort: 30000 protocol: TCPkind create cluster --config=cluster-config.yml#kubectl run nginx --image=nginx --port=3000 --targetPort=80 --exposekubectl create deployment nginx --image=nginx#kubectl create service nodeport nginx --tcp=81:80 --node-port=30000kubectl expose deployment nginx --type=NodePort --name nginx --port=80 --target-port=80#access service curl localhost:30000 Local developmentReference： https://www.telepresence.io/discussion/overview https://github.com/cesartl/telepresence-k8s https://kubernetes.io/zh/docs/tasks/debug-application-cluster/local-debugging/ https://cloud.google.com/community/tutorials/developing-services-with-k8s telepresencehttps://www.telepresence.io/reference/windows 1234567891011121314#For Windows:1. Install Windows Subsystem for Linux.2. Start the BASH.exe program.3. Install Telepresence by following the Ubuntu instructions above.#For ubuntu:curl -s https://packagecloud.io/install/repositories/datawireio/telepresence/script.deb.sh | sudo bashsudo apt install --no-install-recommends telepresence#For CentOS:sudo yum install torsocks sshfs conntrack python3 -ygit clone https://github.com/telepresenceio/telepresence.git /Developer/telepresence \\ &amp;&amp; cd /Developer/telepresence \\ &amp;&amp; sudo env PREFIX=/usr/local ./install.sh Develop https://anjia0532.github.io/2019/01/21/debug-cloud-native/ https://www.telepresence.io/tutorials/docker https://www.telepresence.io/tutorials/kubernetes https://github.com/telepresenceio/telepresence/tree/master/examples/guestbook https://github.com/cesartl/telepresence-k8s 123456789101112131415161718192021222324252627282930313233343536373839404142#https://www.telepresence.io/tutorials/java#kubectl apply -f https://raw.githubusercontent.com/telepresenceio/telepresence/master/docs/tutorials/hello-world.yaml#kubectl create deployment hello-world --image=datawire/hello-world#kubectl expose deployment hello-world --type=LoadBalancer --port=8000#$ curl 127.0.0.1:8000#Hello, world!git clone https://github.com/cesartl/telepresence-k8scd telepresence-k8s#Setting up Quote Of the Moment Servicekubectl run qotm --image=datawire/qotm:1.3 --port=5000 --expose#Basic profile#Using the basic profile, service disovery using K8S API is disable; The Ribbon client use the service #host name directly:qotm: ribbon: listOfServers: qotm:5000#telepresence --docker-run --rm -it pstauffer/curl -- curl http://hello-world:8000/#telepresence --new-deployment telepresence-k8s --expose 8080:8080 --run-shell#telepresence --new-deployment telepresence-k8s --expose 8080 --expose 8081 --run-shell#telepresence --swap-deployment telepresence-k8s --docker-run --rm -it --name mynginx -p 8080:80 nginx#telepresence --new-deployment telepresence-k8s --docker-run --rm -it --name mynginx -p 8081:80 nginx#telepresence --swap-deployment hello-world --expose 8000 --run python3 -m http.server 8000 &amp;telepresence --new-deployment telepresence-k8s --run-shell#Notice: Some of spring-boot versions don't support remote debug through mvnDebug or MAVEN_DEBUG_OPTS:#cat /Developer/apache-maven-3.3.9/bin/mvnDebug#MAVEN_DEBUG_OPTS=\"-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000\"&gt;cd /mnt/d/Developer/workspace/telepresence-k8s/&gt;export KUBERNETES_NAMESPACE=default&gt;export PROFILES=basic&gt;mvn spring-boot:run -Dspring-boot.run.jvmArguments=\"-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8000\"#Preparing to Execute Maven in Debug Mode#It'll pause until the client is connected, you can set suspend=n to against it.#Listening for transport dt_socket at address: 8000#Notice：pom.xml musn't add the section，or you cannot remote debug:&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;#Testing:curl localhost:8080/rest/quote/cesar Notice: Making sure “KUBERNETES_NAMESPACE” is set in the OS environment. You can set it of “remoteEnv” of devcontainer.json file if you develop with VSCODE. JRebel1234567891011121314151617181920212223242526272829303132#https://www.jrebel.com/success/products/jrebel/free-trial#https://www.jrebel.com/products/jrebel/quickstart/eclipse/#https://www.jrebel.com/products/jrebel/download/prev-releases#https://manuals.jrebel.com/jrebel/standalone/activate.html#Plugin for eclipse:#update site: http://update.zeroturnaround.com/update-site#Download ZIPhttp://update.zeroturnaround.com/update-site/update-site.zip#Crack#https://www.cnblogs.com/flyrock/archive/2019/09/23/11574617.html#Generating GUID from: https://www.guidgen.com/#Activation Server URL： Pasting the following url on the \"Licensing service\"https://jrebel.qekang.com/&#123;GUID&#125;https://jrebel.qekang.com/d1b8919f-e1e9-4a8d-84da-0c43d75aa970aaa@bbb.com#Activation for standalone:wget https://www.jrebel.com/download/jrebel/496cd /Developer/jrebel/bin./activate.sh https://jrebel.qekang.com/d1b8919f-e1e9-4a8d-84da-0c43d75aa970 aaa@bbb.com#https://manuals.jrebel.com/jrebel/standalone/springboot.html#spring-boot-2-x-using-maven#https://manuals.jrebel.com/jrebel/standalone/config.html#rebel-xml#https://www.javazhiyin.com/22460.htmlmvn spring-boot:run -Dspring-boot.run.jvmArguments=\"-agentpath:/Developer/jrebel/lib/libjrebel64.so\"#!!!Important!!!: Project must be located at linux folder, a windows folder located won't take affect by JRebel.#If you see the following message, it works.2020-12-14 12:26:33 JRebel: Reloading class 'com.ctl.telepresencek8s.DummyRestController'.2020-12-14 12:26:38 JRebel: Reconfiguring bean 'dummyRestController' [com.ctl.telepresencek8s.DummyRestController] demo: https://github.com/zq2599/blog_demos https://xinchen.blog.csdn.net/article/details/92394559 http://www.mydlq.club/article/31 12345678910111213141516#account-servicecd /mnt/d/Developer/workspace/java-k8s/spring-cloud-k8s-account-servicemvn clean install -Pk8s#cd /mnt/d/Developer/workspace/java-k8s/spring-cloud-k8s-web-service#mvn clean install -Pk8s#telepresence --swap-deployment web-service --run-shelltelepresence --new-deployment web-service --run-shell#new-deployment will get the error message： #Did not find any endpoints in ribbon in namespace [null] for name [account-service] and portName [null]#https://github.com/telepresenceio/telepresence/issues/947#You can fix this with:export KUBERNETES_NAMESPACE=defaultcd /mnt/d/Developer/workspace/java-k8s/spring-cloud-k8s-web-servicemvn spring-boot:run -Dspring-boot.run.jvmArguments=\"-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8000\"curl 127.0.0.1:8080/accountkubectl scale --replicas=0 deployment account-service Notice: Some of spring-boot versions don’t support remote debug through mvnDebug or MAVEN_DEBUG_OPTS: https://docs.spring.io/spring-boot/docs/2.3.4.RELEASE/maven-plugin/reference/html/ Fixed this by: 12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;jvmArguments&gt; -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8000 &lt;/jvmArguments&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; Or： 1mvn spring-boot:run -Dspring-boot.run.jvmArguments=\"-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8000\" kubernetes-maven-pluginhttps://www.eclipse.org/jkube/docs/kubernetes-maven-plugin Building Kubernetes, strongly recommended. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196#Copying remote config into \"~/.kube/config\" of local#https://github.com/eclipse/jkube/tree/master/quickstarts/maven/cd /mnt/d/Developer/workspace/java-k8s/spring-cloud-k8s-account-service#For private docker registry, the most mavenish way is to add a server to the Maven settings file /Developer/apache-maven-3.3.9/conf/settings.xml&lt;server&gt; &lt;id&gt;registry.gcalls.cn&lt;/id&gt; &lt;username&gt;dave.zhao&lt;/username&gt; &lt;password&gt;******&lt;/password&gt;&lt;/server&gt;#Generating the configuration automatically, remember don't adding \"dockerHost\" and \"images\" selection,#Or will be generated two image section in the deployment yaml file:&lt;plugin&gt; &lt;groupId&gt;org.eclipse.jkube&lt;/groupId&gt; &lt;artifactId&gt;kubernetes-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;fmp&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;resource&lt;/goal&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;goal&gt;push&lt;/goal&gt; &lt;goal&gt;apply&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;resources&gt; &lt;imagePullPolicy&gt;Always&lt;/imagePullPolicy&gt; &lt;/resources&gt; &lt;enricher&gt; &lt;config&gt; &lt;fmp-service&gt; &lt;type&gt;NodePort&lt;/type&gt; &lt;/fmp-service&gt; &lt;/config&gt; &lt;/enricher&gt; &lt;/configuration&gt;&lt;/plugin&gt;#Using the external Dockerfile and deployment.yaml/service.yaml：#https://www.eclipse.org/jkube/docs/kubernetes-maven-plugin#external-dockerfile&lt;plugin&gt; &lt;groupId&gt;org.eclipse.jkube&lt;/groupId&gt; &lt;artifactId&gt;kubernetes-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;fmp&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;goal&gt;push&lt;/goal&gt; &lt;goal&gt;resource&lt;/goal&gt; #&lt;!-- Don't use deploy, or twice build was triggered --&gt; &lt;goal&gt;apply&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;!-- &lt;dockerHost&gt;tcp://registry.gcalls.cn:2375&lt;/dockerHost&gt; --&gt; &lt;dockerHost&gt;tcp://localhost:2375&lt;/dockerHost&gt; &lt;images&gt; &lt;image&gt; &lt;name&gt;registry.gcalls.cn/xwallet/$&#123;project.name&#125;:$&#123;project.version&#125;&lt;/name&gt; &lt;build&gt; &lt;!-- https://github.com/eclipse/jkube/issues/149 --&gt; &lt;assembly&gt; &lt;name&gt;target&lt;/name&gt; &lt;/assembly&gt; &lt;dockerFile&gt;$&#123;project.basedir&#125;/src/main/docker/Dockerfile&lt;/dockerFile&gt; &lt;!-- &lt;contextDir&gt;$&#123;project.basedir&#125;&lt;/contextDir&gt; --&gt; &lt;filter&gt;@&lt;/filter&gt; &lt;/build&gt; &lt;/image&gt; &lt;/images&gt; &lt;enricher&gt; &lt;config&gt; &lt;fmp-service&gt; &lt;type&gt;NodePort&lt;/type&gt; &lt;/fmp-service&gt; &lt;/config&gt; &lt;/enricher&gt; &lt;/configuration&gt;&lt;/plugin&gt;#Only include the jar file.maven-dockerinclude:target/*.jar#src/man/docker/DockerfileFROM java:8-jdkRUN mkdir /appWORKDIR /appENV APPNAME=account-service \\ VERSION=0.0.1-SNAPSHOT \\ CONFIG=/config/RUN ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo Asia/Shanghai &gt; /etc/timezoneCOPY target/$&#123;APPNAME&#125;-$&#123;VERSION&#125;.jar /app/ENTRYPOINT [\"sh\", \"-c\", \"java -Djava.security.egd=file:/dev/./urandom -jar /app/$&#123;APPNAME&#125;-$&#123;VERSION&#125;.jar --spring.config.location=$&#123;CONFIG&#125; --spring.profiles.active=@spring.profile@\"]EXPOSE 8100#src/man/jkube/account-service-deployment.ymlkind: DeploymentapiVersion: apps/v1metadata: name: account-service namespace: default labels: app: account-service group: com.xwallet version: 0.0.1-SNAPSHOT provider: jkubespec: replicas: 1 selector: matchLabels: app: account-service group: com.xwallet version: 0.0.1-SNAPSHOT provider: jkube template: metadata: labels: app: account-service group: com.xwallet version: 0.0.1-SNAPSHOT provider: jkube spec: containers: - name: account-service #image: registry.gcalls.cn/xwallet/account-service:0.0.1-SNAPSHOT imagePullPolicy: Always ports: - containerPort: 8089#src/man/jkube/account-service-service.ymlkind: ServiceapiVersion: v1metadata: name: account-service namespace: default labels: app: account-service group: com.xwallet version: 0.0.1-SNAPSHOT provider: jkubespec: type: NodePort ports: - protocol: TCP name: http port: 8080 targetPort: 8080 selector: app: account-service group: com.xwallet version: 0.0.1-SNAPSHOT provider: jkube#Running:#If don't define the dockerHost or private registry parameter, using the following command:#export DOCKER_HOST=\"tcp://registry.gcalls.cn:2375\"#mvn clean install k8s:push k8s:deploy -Pk8s -Ddocker.registry=registry.gcalls.cn#mvn clean install k8s:build k8s:push k8s:resource k8s:apply -Dmaven.test.skip=true -Dspring.profile=dev -Pk8smvn clean install -Pk8s#Building by parameters#Dockerfile#ENTRYPOINT [\"sh\", \"-c\", \"java -Djava.security.egd=file:/dev/./urandom -jar /app/$&#123;APPNAME&#125;-$&#123;VERSION&#125;.jar --spring.profiles.active=@spring.profile@\"]#OR&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;k8s&lt;/id&gt; &lt;properties&gt; &lt;spring.profile&gt;default&lt;/spring.profile&gt; &lt;/properties&gt;#ORmvn clean install -Pk8s -Dspring.profile=dev#Exposing extra port of existing docker container #https://blog.csdn.net/lsziri/article/details/69396990#Assuming docker container's name is: asset-appdocker inspect asset-app | grep IPAddressdocker port asset-appsudo iptables -t nat -nvL --line-number#Exposing 5100 of host -&gt; 5100 of container sudo iptables -t nat -A PREROUTING -p tcp -m tcp --dport 5100 -j DNAT --to-destination 10.244.47.4:5100 sudo iptables-save#docker port asset-app couldn't show the 5100, do this to view:sudo iptables -t nat -nvL | grep 10.244.47.4#Push images to aliyundocker login --username=zhaoxunyong@139.com registry.cn-shenzhen.aliyuncs.comdocker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/zerofinance/fisco:[镜像版本号]docker push registry.cn-shenzhen.aliyuncs.com/zerofinance/fisco:[镜像版本号]","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"WSL","slug":"WSL","date":"2020-12-01T03:00:22.000Z","updated":"2024-12-24T09:12:31.100Z","comments":true,"path":"/2020/12/WSL.html","link":"","permalink":"http://blog.gcalls.cn/2020/12/WSL.html","excerpt":"Windows Subsystem for Linux（简称WSL）是一个在Windows 10上能够运行原生Linux二进制可执行文件（ELF格式）的兼容层。它是由微软与Canonical公司合作开发，其目标是使纯正的Ubuntu 14.04 “Trusty Tahr”映像能下载和解压到用户的本地计算机，并且映像内的工具和实用工具能在此子系统上原生运行。","text":"Windows Subsystem for Linux（简称WSL）是一个在Windows 10上能够运行原生Linux二进制可执行文件（ELF格式）的兼容层。它是由微软与Canonical公司合作开发，其目标是使纯正的Ubuntu 14.04 “Trusty Tahr”映像能下载和解压到用户的本地计算机，并且映像内的工具和实用工具能在此子系统上原生运行。 安装WSL最新版本的操作系统Windows 11安装简单很多，参考：https://docs.microsoft.com/en-us/windows/wsl/install 先安装：适用于 x64 计算机的 WSL2 Linux 内核更新包，否则会报：0x80072f78错误。 1234567891011121314151617wsl --install#或者安装指点的版本wsl -l -o以下是可安装的有效分发的列表。请使用“wsl --install -d &lt;分发&gt;”安装。NAME FRIENDLY NAMEUbuntu UbuntuDebian Debian GNU/Linuxkali-linux Kali Linux RollingopenSUSE-42 openSUSE Leap 42SLES-12 SUSE Linux Enterprise Server v12Ubuntu-16.04 Ubuntu 16.04 LTSUbuntu-18.04 Ubuntu 18.04 LTSUbuntu-20.04 Ubuntu 20.04 LTSwsl --install -d Ubuntu Windows 10版本参考以下内容： 安装最新的Win10：cn_windows_10_business_editions_version_20h2_x64_dvd_f978664f.iso 安装参考：https://docs.microsoft.com/zh-cn/windows/wsl/install-win10 123456789101112#步骤 1 - 启用适用于 Linux 的 Windows 子系统#以管理员身份打开 PowerShell 并运行：dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart#步骤 2 - 启用虚拟机功能#以管理员身份打开 PowerShell 并运行：dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart#步骤 3 - 下载 Linux 内核更新包wget http://aka.ms/wsl2kernelmsix64#步骤 4 - 将 WSL 2 设置为默认版本wsl --set-default-version 2#步骤 5 - 安装所选的 Linux 分发直接在Microsoft Store中搜索并安装 如果已经安装了最新的Win10与docker desktop后，可以直接跳到步骤4，下载以下文件进行安装： https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi 安装docker时也会提示更新这个。另外注意：不需要开启Hyper-V。安装完后设置一下默认为v2: 1wsl --set-default-version 2 安装ubuntu 参考：https://docs.microsoft.com/zh-cn/windows/wsl/install-win10#step-6---install-your-linux-distribution-of-choice 终端Windows终端(建议)： 参考： https://docs.microsoft.com/zh-cn/windows/terminal/get-started https://blog.walterlv.com/post/add-a-new-profile-for-windows-terminal.html 添加git-bash支持： 1234567891011121314151617181920212223242526272829&#123; \"guid\": \"&#123;4d4cc780-cacb-5b6f-a183-29b5f6cdcd23&#125;\", \"hidden\": false, \"name\": \"CentOS\", \"icon\" : \"D:\\\\Developer\\\\Git\\\\centos.png\", \"colorScheme\": \"Campbell Powershell\", \"source\": \"Windows.Terminal.Wsl\"&#125;,&#123; \"guid\" : \"&#123;1d4e097e-fe87-4164-97d7-3ca794c316fd&#125;\", \"name\" : \"Bash\", \"icon\" : \"D:\\\\Developer\\\\Git\\\\git-bash.png\", \"colorScheme\": \"Vintage\", \"fontFace\" : \"Consolas\", \"fontSize\" : 14, \"commandline\" : \"D:\\\\Developer\\\\Git\\\\bin\\\\bash.exe --login -i\", \"startingDirectory\" : \"%USERPROFILE%\"&#125;,&#123; \"guid\": \"&#123;4360b1d2-9732-4bfd-8967-93338bcbb568&#125;\", \"name\": \"192.168.80.98\", \"commandline\": \"ssh dev@192.168.80.98\", \"colorScheme\" : \"One Half Dark\", \"fontFace\" : \"Consolas\", \"fontSize\" : 14, \"icon\" : \"ms-appx:///ProfileIcons/&#123;9acb9455-ca41-5af7-950f-6bca1bc9722f&#125;.png\"&#125;colorScheme请参考：https://docs.microsoft.com/en-us/windows/terminal/customize-settings/color-schemes wsl-terminal： 参考：https://link.jianshu.com/?t=https://github.com/goreliu/wsl-terminal 设置默认Linux发行版参考：http://www.xitongzhijia.net/xtjc/20180316/122477.html 123456wslconfig /lwslconfig /setdefault Ubuntu-20.04wsl.exe -l -v#wsl.exe --set-version Ubuntu-20.04 2#wsl.exe --set-default-version 2 配置WSL为独立的系统默认WSL可以以windows共享vscode/docker等资源，但不支持systemd相关的服务，可以通过以下方式开启，参考：https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/#minikube-enabling-systemd 注意：开启systemd后将不再支持直接在linux开启vscode，与windows共享资源。也就是开启后将是一个完整独立的linux系统。不太建议。如果只是使用docker与k8s的话，直接使用docker-desktop即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# Edit the sudoers with the visudo commandsudo visudo# Change the %sudo group to be password-less%sudo ALL=(ALL:ALL) NOPASSWD: ALL# Press CTRL+X to exit# Press Y to save# Press Enter to confirm#Systemctl couldn't startsudo apt install -y conntracksudo apt install -yqq daemonize dbus-user-session fontconfig# Create the start-systemd-namespace scriptsudo vi /usr/sbin/start-systemd-namespace#!/bin/bashSYSTEMD_PID=$(ps -ef | grep '/lib/systemd/systemd --system-unit=basic.target$' | grep -v unshare | awk '&#123;print $2&#125;')if [ -z \"$SYSTEMD_PID\" ] || [ \"$SYSTEMD_PID\" != \"1\" ]; then export PRE_NAMESPACE_PATH=\"$PATH\" (set -o posix; set) | \\ grep -v \"^BASH\" | \\ grep -v \"^DIRSTACK=\" | \\ grep -v \"^EUID=\" | \\ grep -v \"^GROUPS=\" | \\ grep -v \"^HOME=\" | \\ grep -v \"^HOSTNAME=\" | \\ grep -v \"^HOSTTYPE=\" | \\ grep -v \"^IFS='.*\"$'\\n'\"'\" | \\ grep -v \"^LANG=\" | \\ grep -v \"^LOGNAME=\" | \\ grep -v \"^MACHTYPE=\" | \\ grep -v \"^NAME=\" | \\ grep -v \"^OPTERR=\" | \\ grep -v \"^OPTIND=\" | \\ grep -v \"^OSTYPE=\" | \\ grep -v \"^PIPESTATUS=\" | \\ grep -v \"^POSIXLY_CORRECT=\" | \\ grep -v \"^PPID=\" | \\ grep -v \"^PS1=\" | \\ grep -v \"^PS4=\" | \\ grep -v \"^SHELL=\" | \\ grep -v \"^SHELLOPTS=\" | \\ grep -v \"^SHLVL=\" | \\ grep -v \"^SYSTEMD_PID=\" | \\ grep -v \"^UID=\" | \\ grep -v \"^USER=\" | \\ grep -v \"^_=\" | \\ cat - &gt; \"$HOME/.systemd-env\" echo \"PATH='$PATH'\" &gt;&gt; \"$HOME/.systemd-env\" exec sudo /usr/sbin/enter-systemd-namespace \"$BASH_EXECUTION_STRING\"fiif [ -n \"$PRE_NAMESPACE_PATH\" ]; then export PATH=\"$PRE_NAMESPACE_PATH\"fi# Create the enter-systemd-namespacesudo vi /usr/sbin/enter-systemd-namespace#!/bin/bashif [ \"$UID\" != 0 ]; then echo \"You need to run $0 through sudo\" exit 1fiSYSTEMD_PID=\"$(ps -ef | grep '/lib/systemd/systemd --system-unit=basic.target$' | grep -v unshare | awk '&#123;print $2&#125;')\"if [ -z \"$SYSTEMD_PID\" ]; then /usr/bin/daemonize /usr/bin/unshare --fork --pid --mount-proc /lib/systemd/systemd --system-unit=basic.target while [ -z \"$SYSTEMD_PID\" ]; do SYSTEMD_PID=\"$(ps -ef | grep '/lib/systemd/systemd --system-unit=basic.target$' | grep -v unshare | awk '&#123;print $2&#125;')\" donefiif [ -n \"$SYSTEMD_PID\" ] &amp;&amp; [ \"$SYSTEMD_PID\" != \"1\" ]; then if [ -n \"$1\" ] &amp;&amp; [ \"$1\" != \"bash --login\" ] &amp;&amp; [ \"$1\" != \"/bin/bash --login\" ]; then exec /usr/bin/nsenter -t \"$SYSTEMD_PID\" -m -p \\ /usr/bin/sudo -H -u \"$SUDO_USER\" \\ /bin/bash -c 'set -a; source \"$HOME/.systemd-env\"; set +a; exec bash -c '\"$(printf \"%q\" \"$@\")\" else exec /usr/bin/nsenter -t \"$SYSTEMD_PID\" -m -p \\ /bin/login -p -f \"$SUDO_USER\" \\ $(/bin/cat \"$HOME/.systemd-env\" | grep -v \"^PATH=\") fi echo \"Existential crisis\"fi# Edit the permissions of the enter-systemd-namespace scriptsudo chmod +x /usr/sbin/enter-systemd-namespace# Edit the bash.bashrc filesudo sed -i 2a\"# Start or enter a PID namespace in WSL2\\nsource /usr/sbin/start-systemd-namespace\\n\" /etc/bash.bashrcsudo cp -a ~/.systemd-env /root/.systemd-env#Finally, exit and launch a new session. You do not need to stop WSL2, a new session is enough#can't set the locale; make sure $LC_* and $LANG are correct#https://www.cnblogs.com/skiloop/archive/2013/02/20/2919266.htmllocalesudo locale-gen zh_CN.UTF-8 备注与还原123456789101112131415161718192021222324252627282930#https://www.howtogeek.com/426562/how-to-export-and-import-your-linux-systems-on-windows-10/#Backupwsl --list -vwsl --export Ubuntu-20.04 \"D:\\Developer\\WSL\\Ubuntu-20.04.tar\"# wsl --import Ubuntu D:\\Developer\\WSL D:\\Developer\\WSL\\Bak\\Ubuntu-20.04.tarwsl --unregister Ubuntu-20.04wsl --import Ubuntu-20.04 \"D:\\Developer\\WSL\\Ubuntu-20.04\" \"D:\\Developer\\WSL\\Ubuntu-20.04.tar\" --version 2wslconfig /setdefault Ubuntu-20.04#Import后默认用户变成了root，需要在windows命令行中执行：ubuntu2004.exe config --default-user dave#Movehttps://github.com/pxlrbt/move-wsl#Move CentOSwsl --list -vwsl --export CentOS \"D:\\Developer\\wsl\\CentOS.tar\"wsl --unregister CentOSwsl --import CentOS \"D:\\Developer\\wsl\\CentOS\" \"D:\\Developer\\wsl\\CentOS.tar\" --version 2mydistro config --default-user dave#Move Docker's wsl datawsl --list -vwsl --export docker-desktop \"D:\\Developer\\WSL\\docker-desktop.tar\"wsl --unregister docker-desktopwsl --import docker-desktop \"D:\\Developer\\wsl\\docker-desktop\" \"D:\\Developer\\wsl\\docker-desktop.tar\" --version 2wsl --list -vwsl --export docker-desktop-data \"D:\\Developer\\WSL\\docker-desktop-data.tar\"wsl --unregister docker-desktop-datawsl --import docker-desktop-data \"D:\\Developer\\wsl\\docker-desktop-data\" \"D:\\Developer\\wsl\\docker-desktop-data.tar\" --version 2 其他参考 https://www.jianshu.com/p/f59f902fd885","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"EnglishPod","slug":"EnglishPod学习方式","date":"2020-01-03T00:38:25.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2020/01/EnglishPod学习方式.html","link":"","permalink":"http://blog.gcalls.cn/2020/01/EnglishPod学习方式.html","excerpt":"学习完ESLPod完强烈建议学习这套。","text":"学习完ESLPod完强烈建议学习这套。 原因学习完ESLPod完强烈建议学习这套，原因： 每次的对话几乎都是有口音的，从第一期开始那个意大利餐厅的服务生意大利口音就重，到后来的美国各地的口音，英式口音，印度的墨西哥的等等都有。符合！ 所有节目都是近似节目脱口秀的形式播出，主持人讲解细心，以一种固定形式播讲英语，符合！ 材料难度分级，从基础，中级，中高级，到高级，让学习者慢慢过渡，不仅是说话速度难度加大，高级的内容也多为演讲大学授课等形式，比日常对话词汇难度大，符合！ 所有内容都非常搞笑，有点冷笑话的意思！主持人讲解生动，去官网留言还会有主持人的细心解答。符合！ 虽然是音频，不能直接放图片，但是音效做的非常好，场景利于学习者想象，在讲解某些生词的时候会放出音效让人知道是什么样子，图片和场景的感觉通过音效表达的栩栩如生，EP的工作团队要赞一个！符合！ 输入输出，不用说了吧。符合！ 我拿了EP300期的脚本做了词频统计，所有生词，大约在6000个左右，足以表达生活中的各个方面，不管是日常对话还是正式场合演讲，这么多词 汇，够了。在听完300期以后，听力问题解决后，我还是建议大家好好的多读些报刊，从华尔街日报开始读（网上主页中英对照，免费），纽约时报的电子版（免 费）。在搞定听说，积累主动词汇这方面，EP，全胜！ 所有学习材料选择最重要的两条，i+1原则和情绪机制，EP主要是用近似正常语速的腔调播报，间或主持人的搞怪和幽默，我强烈推荐觉得VOA慢速太简单而 VOA常速又太难的童鞋听这个，而且我建议把它当成流行歌曲放MP3，在精听完一期后，平时有空就放开，有一搭没一搭的多听。 至于觉得EP有点快的童鞋那么ESL最适合你不过了，ESL符不符合Krashen的二语获得理论不需要我论证了，因为它是得到过Krashen本人亲自认可的，官方认证就是牛！Krashen在接受记者采访时候被问道，你丫理论讲的一套套的，有没符合你这么龟毛条件的材料啊！Krashen回答，有！那就是最出名的ESL播客！说实话这老头居然还没死，每一个在我教科书里出现没死的人我都大吃一惊。 根据两百个小时精听搞定听力的原则，慢速50小时，常速150小时左右，大家听完150期的ESL再听完900期的Englishpod听说就完全过关了，前提条件是有效输入，就是集中精力的情况下听力 跟读。可是EP没出道900期，所以大家还是购买正版，支持EP团队再接再厉吧。 顺带一提，学英语的材料，其它的都可以抛弃了，把EP现在出的几百期反复听个好几遍，单词句法都记得，英语想不流利都难。 对如此科学的材料，就说平时带着听听模仿简直就是焚琴煮鹤，一定要下定决心，有系统的在一年之内狠狠的把这材料学个好多遍，之后再有半年阅读和背单词时间，英语就可以做到彻底获得了。 注意！englishpod适合中高级英语学习者，如果英语不太好或者初学者就不要尝试了，容易打击信心，不过如果只学里面的初级部分，也是能够轻松驾驭的。下面是englishpod的介绍。 EnglishPod的音频教程分为4个等级： 1234B－Elementary 初级 C－Intermediate 中级 D－Upper Intermediate 中高级 E－Advance 高级 学习方式 盲听录音5遍：不看文本，盲听录音至少5遍，尽可能听懂大意，并把没听懂的部分记下来。 查阅文本：查阅对话文本，把没听懂的部分查找出来，分析没听懂的原因，做好备注。 二次盲听录音3遍：经过文本的查阅，已经能理解对话的意思，也知道了自己没听懂的地方，重新不看文本去听，做一个验证，看是否已经能全部听懂。 对话材料的分析学习：前面三个步骤只能保证大概理解对话的意思，还需要具体去分析文本中的语言点，包括语法、词汇、句型等，做好相应的笔记，保证能把材料中的知识点理解透。 跟读对话3-5遍：看文本跟着录音去朗读，第一遍可一句一句跟读，往后可以多句跟读，跟读过程中要注意模仿发音和语调。 复述对话3遍：不看文本，不听录音，仅靠记忆把对话复述出来。有复述不了地方可翻查文本，直到能完全复述出来为止。 录音与原音频对比：把自己朗读的对话录音下来，与原音频做对比，发现自己朗读的差异，反复修正，直到满意为止。 音频和文本的复习：最后一个步骤则是复习，因为有了前面非常细致的学习，复习只需要非常简短的时间即可。只要能定期听一下录音或翻一下文本，复习效果就能很好。 总结就是： 12345678910盲听三遍对照文本再次盲听一遍语言点的细究和学习语音现象的分析跟读对话两遍复述对话并录音将录音和原对话进行比对每学习十期内容后进行一次大复习坚持学习 如果想按我介绍的方法听一期完整的englishipod，一期分3部分，每部分不到2分钟，大概一小时左右就可以完全吃透了。贪多嚼不烂，学十篇新文章，不如把一篇文章学十遍，就是这个意思。 比如原来是一小时学习一篇1000字左右的文章，现在可以只看一段文章，直到把这部分内容完全吃透。 听力听力有三层境界。一层听懂慢速；二层听懂常速；三层听懂不同口音和音色。听力的内力提升需要从慢速过度到常速，最后再到适应不同口音和不同人的音色。口语也有三层。一层日常对话，二层流利表达，三层深度表达自我。 泛听步骤 一篇常速的录音下载到手机，无限循环泛听。（至少听30遍，一周循环三篇即可） 泛听，不需要你听懂每一个单词，只需要你仔细听，用心去理解。 泛听是为了磨耳朵，打通耳朵与英语之间的“隔膜” 如果听烦了，可以试着复述，挑战下自己。 注意：如果觉得自己能力提升很快，可以放弃慢速材料，直接拿常速做精听。 达到第三层的方法：明星访谈／美剧／脱口秀＋真人外教对话练习 美剧适合练泛听，不适合练精听。泛听是每日必做之事，初练时最少30分钟，保持每天听听新闻等好习惯就可以了。 练习诀窍：还是遵循重复的原则，看一遍听一篇肯定没用。所以要反反复复的观看，不要看中文字幕的。看的过程中记得记下你觉得地道或者你不知道的表达，这样才会有积累，有进步。 精听步骤 听录音三遍，不看文本，尽力去理解文章大意。（后两遍重点听第一遍听不懂的地方） 查看文本，弄懂所有生单词以及词组并记录下来，日后需要背诵和复习。 再次听录音三遍。听了三遍之后，生单词和词组其实你已经记忆的熟练了。 跟读三遍。何为跟读？录音放一句，你暂停录音，然后跟着重复一句。注意模仿录音的发音、口音、语气。这么做是为了纠正发音、剔除中式口音。这样学出来的发音和老外最近接。 复述三遍。何为复述？发录音，全程不暂停。录音说一句，你紧跟一句直至结束。 上述步骤，每一步根据自身接受情况，重复次数无上限。 复习阶段。第二周复习第一周听的文章。（每周3至5篇就即可） 精听每周三次即可，如果强度比较大，可以降低强度，但需要保持至少每周三次的频率。 严格听写第一遍大概写一部份词句，然后一遍一遍补全，第三遍开始可以反复使用暂停键。反复听写，一遍一遍直至认为写不出的部分已经不是听的问题，而确定是生僻词等造成的，再开始对照文稿。千万别听写第一遍成稿就对照文本。如果是认识的词有听出来，请反复依照音频中的声音来进行模仿，把它当成声音来模仿而不是单词。然后通读，把逻辑理顺，再听几次全文，保证自己再遇到同样的情况 可以顺利过关。 要点听写将所听内容的每句话的核心内容听写 要点复述不要记笔记，在听完某段录音后，迅速暂停听力，并复述之后前1-3分钟的录音的大概内容。针对复述，可以采用外语和母语两方式的复述，外语复述可以练习口语，母语复述可以练习信息存储后再转化成母语的脱口速度。 听力到后期要大大依赖于阅读的积累。 练听力的同时，口语的磨炼绝不可忽视，所以练听力最好结合口语，让听力-口语这条“输入-输出”的线真正联通贯穿进来。练听力的时候要适当丰富不同的口音。 主动语汇的积累基础比较好：2-3制，基础比较差：3-2制。 仔细阅读释义，体会怕有例句，一定要注意词的搭配方式，把字典中觉得很好的例句，修改几个词变成自己感兴趣的话题相关的句子。 2-3制：就是修改2个例句，然后 仿造3个句子 3-2制：就是修改3个例句，然后 仿造2个句子 仿造时一定不要打开字典，也不要看刚才修改过的例句，要尽量凭空，想一个自己最近想谈论的话题或相关的事。做完后再返回字典看看此词条的上下文有没有衍生词、源头词等，一并学习。要去尽量使用一些你最近增加的积极词汇量，这很重要。根据自己的遗忘规律，定期复习： 每天不要超过但5个，每一定要每天进行。 被动词汇的积累按照正常步骤按部就搬就可以。 参考 https://kknews.cc/education/5z968b2.html https://kknews.cc/education/6kygzq.html http://m.sohu.com/a/208645965_100051699","categories":[{"name":"English","slug":"English","permalink":"http://blog.gcalls.cn/categories/English/"}],"tags":[{"name":"English","slug":"English","permalink":"http://blog.gcalls.cn/tags/English/"}]},{"title":"English二语习得方法","slug":"English二语习得方法","date":"2019-11-17T08:44:00.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2019/11/English二语习得方法.html","link":"","permalink":"http://blog.gcalls.cn/2019/11/English二语习得方法.html","excerpt":"二语习得讲究2000个小时的可理解性输入。只有达到至少2000小时的可理解性输入，才能习得一门语言。","text":"二语习得讲究2000个小时的可理解性输入。只有达到至少2000小时的可理解性输入，才能习得一门语言。 学习方式听力输入要从听入手，而且要听各种口音的英语，母语国家和非母语国家的人讲的英语都要多听，因为成年人的耳朵有障碍，听不标准音有对比才能打通，发音才能变好。 狭窄输入Narrow Input在某一段时间内要只听一种特定类型的材料，比如喜欢新闻就在几个月里只听新闻，喜欢听有声书就只听有声书，听熟了后再接触其他材料。 可理解性+1的听力输入就是i+1，i代表你现在的水平。听力一定要在自己听懂的水平上稍微难一点点，然后逐级过渡，一开始就是正常语速，那叫无效输入，跟杂音没两样（针对成人，小孩子没关系） 情绪机制学习材料要有趣让人感兴趣，学习者在学习过程中始终感到愉快有收获，挫折感不是太重。 图像理论一开始一定要重视图像和语言联系的作用，这样就直接是听到外语反映图像，而不是听到外语，反映中文，再是图像。 输入和输出理论输入一定要重视！只有在输入达到足够多的情况下，口语和写作的输出在会流利正确，甚至有很多人专门练习输入从未练过输出但是去国外后畅通无阻的。 听说在前先达到一千个小时左右的有效输入，这样听力和口语问题彻底解决后，前期积累的5000左右单词都是主动词汇，然后再大量接触各种原版读物，增加词汇量。随后再练习写，彻底解决英语问题，跟骑自行车一样，彻底获得英语，而不是总是在苦苦的煎熬学习英语。 推荐教材赖世雄美语从头学系列零基础或基础差的同学最好花一年时间学习这套教材，强烈推荐。 ESL Pod播讲类慢速英语，建议学习完赖世雄美语从头学学习这个，学习大概200集左右。学习方式： 12341.听一遍音频2.看一遍文本3.再听音频4.再听音频 第一遍听音频时，如果理解度是50-60% 第四遍听的时候可以达到80-90% 这就够了。 如何到达2000个小时的可理解性输入？成人可自行根据自己每天的空闲时间进行划分。可以每天1小时，2小时，3小时都行。除以天数定个计划。再根据自己的学习情况分为每天解决5分钟还是10分钟的视频分钟数。例如：每天两小时的可理解性输入，每天吃透10分钟。与孩子不同的是，需要把视频里的生词挑出来，挑重点词即可，有些用处不大的词可过滤掉。背了再继续看和听，这样才能把不可理性输入转为自己的可理解性输入，然后就是慢慢升级英语资料的难度。 English Pod学习完ESLPod完强烈建议学习这套。EnglishPod的音频教程分为4个等级： 1234B－Elementary 初级 C－Intermediate 中级 D－Upper Intermediate 中高级 E－Advance 高级 具体学习方式详见EnglishPod学习方式 练习方式泛听每一个新单元都先播放两遍，测试自己能听懂多少内容。 看着文本听第三遍看着英文文本搭配外教音频，目的是要让你能结合平面的文字与立体的声音。此时可以小声地跟读，但是重点还是要听母语人士的发音。 看着中文听很多人觉得口语书的对话太简单了，所以无须看中文，其实中文译文部分可以拿来作中英互译训练使用，这对日后的日常交流会起到非常大的作用，毕竟我们是中文为母语的人士。 精听这一步是要强化对声音的记忆，此阶段的精听可以在任何时间、任何地点进行，例如排队和等待搭车等任何闲暇时间。 跟读练习接着就是直接开口跟读，但是务必不要看着文本念，国人习惯依赖课本，当课本盖起来就不敢念了，其实把课本盖上后去想怎么说才是最重要的步骤。 跟读方式: 不看文本，播放一句暂停，然后跟读，如果跟读不了，倒带反复。 翻开文本不暂停，跟着录音反复朗读，一气呵成。 丢开文本不暂停，跟着录音反复朗读，一气呵成。 一人分饰两角最后可以一人分饰两角，自己把对话练习出来，可以训练对话思维。这么做有助于日后轻易说出流利的英语。注意，不一定要一字不漏地背出对话，而是要真正地融会贯通。","categories":[{"name":"English","slug":"English","permalink":"http://blog.gcalls.cn/categories/English/"}],"tags":[{"name":"English","slug":"English","permalink":"http://blog.gcalls.cn/tags/English/"}]},{"title":"A Guide to English Grammar","slug":"A-Guide-to-English-Grammar","date":"2019-08-28T01:01:32.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2019/08/A-Guide-to-English-Grammar.html","link":"","permalink":"http://blog.gcalls.cn/2019/08/A-Guide-to-English-Grammar.html","excerpt":"A guide to english grammar.","text":"A guide to english grammar. 句子的形成 S+V 任何一个句子一定由主语又subject，简写成S）及动词（verb简写成V）形成。 123456789约翰工作很努力John works hard.他钢琴弹得很好He plays the piano well.玛丽似乎喜欢学英文Mary seems to like learn English.Mary seems to be fond of learning English. 祈使句： 有时主语可以省略，而形成祈使句。这种祈使句句首为原形动词，之前省略了You should（你应当）。 12345678910111213努力工作Work hard.=(You should) work hard.安静Be quiet.=You should be quiet.别游手好闲Don&apos;t fool around.别傻了Don&apos;t be silly. 可作主语的词类名词12345这孩子喜欢说谎The child is apt to tell lies.黄金很值钱Gold is of much value. 代词12345他总是信守承诺He always keeps his promise.爬山蛮好玩的It&apos;s a lot of fun to climb mountains. 动名词或不定式短语123456看到他就令我生气Seeing him makes me angry.我来这儿是为了看他I come here to see him.To see him is my purpose of coming here. 动名词作主语，通常用以表示已知的事实或曾经做过的经历 12345678910集邮是他的爱好之一 Collecting stamps is one of his hobbies.=It&apos;s one of his hobbies (to collect stamps真正主语).听音乐使我快乐 Listening to music makes me happy.=It makes me happy to listen to music. It&apos;s interesting learning English.(可)=It&apos;s interesting to learn English.(佳) 不定式作主语通常表示一种意愿、目的或未完成的事 123456789101112出国念书是我最大的愿望。 To study abroad is my greatest desire.=It&apos;s my greatest desire to study abroad.到日本游玩是我今年的计划 To go to Janpan for a visit is my plan for this year.=It&apos;s my plan for this year to visit Japan.覆水难收It no use crying over spilt milk.向他求救是没有用的It no use asking him for help. 名词性从句作主语可作主语、宾语或在be动词之后作表语。名词性从句的种类: 1234名词性从句包括宾语从句、主语从句、表语从句、同位语从句。名词性从句一共有三种:l ) that从句2 ) whether从句3）疑问词所引导的从句 that从句123456789任何一个主语起首的句子：他不读书令我生气he doesn&apos;t study makes me angry.-&gt; That he doesn&apos;t study makes me angry.他不相信我说的话That he doesn&apos;t believe my words.有很多工作要做That there is lot of work to do. whether从句1234567891011121314151617181920212223242526可用yes/no回答的问句变化而成：a）问句有be动词时：他是否快乐Is he happy? -&gt; whether he is happyb）问句有一般助动词（Can、will、may、should、ought to、must、have）时，主语与助动词还原，前面冠以whether：他是否能做这件事Can he do it? -&gt; whether he can do it.他是否已做好此事Has he done it?-&gt; Whether he has done it.c）问句有do、does、did等助动词时主语与助动词还原，再将do、does、did去掉，后面的动词依人称和时态变化:他是否来了Did he come? -&gt; whether he came.他是否喜欢它Does he like it? -&gt; whether he likes it. 疑问词所引导的从句本从句系由疑问词（when、what、how、where、why）等引导的问句变化而成，这种由疑问词引导的问句称为特殊疑问句。 问句有be动词时： 12345主语与be动词还原，前面保留疑问词。他正在做什么What is he doing? -&gt; what he is doing. 问句有一般助动词时： 12345主语与助动词还原，前面保留疑问词。他在哪里能找到它Where can he find it? -&gt; where he can find it. 问句有do、does、did等助动词时： 123456789101112131415161718192021222324252627主语与助动词还原前面保留疑问词，再将do、does、did去掉，动词依人称和时态变化。他写什么What did he write? -&gt; what he wrote.他何时来When did he come? -&gt; when he came.他怎么做这件事How does he do it? -&gt; how he does it.who/what/which为疑问代词，若在问句中作主语，变成名词性从句时结构不变:谁来这儿Who came here? -&gt; who came here.昨晚发生什么事What happended last night? -&gt; what happended last night.哪个被买走Which was bought? -&gt; which was bought. 名词性从句的功能: 名词性从句作主语： 1234567891011“诚实为上策”是一句我们应时刻谨记在心的蔑言That honesty is the best policy is a proverb (which) we should always keep in mind.=It&apos;s a proverb we should always keep in mind that honesty is the best policy.他住哪里仍不确定 Where he lives is still in doubt.=It&apos;s still in doubt where he lives.他能胜任与否仍有待观察 Whether he can do it remains to be seen.=It remains to be seen whether he can do it. 名词性从句作及物动词的宾语： 及物动词在英文中称作transitive veth，简写为vt.；宾语则称作object，简写为o． 12345678我知道他最近即将出国I know that he will go abroad in the near future.我想知道他是否已做完这份工作I wonder whether he has finished the work.我不知道他将如何处理这件事I don&apos;t know how he&apos;ll handle it. 作介词的宾语： 介词在英文中称作preposition，简写成prep. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566a）此时仅能用whether从句或疑问词引导的名词性从句作宾语· that从句不可作介词的宾语。我很担心他是否能做这件事I am worried about whether he can do it.我很好奇他将如何应付这问题I am curious about how he&apos;ll cope with the problem.b）遇有介词，且非要使用that从句时其补救方法如下：(1) 介词+the fact+that从句:建议使用:我确定这一队赢了这场比赛I am sure of that the team has won the game.(x)I am sure of the fact that the team has won the game.(the fact that).我担心他不念书I am worry about that he doesn&apos;t study.(x)I am worry about the fact that he doesn&apos;t study.(2) 保留介词，不加the fact但介词之后的that从句要做下列变化:第一步：除去that;第二步：that后的主语变成所有格；第三步：动词变成动名词。不常用:I am worried about that he plays around all day.(x)I am worried about the fact that he plays around all day.I am worried about his(or him) playing around all day.(a) that从句若有助动词do、does、did时，予以去掉即可。I am worried about that he doens&apos;t study.(x)I am worried about his not study.(b) that从句若有助动词will或would时，予以去掉即可。我确信这一队会赢得这场比赛。I am sure of that the team will win the game.(x)I am sure of the team&apos;s winning the game.(c) 从句若有以下助动词时，做下列变化：may -&gt; be likely tocan -&gt; be able to我很高兴他可能会来。I am happy about that he may come.(x)I am happy about his being likely to come.我确定他能做这件事。I am sure of that he can do it.(x)I am surce of his being able to do it.(3) be + adj. + that从句：也就是去掉介词，将that从句放在形容词后面，作其宾语。建议使用I am worried about that he plays around all day.(x)I am worried that he plays around all day.有时介词之后亦可直接接that从句而不需the fact，形成惯用语，亦应牢记。in that=because 因为他很有才华，因为他能说五种语言。He is talented in that he can speak five different languages.except that 只可惜、除了他人很不错，只可惜有时会说谎。He is nice except that sometimes he tell lies.=despite the fact that 尽管=in spite of the fact that尽管他人好，我却不喜欢他。Despite the fact that he is nice, I don&apos;t like him. 名词短语作主语名词短语系由“疑问词＋不定式短语”形成。 疑问副词 1234where to live 住哪里whether to try again 是否再试一次when to talk to him 什么时候和他谈how to do it 如何做那事 疑问代词 1234what to do 做什么whom to See 看谁whlch to buy 买哪一个whom to talk to 与谁谈 1234567在哪里见他还未决定。 Where we should meet him is not decided yet.=Where to meet him is not decided yet.我们如何处理这问题要视我们能募集到多少钱而定。How we can handle the problem depends on how much money we can collect.How to handle the problem depends on how much money we can collect. 表距离的地方副词短语作主语from+地方名词+to+地方名词:此为表距离的地方副词短语，亦可作主语，与单数的be动词连用。 123456 From A to B is about 1000 km.=It&apos;s about 1000 km from A to B.从上海到北京大约1000公里From Shanghai to Beijing about 1000 kilometers.It&apos;s about 1000 kilometers From Shanghai to Beijing. 动词的种类及其用法动词（verb）基本上可分为五大类12345678910a．完全不及物动词：(complete Intransitive veth，简写：c.vi.)b．不完全不及物动词：(incomplete Intransitive veth，简写：i.vi.)c．完全及物动词：(complete transitive verb，简写：c.vt.)d．不完全及物动词；(incomplete tra力sitive verb，简写：i.vt.)e．授与动词：(dative verb简写：d.v.) 如何判断完全不及物动词是否及物动词的判断, 以下翻译成中文若无毛病，就是及物动词，否则就是不及物动词，准确率高达99%：12我___他(主动)他被我__(被动) 123He wrote the book.The book was written by him.Something happened. be动词123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657581）名词：他是大英雄He is a great hero.2）名词性从句：我的麻烦是缺钱。My trouble is that I lack money.问题是他是否能加人我们。The problem is whether he can join us.3）名词短语：问题是何时出发The question is when to set out.4）作名词用的动名词短语：我的兴趣是集邮My hobby is collecting stamps.5）作名词用的不定式短语：我来这里的目的是要见他。My purpose here is to see him.6）形容词：她很美。She is beautiful.7）作形容词用的现在分词：这故事真有趣。The story is interesting.8）作形容词用的过去分词：我对这故事感兴趣。I am interested in the story.9）作形容词用的介词短语：此类介词短语由“ of＋抽象名词”形成。这本书很有价值。 The book is of great value.=The book is valuable.那部机器毫无用处。 The machine is of no use.=The machine is useless.10）地方副词：她在那儿。She is there.他们在楼上。They are upstairs.他现在在家吗？Is he home now?11）地方副词短语：她人在城里。She is in town.他们现在在家。They are at home now.她有危险。She is in danger. be动词之后的现在分词有两种词性： 一作形容词，一作动词进行时的现在分词。换言之，同学遇到v一ing；可译成”⋯的”时，就是作形容词的现在分词，否则就是表“进行状态”的现在分词，要译成“正在⋯”，而不视为形容词。 1234这女孩很迷人The girl is charming.这女孩在唱歌。The girl is singing. be动词之后的过去分词亦有两个词性： 一作形容词用，可译成“感到⋯的”；一作及物动词的被动语态，译成“被而不视为形容词。 1234这个人很累The man is tired.这男的被杀了The man was killed. becomebecome可用任何名词、形容词或可作形容词用的现在分词或过去分词（即可以译成“ ⋯ ⋯的”之类的分词）作表语。 12345678他生气了。He become angry.如果你用功，就可成为好学生。If you study hard, you will become a good student.她愈来愈迷人。She is becoming more and more charming.失败后，他变得意志消沉。After failing, he became depressed. turnturn通常只用形容词作表语，而且所能使用的形容词多与颜色或情绪有关。 12345678听到那消息时，他脸色变得苍白起来。His face turns pale when he heard the news.树叶变黄了。The leaves were turning yellow.看到玛丽时，他气得脸都红了。His face turned red with angry when he saw Mary.她变漂亮了She become beautiful. getget通常用表“生气”或“激动”的形容词作表语。若用其他形容词时，宜用become 。 12345678910111213141516他生气了。He got mad.她愈来愈美丽。She is getting more and more beautiful.事情愈来愈顺利。Things are getting better and better.她变漂亮了She become beautiful.get+p.p（被动语态）=be+p.p（被动语态）他在车祸中丧生 He got killed in the accident.=He was killed in the accident.你如果不小心点，就会受伤。If you are not careful, you will get hurt. seem=appear，之后用不定式作表语。123456789他似乎知道此事。He seems to know it.但在seem to be+名词/形容词结构中，to be可省略，直接用名词或形容词作表语。他似乎很快乐He seems to be happy.=He seems happy.(to be可以省略掉)他似乎是个好人He seems a nice man. 感观动词感官动词一共有五个，一律译成“⋯起来”，之后一律用形容词（或可作形容词用的分词）作表语。 look/sound/smell/taste/feel + adj 12345678910他的主意听起来不错His idea sounds good.His idea sounds like a good idea.这食物闻起来很香The food smells good.我现在觉得很疲倦I feel tired now.It looks interesting.它看起来很有趣It sounds like a good idea. look/sound/smell/taste/feel + like + noun 12345678like为介词译成“像”，之后的名词为其宾语。听起来像是个好主意It sounds like a good idea.那质料摸起来像丝The material fells like silk.这食物尝起来像鱼肉The food tastes like fish. feel like + noun: 感觉像feel like + 动名词: 想要 12345当简说她要嫁给我时，我觉得我就像个新生儿一样。When Jane said she would marry me, I feel like a newborn baby我今天想散散步I feel like taking a walk today.I would like to take a walk today. feel、taste、smell亦可当完全及物动词，可用名词作宾语。此时feel、taste、smell的意思均有改变。 12345678feel/taste/smell + noun.医生摸我的额头说我发烧了The doctor feel my forehead and said I had a fever.别尝那食物，它已经馒了Don&apos;t taste the food, it has spoiled.他闻到有东西烧焦了He smells something burning. like与介词连用时，可形成短语动词，视为及物动词。 12345678910look into = investigate 调查look over = examine 检查look at = watch 看他愉快地看着我He looked me happily.He looked happily at me.他仔细地调查这件案子He looked into the case carefully.He looked carefully into the case. 不完全不及物动词的重要相关短语12345678fell ill(fall ill)约翰病了，因此今天无法上学John fell ill and couldn&apos;t attendd school today.fell asleep(fall asleep)他很快就睡着了。He soon fell asleep.我不知道他怎么会变得如此受人欢迎。I don&apos;t kown how he became to be so popular. 完全及物动词的用法完全及物动词的宾语，计有名词、代词、不定式、动名词、名词性从句及名词短语等。同学稍加注意，就可发现这些词类、短语或从句均有名词的特性，故亦可用作主语。 名词当及物动词的宾语 12他没有买那部车He didn&apos;t buy that car. 代词当及物动词的宾语 12我不喜欢约翰。事实上，我讨厌他I don&apos;t like John. In fact, I hate him. 不定式当及物动词的宾语 这些动词皆有表示某种愿望、企图之意，常用的此类动词如：want（想要）、hope（希望）、 desire（想要）等。 1234我现在就要见他I want to see him now.我希望有一天能出国留学I hope to study abroad someday. 动名词当及物动词的宾语 并非所有及物动词均可用动名词作宾语。常以动名词作宾语的动词有下列几个： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253considered/imagined/fancy/risked/avoided/escaped/suggested/recommended/enjoy/resent/practiced/stopped/quit/mind他考虑出国念书He considered studying abroad.我曾想像和他去约会I once imagined going on a date with him.他冒险做这事He risked dong it.他建议去国外旅行He recommended traveling abroad.我喜欢跳舞I enjoying dancing.我讨厌要做这么多工作I resent having to do so much work.我练过钢琴了I practiced playing the piano.他戒烟了He stopped/quit smoking.你介意为我打开窗户吗？Would you mind opening the window for me?resist V一ing 抗拒...refuse to V 拒绝...我拒绝做此事I refused to do it.I resist doing it.我忍不住要再吃些冰淇淋I couldn&apos;t resist having some more ice cream.expect to V 期望=anticipate V一ing我期望和他一起环游世界I expect to travel around the world with him.I anticipate traveling around the world with him.stop V一ing 停止=quit V一ing他结婚后就不再写作了He stopped writing after he got married.他决定戒烟He decided to quit smoking.stop to V 停下来而去...他看见我时，便停下来和我说话He stop to talk to me when he saw me.continue（继续）、start（开始）、bogin（开始）、like（喜欢）love（爱）、hate（恨），可用不定式或动名词作宾语意思不变:他继续在赌场里试他的手气。He continued trying(/to try) his luck at the casino.他们开始建这栋房子They start building(/to build) the house. 名词性从句作及物动词的宾语12345678910111213141516171819202122232425262728293031323334353637名词性从句有三种：1) that引导的名词性从句that he can&apos;t do it （他不能做此事）2) whether引导的名词性从句whether he will come （他是否会来）3) 疑问词引导的名同性从句when he will do it （他何时做）what he is doing （他正在做什么）how he&apos;ll handle it （他如何处理此事）为了方便记忆起见，我们可将名词性从句记成“戴慧怡” 。戴：即that之讹音慧：即whether第一音节之讹音怡：即疑问词从句“疑”之同音词由于名词性从句具有名词的功能，故亦可作及物动词的宾语。我相信他是个言而有信的人He believ that he is a man of his word.我不知道他是否能够做此事I don&apos;t know whether he can do it.我不知道他住哪里I don&apos;t know where he lives.that从句作及物动词的宾语时，名词性从句连接词that通常予以省略。他们觉得不可能赢这场比赛They feel (that) it&apos;s impossible to win the game.他认为这本书值得一读He think (that) the book is worth reading.whether从句作及物动词的宾语时，名词性从句连接词whether亦可被if取代。我不知道台风是否会来。I don&apos;t know whether the typhoon will come=I don&apos;t know if the typhoon will come.我不知道这个人是否有足够的能力处理此事。I don&apos;t know if the man has enough ability to handle the it.I doubt if the man is competent enough to handle it. 但whether从句若作主语、介词的宾语或者be动词后的表语时，不能用if: 12345678a. whether从句作主语：他是否能去还不知道。Whether he can go is not yet know.b. whether从句作介词的宾语：他很担心是否能通过这次考试。He is worried about whether he can pass the exam.c. whether从句作be动词后的表语：The problem is whether he has enough money for the trip. 名词短语1234我知道如何处理这问题。I know how to handle the problem.我不知道该向谁说。I don&apos;t know whom to talk to. 不完全及物动词123本句意思不完全，故需补语。他使玛丽快乐。He made me happy. 使役动词 叫 make/have + 宾语 + 原形动词 123456789101112我叫他洗车I made him wash the car.I had him wash the car.I got him to wash the car.我叫约翰向我报到。I had John report to me.get亦可表“叫⋯ ”之意，但只能用不定式短语作宾语补语，句型如下：get sb to V 叫某人从事⋯我叫他洗车。I got him wash the car. make可使用于被动语态中但have及get则不可。 主动语态：12I made(had) him wash the car.I got him to wash the car. 被动语态: 变被动语态时，原形动词wash就要变成不定式短语to wash，而成to wash the car。have、get无被动用法。12他被要求洗车He was made to wash the car. 让 12345678910111213141516171819202122232425a. let＋宾语＋原形动词（作宾语补语）我让他洗车I let him wash the car.我让彼得试试看I let Peter try it.b. let＋宾语＋作副词用的介词（in、out、down)他让我进来He let me in.别让他出去Don&apos;t let him out.他的表演令我失望His performance let me down.上述介词由于其后无宾语，可单独存在作副词用，因此称为介副词。与地方副词（如there 、 here 、 home）一样，介副词可作be动词之后的表语:他在家He is inHe is outHe is there.He is here.He is home.He is downtown.He is in the house. 强迫/要求/怂恿/催促 此类动词＋宾语＋不定式短语（作宾语补语）force/ask/encourage/tell+不定式 123456789101112131415我强迫他背这课I forced him to recite the lesson.我要求他写这封信I asked him to write the lettle.我鼓励他更努力些I encouraged him to work harder.他强迫我做违反意愿的事He forced me to do it against my will.他叫我10点钟以前做完这件工作He told me to finish the work by 10.此类动词尚有许多个，兹将常出现的此类动词列举如下：push one to... （催促某人去⋯ ⋯）expect one to... （期望某人去⋯ ⋯）want one to... （要某人去⋯ ⋯） 使…成为 此类动词只有make一个，加了宾语之后，要用形容词或名词作补语: 主语+make+宾语+形容词(或名词)（作宾语补语）1234他的老师使他成为好学生His teacher made him a good student.这次旅行使他很愉快The trip made him happy. 知觉动词此类动词有三类：看：see、observe（观察）、watch（看）、lookat（注视）、notice（注意）听：hear、listen to感觉：feel 此类动词可作完全及物动词，加了宾语之后无须另加补语 123456你看见他没有Did you see him?他正在听音乐he was listening music.我觉得背痛I felt a pain in my back. 但此类动词亦可作不完全及物动词，加了宾语之后，须另加宾语补语。用法如下： 123456789101112131415161718192021222324a. 表事实时，用原形动词作补语，译成“...了”我看见他跳舞了I saw him dance.我听见他唱歌了I heard him sing.我感觉到他动了I felt him move.b. 表进行状态时，用现在分词作补语，译成“...正在...”我进来时，看见他正在跳舞I saw him dancing when I walked in.我推开门时，听见他正在唱歌I heard him singing when I pushed the door open.As I pushed the door open, I heard him singing.门铃响时，我感觉到我的双腿正在发抖。When the doorbell rang, I felt my legs trembling.c. 表被动状态时，要用过去分词作补语，译成&quot;...被...&quot;我看见他被杀死了I saw him killed.我听见门被关起来了I heard the door closed.我感觉到自己被举起来了I felt myself lifted. 认定动词此类动词均表“视⋯为”之意，加了宾语之后，用名词或形容词作宾语补语。 与介词连用者： 视…为：与as或for连用： 1234我们都把他视为天才We regard(look upon/think of/see/view) him as a genius.我误将那长发的男孩视作女孩I mistook the long一haired boy for a girl. 与to be连用者： 12345我们认为他是天才／人很好。We consider/deem/think him to be a genius.We consider/deem/think him a genius.We consider/deem/think him to be nice.We consider/deem/think him nice. a. 一般介词之后只能用名词、代词或动名词作宾语 12345678我很讨厌他粗鲁的态度I am sick of his rude attitude.我讨厌与他为伍I am sick of associating with him.但表“视...为”的动词与as或for连用时，它们之后除可接名词外，亦可直接接形容词作补语。我认为他的行为不当I regard his behavior as inappropriate. b. think、believe、find、deem、consider等五个动词作不完全及物动词时，不得直接用不定式短语作宾语，一定要用形式宾语it取代。此时think译成”认为⋯是⋯”、 deem译成“认为…是…”、consider译成“认为…是…” 123456789101112131415主语+think(believe/find/deem/consider)+it+宾语补语(n. or adj.)+to V.我认为爬山是很有趣的―我认为爬山很有趣。I think it fun to climb the mountains.我发觉做这件事是有必要的―我发觉做这事有必要I find it necesssary to do the work.我相信用功是值得的―我相信用功很值得。I believe it worthwhile to study hard.但find、believe、think、deem、consider也可作完全及物动词此时要用that引导的名词性从句作宾语。此时find、believe、thlnk、deem、consider分别译成“发现、相信、认为，认为、认为” 。我认为爬山很有趣I think that it&apos;s interesting to climb the mountains.我相信用功是值得的I believee that it&apos;s worthwhile to study hard. c. make表“使…成为…”时，亦为不完全及物动词，其后不得用不定式短语作宾语，一定要用式宾语it取代。 12努力工作使他有可能赢得此荣誉Hard work made it possible for him to win the honor. d. find、believe、think、deem、consider乍不完全及物动词时，亦不得直接用that引导的名词性从句作宾语一定要用形式宾语it取代。 1234我发觉他歌唱得这么好真是太棒了I find it wonderful that he sings so well.我相信他努力用功是有必要的I believe it necessary that he should work hard. 转变动词此类动词均表”使⋯变成⋯”之意常用的有change与加rn两个。通常要与介词into连用。 12这个经验使他变成好学生了。The experience turned/changed him into a good student. 其他重要的不完全及物动词12345678我将门漆成绿色1 painted the door green.他们给小宝宝取名为汤姆They named the baby Tom.别让门敞开着Don&apos;t leave the door open.你一定要让你的牙齿保持干净You must keep your teeth clean. 授予动词1234我要送你这只表I&apos;ll give you this watch.请把你的书借我Please lend me your book. 表“给予”的概念要用to： 123我给他这本书I gave him the book.=I gave the book to him. 表“代劳”的概念要用for： 123我为他买了这本书I bought him the book.=I bought the book for him. 表“从⋯中”的概念，要用of： 1234567891011121314151617我问他一个问题I asked him a question.=I asked a question of him.我对他不抱很大的期望I didn&apos;t except much of him.他对我毫无所求He required nothing of me.他抢了我的钱He robbed me of my money.我的钱被抢了I was robbed of my money.我无法摆脱这段痛苦的回忆I cannot get rid of the painful memory.你应该戒除抽烟的恶习You should (get rid of|break yourself|rid yourself) the bad habit of smoking.医生治好了他的病The doctor cured him of his disease. 表提供：offer/provide/supply/furnish/present, 除offer外，要与with连用: 123456他提供我所需要的东西He offered me all I needed.=He provided me with all I needed.他送我一块表He presented me with a watch.=He presented a watch to me. explan/introduce/propose/recommend/express要与to连用: 12345678他向我解释理由He explained the reson to me.他把那位女孩介绍给我He introduced the girl to me.他建议我用这个方法He recommended the method to me.他向委员会提了一个提议He proposed a motion to the committee. take a picture 1234#我拍了一张他的照片I took a picture of him.#我为他拍了一张照片I took a picture for him. 其他1234567Never try it.=Don&apos;t ever try it.#How之后只能与单数名词, what没有限制. 只记how+adj，what+noun就可以What a great man he is!=How great he is!=How great a man he is!How hard he studied! 两句的连接方式两句可能用以下的符号连接： 1234561. 破折号 2. 冒号 3. 分号 4. 并列连词 5. 副词连词 6. 关系词 并列连词1234567and -&gt; both...and.. 一方面...同时也...or -&gt; either...or... 要不就是...要不就是... neither...nor... 既非...亦非...but -&gt; not...but... 并非...而是... not only...but also... 不仅...而且...as well as... 以及rather than... 而非 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950他非但不仁慈，反而很残忍He is not kind but cruel.他不是走就是留He will either stay or leave.她不仅会唱歌，她也会跳舞Not only can she sing, but she can (also) dance.我们和约翰都喜欢它We as well as John like it.他很快乐，而不是伤心He is happy rather than sad.#as well as或rather than连接主语时，动词始终随第一个主语做变化他对音乐有兴趣，而非对绘画有兴趣He is interested in music rather than in painting.他来此地学英文并了解更多有关生活的事情He came here to learn English as well as to know more about life.他和我都对音乐有兴趣He as well as I is interested in music.他对音乐有兴趣，而我则不He rather than I is interested in music.=He instead of me is interested in music.=He,not I, is interested in music.我喜欢这本书，而不喜欢那本I like this book, not that one.=I like this book rather than that one.#除both用复数外，其它随最近的主语变化：他与我对此事都高兴#他和我都...(不可连接句子)Both he and I are happy about it.不是他就是我错#不是他就是我...(不可连接句子)Either he or I am wrong.既不是你也不是他该负责既不是你也不是他...(不可连接句子)Neither you nor he is to blame.不是他而是我应对此事负责#不是他而是我...(不可连接句子)Not he but I am to be responsible for it.#不仅...而且...(可连接句子，引导句子时要倒装)Not only they but(also) John has passed the exam.Not only can she sing, but she can also dance.Not only does she sing well, but she dances beatifully.She is not only beautiful but (also) kind.=She is not only beautiful bot kind as well. 副词连词常用的的副词连接词：123456because（因为）though（虽然）if（如果）unless（除非）as soon as（一旦）once（一旦）等 条件句与主句的时态 12345678910111213141516171819when:当 一般现在时(或现在完成时)动词，主语+will+原形动词。if:如果 once/as soon as:一旦 unless:除非我有钱时就会买辆车When I am rich, I will buy a car.他若有时间就会来If he has time, he&apos;ll come here.我一做完就会让你知道As soon as(Once) I have done it, I&apos;ll let you know.我一旦有空就会出国旅行Once I am free, I will take a trip abroad.除非他明天回来，否则我就要走了Unless he comes back tomorrow, I will leave.他还要两年才毕业It will be another two years before he graduates.他还要多久才来？How long will it be before he comes? 连接性副词 12345678910111213141516171819202122232425262728293031323334however: 然而(有but的意思): 后面要加逗号，句中两旁用逗号相隔。他人很好，然而我却不喜欢他He is nice; however, I don&apos;t like him.He is nice, but I don&apos;t like him.He is nice. However, I don&apos;t like him.He is nice. I, however, don&apos;t like him.therefore/thus：因此(有so的意思): 后面要加逗号，句中两旁用逗号相隔。(thus不用加逗号)他很仁慈因此我们全都喜欢他He is kind; therefore, we all like him.He is kind, so we all like him.He is kind. Therefore, we all like him.He is kind. We, therefore, like him.moreover/furthermore/in addition：此外（有and的意思）: 后面要加逗号，句中两旁用逗号相隔。她舞跳得好，而且也擅长唱歌She dances well; furthermore, she is good at singing.She dances well, and she is good at singing.besides与in addition均表“此外”，不过前者多用于有否定或消极意味的句中，而in addition则用于肯定句或积极意味的句中。我想我不会跟你去看电影了，约翰。我太累了；此外，我也没钱。I don&apos;t think I will go to the movies with you, John. I&apos;m too tired; besides, I have no money.while: #句首时：当... While I was writing a letter, she was doing the sishes.#连接并列句时：而...He is nice, while(whereas) his brother is bad.While he is nice, his brother is bad. as: 当、虽然、因为123456789#当我...As I was young, I was quite happy.As rich as he is, he isn&apos;t happy.#虽然...=Thouth he is rich, he isn&apos;t happy.=Rich as he is, he isn&apos;t happy.#因为...As he has apologized to me, I&apos;m going to forgive him.Because he... where: 在…的地方12345=in the place where.=in the place in whichWhere I live, people are very nice.in the place where I live, people are very nice.Put the book where I can see it.=Put the book in the place where I can see it. once、as soon as: 一旦 or: 否则12You must work hard, or I&apos;ll fire you.=You must work hard; otherwise I&apos;ll fire you. nor: 也不, nor之后的分句要倒装12She can&apos;t sing, nor can she dance.He isn&apos;t clever, nor is he diligent. so that: 这样的话、也便, 从句中应有助动词may/wll/can等12I got up early so that(in order that) I could get to school on time.=I got up early so I could get to school on time. so…that…: 如此…以致于…12He is so nice that we all like him.=He is so nice, we all like him. in that: because。置于主句之后1He is great in that he speaks five different languages. except that: 只可惜、只不过是1He is nice except that he is a little shy. since: 因为、自从1234#由于...Since you&apos;ve finished the work, you may leave anytime.#自从...He has been studying quite hard since he went to college. 关系词关系代词关系代词种类及功能1234567人： who/whom物： which/which句子:which/whicha. 关系代词之前要有先行词b. 关系代词在引导的定语从句中要作主语、宾语或表语c. 否则关系代词之前一定要有介词, 介词可移到句尾 123456789He is a good student who studies hard.He is a boy whom everyone loves.He is a man for whom I enjoy working.=He is a man whom I enjoy working for.The house in which we live is very large.=The house which we live in is very large.#which代替smokes three packs of cigarettes a dayHe smokes three packs of cigarettes a day, which definitely does harm to his health. 非限制性定语从句该关系代词前一定要有逗号，按顺序翻译即可。 123I met John, who is my classmate.Here comes my father, who is an English teacher.I went to the train station this morning, which was crowded with passengers. 限制性定语从句该关系代词前不要有逗号，翻译成”…的” 1234He is a boy who works hard.I like the book which he bought yesterday.This is my mother, who loves me very much.(对)This is my mother who loves me very much.（错） 关系代词的省略限制性定语从句中，若关系代词为及物动词的宾语，则该关系代词可省略。若关系代词为介词的宾语，介词移到定语从句的句尾时，则该关系代词可省略。 123456789This is the boy whom I saw yesterday.(vt)=This is the boy I saw yesterday.This is a question which you should pay attention to.(prep)=This is a question you should pay attention to.This is the house in which he lives.=This is the house which he lives in.=This is the house he lives in. that关系代词a. that之前不能有逗号 b. that之前不可有介词 以上满足时可取代who、whom、which. 123I like the girl that(who) is sitting there.This is the knife whth which he killed Marry.(有with，不能用that取代which)=This is the knife that(which) he killed Marry with. 下列情况下，只能用that 序数词：the first/second…/last12Gambling is the last thing that I would do.He was the first man that came up with such an idea. 最高级后，只能用that：12He is the best student that I&apos;ve ever taught.The most beautiful scene that I&apos;ve everseen is Guilin. all后，只能用that：1234All the students (that) are studying here are hardworking.All (that) he said is true.Everything (that) he said is true.#以上时，that可以省略掉 the only后，只能用that：1He is the only friend that I have. 关系代词在be动词后作表语，只能用that：12#他已不是当年的他了He is not the man that he once was. 其他：12Who that lies can win our trust?What is it that you want? 限制性定语从句可化简为分词适语12345678910The girl who danced here yesterday is my sister.=The girl dancing here yesterday is my sister.Who is that boy that is standing over there?that is-&gt;being-&gt;省略=Who is that boy standing over there?I like the car which is parked in front of the post office.which is&gt;being-&gt;省略=I like the car parked in front of the post office. 非限制性定语从句不能化简为分词短语。但Who/which + be + n可以化简为同位语123456789I met John, who is a good friend of mine.who is-&gt;being-&gt;省略=I met John, a good friend of mine.He works hard, which is a fact that is known to all of us.=He works hard, a fact that is know to all of us.John, a good friend of mine, came to see me yesterday.=A good friend of mine, John came to see me yesterday. 定语从句与插入语的关系12345I believ(我相信)I think/consider(我相信)I feel(我觉得)I know(我知道)I find(我发现) 123456#他是个从不食言的人He is a man who never breaks his word.He is a man who I think never breaks his word.He is the man whom I believe I met yesterday.He is a man who I think never breaks his word. 12345I think that he is nice.=I think him to be nice.He is a man who I think is nice.=He is a man whom I think to be nice. 定语从句可简化为不定式短语关系代词所引导的定语从句中，若该关系代词之前有介词时，可将该从句换成不定式短语。有点“…目的”的意思： 1234567He has no house in which he can live.=He has no house in which to live.=He has no house to live in.Lend me your pen with which I can write my address.=Lend me your pen with which to write my address.=Lend me your pen to write my address with. 关系代词所有格his/your/their/our/its…变化而成： 12345whosea. 关系代词所有格之前要有先行词b. 关系代词所有格在引导的定语从句中要作主语、宾语或表语c. 否则关系代词所有格之前一定要有介词, 介词可移到句尾 12She has an uncle. His name is Peter.=She has an uncle whose name is Peter. whose+n.=the+n.+of which=of which+the+n.:123This is a fancy car, whose color I like very much.=This is a fancy car, the color of which I like very much.=This is a fancy car, of which the color I like very much. 关系副词关系副词用以修饰名词 介词+关系代词+which:1234where=in which/on which/at which: 修饰表地方的名词when=in which/on which/at which: 修饰表时间的名词why=for which: 修饰的reasonhow=in which: how和the way只能用其中之一 1234This is the city where(in which) I met Marry.He came on Friday, when(on which) it was raining very hard.I don&apos;t know the reason why he cried.That&apos;s how(the way) he handled the thing. where在…地方(在…内/上或其他) 12345This is the city in which I met Mary.=This is the city where I met Mary.Do you see the desk on which there is a book.=Do you see the desk where there is a book. when那时、那天、那年… 年月：in 日子：on 时间：at12345He came on Friday, on which it was raining hard.=He came on Friday, when it was raining hard.That&apos;s the year when he graduated.The sun rose at 5:30, when most people were still in their dreams. why只作修饰the reason，the reason可以省略掉 123I don&apos;t know the reason for which he cried.=I don&apos;t know the reason why he cried.=I don&apos;t know why he cried. howhow和the way只能用其中之一，表…的方式/方法 1234567That&apos;s the way in which he handled the problem.=That&apos;s the way he handled the problem.I don&apos;t like the way he talks.I am sick of the way he looks at me.=I am sick of how he looks at me. 关系副词使用要点在be动词后作表语时，可省略该名词，只保留关系副词 12345678That is the place where he was born.=That&apos;s where he was born.That is the day when he&apos;ll come.=That&apos;s when he&apos;ll come.That is the reason why he left.=That&apos;s why he left.That is the way he did it.=That&apos;s how he did it. where/when/why/how可视为疑问代词，引导名词从句，此时可作及物动词的宾语 123I konw where he is.I wonder why he left.I don&apos;t know how he&apos;ll do it. where可作副词 12345Put it where you found it.=Put it in the place where found it.=Put it in the place in which found it.Where there is smoke, there is fire. the moment(when)… 一…就… 12345678#他一听到这个悲伤的消息就哭了起来As soon as he heard the sad news, he burst into tears.#他来的时候，我正在睡觉The time(when) he came, I was sleeping.=When he came, I was sleeping.#他离开的那一天，正在下雨The day(when) he left, it was rainning.=When he left, it was rainning. 复合关系代词复合关系代词种类由两个词类复合而成：先行词(名词)+关系代词 123456789101112131415#代替人whoever(凡...的人)、任何...的人=anybody whowhomever(宾格)=anybody whom#代替人或物whichever(三者以上)=any one which(/who): 同一类的任何一个...whichever(二者)=either which(/who): 同一类的任何一个...#代替物what=the thing(s) which: 所...的东西whatever=anything which: ...的任何东西 whoeverwhoever=anybody who 1234567891011Anybody should be punished.who makes such a mistake.Anybody who makes such a mistake should be punished.=Whoever makes such a mistake should be punished.I hate anybody.who tells lies.I hate anybody who tells lies.=I hate whoever tells lies. whomever12345Give it to anybody.whom you like.Give it to anybody whom you like.Give it to whomever you like. whichever是指同一类的任何东西 12There are many book here. You may take any one which you like.There are many book here. You may take whichever you like. whatthe thing/the things which. 所…的(东西/事/话…): 前面不要有先行词(名词) 可作主语、宾语或在be动词后作表语 123456What he said is true.=The words which he said is true.I am interested in what he is doing.What bothers me is that I have no time.I don&apos;t believe what he said. whatever与whichever类似，不同在于whatever指不同类的任何东西，whichever指同一类的任何东西 12345Whatever he said is true.=Anything which he said is true.I am interesting in whatever he is doing.=I am interesting in anything that he is doing. whatever及whoever作副词连词12345678whatever=no matter what: 不论什么whoever=no matter who: 不论谁Whatever he says, I don&apos;t believe him.=No matter what he says, I don&apos;t believe him.Whoever does it, he should finish it by ten.No matter who does it, he should finish it by ten. whatever及whoever作副词连词, 引导的状语从句与主句中间有逗号相隔，而复合关系代词时没有。 1234567#他所说的任何话都是谎言Whatever he says is a lie.#不论他说什么，那都是谎言Whatever(No matter what) he says, it is a lie.Whoever makes the mistake should be punished.Whoever(No matter who) makes the mistake, he should be punished. however连接性副词: 然而 123He is nice; however, I don&apos;t like him.=He is nice. however, I don&apos;t like him.=He is nice. I, however, don&apos;t like him. 副词连词：无论如何 12345No matter how nice he is, I don&apos;t like him.=However nice he is, I don&apos;t like him.No matter how hard he works, I don&apos;t respect him.=However hard he works, I don&apos;t respect him. whether表无论是否只需whether即可，不可使用no matter whether，引导状语从句，以逗号分隔。 引导名词性从句，无逗号相隔，表是否。 12345Whether he is rich(or not), Jane will marry him.#他是否富有都没关系Whether he is rich(or not) doesn&apos;t matter.(无逗号相隔)=It doesn&apos;t matter whether he is rich(or not). however12However poor he is, he doesn&apos;t wnat anyone to help him.=However poor he may be, he doesn&apos;t want... 准关系代词that/as/but 123I have more money than you(do).He has as much money as I(do).There is nothing but he knows. thatthat + the 前面的动词 + 关系代词(who/whom/which), 可作主语、宾语、be动词后的表语 1234567891011121314#主语I have more mony than is needed.than=that the money which，所以that中已经包含了主语，不用再添加主语More guests that were invited came.There are more books that are needed.#宾语I have more money than he needs.I have more books than you do.more...that=比...多#be词语后的表语He is a better student than you. as12345678910111213141516171819202122232425262728293031323334#像...那样的...such+n.+as...#主语I have such car as you.I don&apos;t like such a man as tells lies.#宾语I respect such an honest man as you described.#be动词后的表语In our company there is not such a man as David(is).Such a lazy student as John will get nowhere.#和...相同的...the same+n.+as...#主语I have the same book as is laid on the desk.as=as the book which#宾语I have the same dictionary as you bought yesterday.#be动词后的表语I have the same dictionary as this one(is).#和...一样的...as+adj.+n.+as...#主语#他是有史以来最勤奋的人He is as diligent a man as ever lived.as=as any man that#宾语#我有一本和你一样好的词典I have as good a dictionary as you(do).#be动词后的表语#他是个和John一样好的人He is as good a man as John.as=as the man that 12345678910Such a man like him is bad.(x)Such a man as he is bad.the same...as...: 和...相同的...(两个人或物)the same...that...: 就是同一个...(一个人或物) #我有一本书，与他正在看的那一本相同I have the same book as he is reading.#我借了一本书，就是他上周借的那一本I borrowed the same book that the borrowed last week. as等于which，用以代替整个主句, as可以移到句首，但which不能 123He is nice, which we all know.=He is nice, as we all know.=As we all know, he is nice. 123He was drunk, as usual.His conditions are as follows.His listed the names as follows. 只能与单数可数名词使用的结构： 12as good a student as...as diligent a man as... 1234567891011121314151617181920212223#以下都只能用单数#so...that...: 如此...以致He is so good a boy that I like him.#as...as...: 和...一样地...He is as handsome a man as Peter.This music is as good as that.#too...to...: 太...而不...He is too old a man to do it.This work is too difficult to do.THis is too difficult a job to do.#how...: 多么地...How great a man he is!#what可接任何名词What good students they are!He has so many things to do that he can&apos;t go picnicking with us.There is so much work to do that I don&apos;t think I can go to bed early.He has too little money left to buy a car.I have as much money as he.I don&apos;t know how much time you can give me for that job. such…that可置任何名词 12This is such good music that I love it.They are such good students that I like them. but作准关系代词时，只用于”no+名词”之后，等于who(whom/wich)…not。 12345678910111213141516There is no+名词+who(whom/which/that)...not=There is no+名词+but...#没有...是不...#没有人不对它感兴趣There is no one who is not interested in it.=There is no one but interested in it.#没有什么东西是他不喜欢的There is nothing which he is not found of.=There is nothing but he is fond of.There is nothing but he can do.There is no book but he likes to read.=There was no noe but he hated. 非谓语动词不定式、分词、动名词 不定式不定式的功能当名词、形容词、副词： 12345678#名词To take a trip around the world has been my dream.#形容词I have something to do(to do修饰something).#副词I came to see her.(to see her当副词修饰came) 名词不定式的用法当作主语时，可用It is…to V替代:12It is my goal to marry her.It is difficult to learn English. 名词不定式作及物动词的宾语： want/desire/intend/expect+to+V，但anticipate+动名词。 注意：think也有想要的意思，但只用以表示“想到某一件事”，通常用that引导名词性从句作宾语1234I think that he is great.I thought of Mary a while ago.I have been thinking of buying a car.=I have been considering buying a car. 1234567891011S + find(发现) + it + 名词/形容词(作补语) + 名词不定式(真正宾语) think(认为) believe(相信) consider(认为) deem(认为) feel(觉得) make(使)I find it interesting to climb mountains.#我习惯早起I make it a rule to get up early. 在be动词表意愿、企图的名词：12My plan is to see him.His goal is simply to become a teacher. 形容词不定式的用法置于名词的后面，作形容词用，修饰其前面的名词： 12I have some work to do.There is one thing to be done. 形容词不定式可以等于关系代词作主语所引导的定语从句： 1234567891011I have on friend to advise me.=I have no friend who can advise me.There is one thing to be done.=There is one thing which should be done.#形容词不定式也可等于关系代词作宾语所引导的定语从句：I want something to eat.=I want something which I can eat.I have something to do.=I have something which I must do. 此类句型中，被修饰的名词一定要作不定式短语中动词的宾语，否则就要作为介词的宾语： 12He is a nice man to work with.Give me a chair to sit in. be动词后加的形容词不定式有下列意义： 1). 主动形式： a). 表将来=will 123He is to come tomorrow.=He is going to come tomorrow.=He will come tomorrow. b). 表义务=should 12345You are to do it.You should do it.What am I to do?=What should I do? 2). 被动形式： a). 表当然=should 12Such a lazy man is to be fired.=Such a lazy man should be fired. b). 表可能=can 12My hat was nowhere to be found.=My hat could not be fould anywhere. 副词不定式的用法修饰动词：12345He came to see me.He as agreed to do it.He sang a song to please her.He tends to lie.You&apos;ll soon get to know him. 修饰形容词：一定置于该形容词之后：123He is able to handle the problem.He os apt to lie.=He tends to lie.This book is good to read. 修饰副词，一定置于该副词之后：12He is too old to swim.He is old enough to go swimming alone. 疑问词+不定式=名词短语：123456where to gowhen to do ithow to swimwhat to dowhich to buywhom to see 123What to do was the question.He is learning how to do it.The problem is where to start. that从句化简为不定式P157： 单一动词:123456It seems that he works hard.=He seems(appears) to work hard.#我刚巧在那里It happended that I was there.=I happended to be there. be+过去分词:1234567It is said: 据说 + that he is good.It is reported: 据报道It is rumored: 谣传It is known: 据了解It is thought: 一般认为It is believed: 一般相信=He is said(reported/...) to be good. 如何变化：时态相同时, to之后加原型动词：1234567891011It seems that he works hard.=He seems to work hard.It appears that they are nice.=they appear to be nice.It was said that he loved music very much.=He was said to love music very much.It is said they love music very much.=they are said to love music very much. 时态不同时，to之后加“have(曾经、已经)+过去分词”：12345It seems that he worked hard.=He seems to have worked hard.It is said that he was a thug(流氓) in the past.=He is said to have been a thug in the past. to be的省略：12He seems to be nice.=He seems nice. prove+to be:12345678910111213#表证明：接名词或名词性从句作宾The investigation proved his guilt.=The investigation proved that he was guilty.#to be: 表显示、结果是：#他的话竟然是真的What he said proved to be true.#这份报告结果是错的The report proved to be false.=The report proved false.=The report turned out to be false.=The report turned out false. 使役动词加不定式短语作宾语补语P160: 注意与P35的区别，叫、让之类的直接接动词原型。1234567891011121314cause/get: 促使...lead: 引导...allow/permit: 允许...advise: 劝告...persuade: 说服...enable: 使能够...tell: 告诉...beg/ask: 请求...order: 命令...want/wish/expect/intend: 期望...The sad story caused him to cry.His speech led me to understand the importance of learning English.He advised me not to smoke again. 不定式短语作主语时，若短语中最后一个词为宾语，该宾语可移置句首作主语，其余部分则移到句尾： 12345To please my father is hard.=My father is hard to please.To get along with him is easy.=He is easy to get along with. 原型不定式1234can/could/shall/should/will/would/may/might/ought to/must + 原型动词I can do it.He should finish it. 知觉动词与原型不定式的关系看、听、感觉+原型动词 12345678看：see/watch/observe/look at听：hear/listen to感觉：feel#...了I saw him enter the room.I never heard him speak English.I felt the house shake. 使役动词与原型不定式的关系P164：12345678910111213141516let(让)、make(使)、bid(叫)、have(叫)+原型动词、当get(叫)+to+原形动词：Let me try it.=Allow me to try it.I made him wash the car.I had him wash the car.=I got him to wash the car.#make/let被动时，要改为to+原形动词I let him leave.=He was allowed to leave.The made John clean the window.=John was made to clean the window. 原形不定式的特殊结构1234567891011121314151617#do nothing but + 原形不定式He did nothing but eat all day.#choose(expect/want/disire) nothing but + to + 原形动词#He wanted to sleepHe wanted nothing but to sleep.#be interested in nothing but+动名词/名词He is interested in nothing but singing.#enjoy nothing but+动名词/名词I enjoy nothing but dancing.#cannot but+原形不定式When I heard the story, I couldn&apos;t but laugh.=When I heard the story, I couldn&apos;t help laughing.When I heard the story, I couldn&apos;t help but laugh. 分词分词包括现在分词与过去分词： 12345678#进行： be+现在分词He is doing the work.#完成：have+过去分词He has done the work.#被动：be+过去分词The job was done. 分词作形容词12345678910111213141516171819202122232425262728293031323334353637383940#表主动/被动的概念令人...的：现在分词感到...的：过去分词受到...的：过去分词#表进行/完成的概念正在...的：现在分词即将...的：现在分词已经...的：过去分词#现在分词作形容词：令人...的The student raised a confusing question.#现在分词作形容词：即将...的The retiring teacher walked into the classroom.#过去分词作形容词：已经...的The retired teacher walked into the classroom.#过去分词作形容词：置于be动词后作表语：感到...的The teacher was confused.#过去分词作形容词：受到...的: #受伤的士兵被急忙送到医院The wounded soldier was rushed to hospital.He is gone.=He has gone.We&apos;re closed=We&apos;ve closed.He is retired.=He has retired.We are finished with the work.=We have finished the work.We are sold out of this book.=We have sold out of this book. 分词作表语a. be动词(或remain/become/appear/seem等系动词)后的表语：123He is interested in it.The bady is tiring.He seems tired. b. 及物动词后的宾语补语：12I found him killed.They found him tired. 动词若为及物动词，有两种形态：12345#现在分词+宾语I found him killing cockroaches.#过去分词I found the cockroaches killed.I found him killed. 动词若为不及物动词作宾语补语时，一律变成现在分词：12I found him trembling.I kept John waiting. c. with复合结构中的宾语补语：12345He talked to me with his legs shaking.He stood with his arms folded.He talked to me with a pipe in his mouth.=He talked to me pipe in mouth. 分词作名词12345The young should learn to cherish life.=Young people should learn to cherish life.The wounded were rushed to the hospital.=Wounded people were rushed to the hospital. 分词作副词12345678910It&apos;s freezing cold today.The tea is boiling hot.An estimated 54 people were killed in the air crash.=It&apos;s estimated that 54...A great many students are more and more interested in learning English.He is dead(completely) wrong.It&apos;s awful(awfully/very) hot day. 分词结构一动词变化a. 若两个动词无连词相连时，动作同时发后，第二个动词变成现在分词, 若为be动词，变成being后省略： 1234567He came home cried.(x)He came home cring.He sat in the corner reading a newspaper.He came home was tired.(x)He came home tired. b. 若两个动词无连词相连时，动作有先后顺序，第二个动词变成to+原形动词, 若为be动词，变成being后省略： 1234He came here saw me.(x)He came here to see me.He stood up to smoke a cigarette. c. 若两个动词有逗号相隔，而无连接时，不必考虑动作的先后顺序，第二个动词一定变成现在分词： 12345He left home at six in the moring, arrived here about four in the afternoon.(x)He left home at six in the moring, arriving here about four in the afternoon.He ran away quickly,looked as if something terrible had happened.He ran away quickly,looking as if something terrible had happened. 分词结构一单句化简法两句在一起，若无连词相连时，往往第一个句子要化简，变成分词短语： a. 两句主语相同时，被简化的主语要删除，若主语主同时则要保留b. 之后的动词要变成现在分词c. 若该动词为be动词，变成现在分词being后，可以省略，但亦可不省略以强调因为的意思。 1234567891011121314151617181920212223242526He has nothing to do, he feels bored.(x)having nothing to do, he feels bored.He was sick of studying, he ran away from here.(x)Sick of studying, he ran away from here.The sun set, the cowboys rode back to the ranch.(x)The sun setting, the cowboys rode back to the ranch.He was not satisfied with the result, he decided to try again.(x)Not satisfied with the result, he decided to try again.He didn&apos;t intend to see her, he left early.(x)Not intending to see her, he left early.He had done the work, he felt happy.(x)Having done the work, he left happy.I have not seen her for ages, I miss her.(x)Not having seen her for ages, I miss her.#也可以消灭第二个句子，变成分词结构He was reading a book, his wife knitting(编织) beside him.=He was reading a book, and his wife was knitting beside him.We will go picnicking tomorrow, weather permitting.=We will go picnicking torrow if weather permits. 分词短语一定语从句化简法P184 a. 删除关系词b. 其后动词变成现在分词c. 若be动词变成being省略d. 非限定修饰的定语从句(关系代词前有逗号的)，通常不得化简为分词短语. 如果为关系代词+be+名词时，仍可化简 1234567891011The man who is talking to Mary over there is my father.=The man talking to Mary over there is my father.Who is the body that is locked behind the door?=Who is the body locked behind the door?I like Tom, who is talking to Mary.(ok)=I like Tom, talking to Mary.(x)John, who is a good friend of mine, studies hard.=John, a good friend of mine, studies hard. 分词结构一状语从句化简法once/when/while/if/unless/though等所引导的状语从句中，若主语与主句中的主语相同时，亦可化简为分词结构 once/if/unless只限：主语+be+分词/形容词 123456789101112If I am free, I will go with you.=If free, I will go with you.In those days, when he didn&apos;t know how to proceed in an emergency, he would consult his father.=In those days, when not knowing to prceed in an emergency, he would consult his father.He will do it if he is properly encouraged.=He will do it if properly encouraged.#除非另有指示,你应照我的话去做Unless you are otherwise instructed, you should do as I said.=Unless otherwise instructed, you should do as I said. 身体组织的名词可变成过去分词当形容词用12345The girl has big eyes.I love the big-eyed girl.She has red hair.Do you see the red-haired girl there? 少数现在分词当介词12345678910111213including(包括)excluding(除外)considering(考虑)regaring(关于)concerning(关于)Everyone likes the movie, including John.=Everyone likes the movie, John excluded.Considering his performance, he can be a good teacher.=His performance considered, he can be a good teacher.He wrote an article regarding(concerning/about/on) environmental pollution. 独立分词短语P189 1234567Generally speaking... 一般而言Strictly speaking... 严格说来Frankly speaking... 老实说Judging from his appearance... 从他的外表看来Talking of novels... 谈到小说According to John... 根据John的说法Seeing that... 既然... 动名词可在句中作主语、宾语及表语。 作主语123Working with him is fun.Not knowing what to do was an embarrassment for him. be动词后的表语12345678Seeing is believing.#动名词His hobby is collecting stamps.#不定式My hope(goal/desire/wish/intention/plan/purpose) is to fullfill the mission.She is dancing. 动词的宾语P195 1234567891011121314151617181920212223242526272829303132I remember seeing him.He considered traveling abroad next year.He recommended doing it soon.#allow/permit/forbid(禁止)He allowed(permitted) me to smoke.He allowed(permitted) smoking.#禁止He forbade me to smoke.He forbade smoking.#remember/forget/regretremember(记得曾) + 动名词forget(忘记曾)regret(后悔曾)I remember seeing her before.I remember having seen her before.#我后悔做了此事I regret doing it.I regret having done it.remember(记得要) + to Vforget(忘记要)regret(遗憾要)I remember to see him tomorrow.I forgot to mail this letter.#我很遗憾...I regret to tell you the bad news.=I&apos;m sorry to tell you the bad news. 下面动词用不定式或动名词作宾语，意思不变： like/love/hate/continued/gegan/started, dislike只能用动名词。 12I like to do it.I like doing it. 介词的宾语1234He is fond of taking a walk.His illness prevented us from starting our own business.None would have dreamed of there being such a place.The book is worth reading. worth: prep. 值得 123#is worth+名词或动名词(recommended)The issue is worth paying attention to.The car is worth a fortune. worthy: adj. 值得的(与of连用) 1The issue is worthly of everyone&apos;s attention. worthwhile: adj. 值得的(置于名词前，亦可置于it it之后) 123That&apos;s a worthwhile book to read.It is worthwhile to read that bookIt pays to read that book. 所有格与动名词的关系P199 123456789101112#that引导的名词性从句That he teches well is something that pleases me.=His teching well is something that pleases me.That John refused to do his homework made his father angry.John&apos;s refusing to do his homework made his father angry.That he didn&apos;t finish the work on time disappointed me.His not finishing the work on time disappointed me.That they had helped John with the work won our great admiration.There having helping John with the work won our great admiration. like/diske/enjoy/mind后不可直接接that从句 123456789I dont like his smoking here.I dont enjoy John&apos;s joining us.#如果我开窗你介意吗？Would you mind that I open the windows?(x)Would you mind my opening the windows?=Would you mind if I opened the windows.#请你开窗子，你介意吗？Would you mind opening the windows. 常用的动名词惯用语1234567891011121314151617181920#There is no V+ing: 不可能=It is impossible to VThere is no reasoning with such a stubborn man.It is impossible to reason with such a stubborn man.#It is no use V+ing: ...是无用的=It is of no use to VIt is no use trying to escape.It is of no use to try to escape.#cannot help(resist/stop) + V一ing: 忍不住...=cannot but V=cannot help but VHe couldn&apos;t help crying when he saw her.He couldn&apos;t but cry when he saw her.#on V一ing: 一...就...=As soon as/once/The moment/The instantOn(Upon) hearing it, he cried.As soon as he heard it, he cried. 助动词及易用错的动词助动词shall/will征求对方意见时：12Shall I open the door?=Would you like me to open the door? 请求对方合作时：12Shall we go for a walk?=Let&apos;s go for a walk, shall we? 命令对方时：12You shall obey the law.=You must obey the law. should主要功能就是表示一种义务，后面接原型动词：应当，=ought to1We should be kind to others. 但也会有以下的其他意思。 会123456789101112131415It is natural(自然的) that...should ....会... proper(适当的/合理的) right(正当的) advisable(恰当的) desirable(较好的) no wonder(无疑的)#他会生气是很自然的事It is natural that he should get angry.#努力的人会成功，这个是合理的It is proper that one who works hard should be successful.It is no wonder that such a naughty boy should be punished.=No wonder such a naughty boy should be punished. 应当12345678910It is necessary that...should... ...应当...是有必要的 imerative essential important urgent#should可以省略It is necessary that he (should) finish the work before leaving.It is important that he finish the job before leaving. 居然12345678It is surprising that...should(居然)... 令人惊异的是...居然 amazingit is surprising that he should be so kind.It is a pity that...should(居然)... 令人遗憾的是...居然... a regret regrettableIt is a pity that he should be so rude. 意志动词表建议(propose/recommend/suggest)、要求(ask/demand/desire/require/insist/request)、命令(order/command)、规定(rule/regulate)等后有that从句时，that从句中亦使用should，should往往省略:1234#他建议我们立刻离去He suggested that we(should) leave at once.#他们要求他安静They demanded that he(should) be quiet. 以免12He came early lest he(should) be late.=He came early for fear that he might be late.(might不可省略) 万一1If you should be late again, you will spoil the plan. should have+过去分词表示与过去事实相反的虚拟语气：早应…/应该早点…12345678#你当时要是没救我，我就死了If you had not helped me, I should have died.#你应该早点做好的You should have done it earlier.#这么好的景色，你当时真该看看的It was such a fine sight. You should have seen it. 表示”居然已经”:1234#令人惊讶的是他居然已通过了考试It is surprising that he should have passed the examination.#真遗憾他竟然做了这么愚蠢的事It is a pity that he should have done such a stupid thing. wouldwould是will的过去式，用以表示过去将来时123He said, &quot;I will try again.&quot;He said that he would try again.Did you know when he would come? would虚拟语气would+原形动词： (如果…)就会…would have+过去分词 与现在事实相反：12#如果我现在有钱，我会买部车(但我现在没有钱)If I had money now, I would buy a car. 与过去事实相反：12#当时我要是有钱，早就买车了If I had had money then, I would have bought a car. would rather+原形动词宁愿…1234#我宁愿走也不要留在这儿I would rather go than stay here.=I would sonner go than stay here.=I would as soon go as stay here. I would rather that我多么希望…. = I wish + that从句，属于虚拟语气，若与现在事实相反，that从句用一般过去式，若与过去事实相反，则用过去完成式： 与现在事实相反：123#我多么希望他现在在这里I would rather (that) he were here.=I wish (that) he were here. 与过去事实相反：12#我多么希望他昨天就在这里I would rather he had been here yesterday.(But he wasn&apos;t here.) would you mind加Ving或if从句： 您介不介意…=Do you mind… 1234Would you mind doing it for me? if you did it for me?=Do you mind doing it for me? if you do it for me? would you mind会比较客气。would you mind从句中用过去式，do you mind从句中用现在式。 can/could1.表”能力”：be able to，译成：能够12He can cope with the problem.=He is able to cope with the problem. 2.表可能性，译成：有可能123#他要是这么说就可能错了He can be wrong to say so. if he says so. 3.表许可，=may，译成：可以1234#你可以现在回家了You can go home now.#你不可以在这儿抽烟You can&apos;t smoke here. 4.在疑问句或否定句中，表可能性，译成：有可能或不可能12#这可能是真的吗？不，这不可能是真的Can it be true? No, it can&apos;t be true. 5.表对过去事实的否定推理123456can&apos;t have+去过分词 不可能曾...#他这么老实不可能昨天会偷你的钱He is so honest that he can&apos;t have stolen your money yesterday.#can仅用以表示对过去事物的否定推论，无法做肯定推论。无下列用法he can have stolen your money.(x) 6.对过去事实的肯定推论，有两种句型：1234567891011must have+过去分词 一定曾经...#他看过来紧张兮兮，一定是偷了你的钱He looks nervous; he must have stolen your money.may have+过去分词 可能曾经...#他看过来紧张兮兮，很可能偷了你的钱He looks nervous; he may have stolen your money.Can+S+have+过去分词 有可能...吗？#他有可能偷了你的钱吗？Can he have stolen your money? 7.would/could用在问句中表客气1Could(would) you please do it for me? 8.could=was/were able to 9.can的惯用语1234567891011121314151617#我只能这么做I can but do so.=I can only do so.#我忍不住大笑I cannot but laugh.=I cannot help bu laugh.=I cannot help laughing.#cannot be too...: 再...也不会过#我们在择友时再小心也不为过We cannot be too careful in choosing friends.#我一点儿也不在乎(我不能低于比现在不在乎的程度了)I couldn&apos;t care less.#我同意极了(我不能比现在更加地同意了)I couldn&apos;t agree more. may/might1.表许可，译成：可以1234You may take whatever you like.May I go home?#might与would/could一样，在问句中有客气的语气Might I make a suggestion? 2.表推测，译成可能，=can123It may be true.He may come, or he may not(come).=Perhaps he will come;perhaps he will not(come). 3.may not有两种意思： a.可能不会1It&apos;s getting late; he may not come. b.不可以=must not12You may not cheat during exams. must not 4.might表过去的状况12He said, &quot;You may go&quot;He said that I might go. 5.might+原形动词 也许会… might have+过去分词 当时也许会… a.与现在事实相反1If he tried hard, he might succeed.(But he doesn&apos;t try hard.) b.与过去事实相反1If he had tried hard, he might have succeeded.(But he didn&apos;t try hard.) 6.may的管用语 a.may well+原形动词 大可/足可以…12#genius:天才He knows so many things that he may well be called a genius. b.may as well+原形动词 不妨…(语气缓和) had better+原形动词 最好…(语气较强)1234#你不妨留在家中You may as well stay home.#你最好留在家中You had better stay home. c.may as well+原形动词+as+原形动词 与其…倒不好…=had better+原型动词+than+原形动词12#你倒不如留在家里，与其和他们出去。You may as well stay home as go out with them. may as well…as…表较大的可能性，might as well…as…表较小的可能性1You might as well die as make friends with Mary. must1.must接原形动词 2.表义务，表“必须”1You must finish the work before leaving. 3.must表一种义务上强制的必须，have to，有勉强的意味，表“有必要”1You must love your country. must只用于表示现在或将来，而have to则可用于表过去、现在及将来的状况123456#现在You must come now.#将来You must come tomorrow.#过去He will have to come tomorrow. 否定式： 1) must not=may not 不可以12#你绝不可以做这事You must not do it. 2) don’t have to=need not 不必12#你不必做这事You don&apos;t have to do it. 口语中，have to被have got to或gotta取代123456I have to tell you the truth.I&apos;ve got to tell you the truth.(I&apos;ve可以直接读成I)I gotta tell you the truth.但:He has got to tell you the truth.=He&apos;s totta tell you the truth.(s不能去掉) 4.must对现在状况或过去状况的推论 a.对现在状况的推论：must+原形动词 一定…1234#这一定是真的It must be true.但否定时，不能用It must not be true.(x)，还用：It can&apos;t be true.(它不可能是真的) b.对过去状况的推论：must+have+过去分词 一定曾经…1It must have rained last night. ought to与should使用相同 need1.需要123He needs to go.The car needs to be cleaned.The car needs cleaning. 2.否定：need not+原形动词 不必123#need not为固定，need第3人称不用加sHe need not go.=He doesn&apos;t need to go. dare1.助动词1234567dare not+原形动词 不敢...dare one+原形动词 某人敢...吗？#他不敢去He dare not go.#他敢去吗?Dare he go? 2.How dare+一般陈述句？ …怎敢…?1How dare you say such a thing to me? 3.在whether从句中，dare可作助动词12#我怀疑他敢不敢做这事I wonder whether he dare do it. 4.在肯定中，dare为一般动词1234#他敢去He dares to go.#他当时敢去He dared to go. 5.dare not+原形动词 不敢…1234567=do not dare(to)+原形动词#他不敢接电话He dare not answer the phone.=He doesn&apos;t dare(to) answer the phone.#我从来不敢与他说话I have never dared(to) speak to him. 6.dare可作及物动词，表“向某人挑战”12345dare sb to+原形动词=challenge sb to+原形动词#他向我挑战跳过这条小溪He dared me to jump the stream. 7.I daresay+that从句 我敢说…12#我敢说他又会迟到I daresay he will be late again. used to1.used to+原形动词 过去曾经…12345678#他以前住过这儿He used to live here.#这座园子以前有个池塘There used to be a pond in this garden.He used to work hard.Did he use to work hard? 2.人+be used to+名词或动名词=accustomed 某人习惯于…12345678He is used to working alone.#他还不习惯都市生活He hasn&apos;t beed used to city life yet.#他习惯大声说话He is used to speaking alone.=He is accustomed to speaking aloud. 物+be used to+原形动词 被用来…12#这本书可用来教我们英语写作The book can be used to teach us English writing. 物+be used as+名词 被用作…12#这把刀被用来当武器The knife was used as a weapon. 易用错的动词P233 fly/flow12fly的变化：fly/flew/flown/flying.flow的变化：flow/flowed/flowed/flowing.(流) take/bring12345take (从此处)拿走bring (从别处)拿来Take the book to the library, please.Bring them back here. refuse1234567891011121314refuse+名词#他拒绝我的建议He refused my suggestion.refuse to+原形动词He refused to go with me.reject(排斥)+名词或者动名词He rejected working with Mary because he thought she was too selfish.refuse后不能接that从句#他拒绝承认做错事He refused toadmit that he had done something wrong.=He denied that he had done something wrong. take/cost12345It takes+人+时间+to VIt costs+人+金钱+to VIt takes(one) about two hours to go from here to Paris.It cost(me) five dollars to buy that book. reach/get to/arrive in(at)123I |reached |Chicago at 12. |arrived in | |got to | prefer12345678910prefer+名词(或动名词)+to+名词(或动名词): 比较喜欢...胜过...I prefer coffee to tea.I prefer going to the movies to watching TV.prefer to+原形动词+|instead of+动名词 | | rather that+原形动词| I prefer to go to the movies instead of watching TV.=I prefer to go to the movies rather than watch TV.=I prefer going to the movies to watching TV. mind12345678910mind + if从句=mind + 动名词#你介不介意我把门打开？Would you mind If I opened the door?=Would you mind my opening the door?#你介不介意去把门打开？Would you mind if you opened the door?=Would you mind opening the door? resemble sb/sth像…123#你很像你哥哥 You resemble your brother.=You look like your brother. succeed/fail1234567891011121314151617181920succeed in+名词/动名词 在...获得成功/成功地...#经过多年的努力，John经商成功了John succeeded in business after years of hard work.How many of you have succeeded in passing the test.succeed+人+as 继承某人担任...的职位John succeeded his father as president of this company.fail: 失败He failed to pass the test.fail: 未通过考试、使某人不及格He failed the test.#老师让三分之一的学生不及格The teacher failed one一third of his students. flunked#不要辜负我You are the only person I trust. Don&apos;t fail me. 设计、企图1234567891011try + to + 原形动词 设法要...attempt + to + 原形动词 企图要...manager + to + 原形动词 设法要...endeavor + to + 原形动词 努力要...I&apos;ll try to study harder.=I&apos;ll attempt to study harder.=I&apos;ll manager to study harder.#我们努力拯救公司免于破产We&apos;ll endeavor to save our company from going bankrupt. remember/forget/regret12345678remember to V 记得要...remember +Ving 记得曾...forget to V 忘了要...foret + Ving 忘了曾...regret to V 抱歉/遗憾要...regret + Ving 后悔曾... 时态种类一般时123456781. 一般现在时He writes a letter every day.2. 一般过去时He wrote a letter yesterday.3. 一般将来时He will write a letter tomorrow. 完成时123456781. 现在完成时He has written the letter.2. 过去完成时He had written the letter when I came.3. 将来完成时He will have written the letter before I come. 进行时123456781. 现在进行时He is writing a letter now2. 过去进行时He was writing a letter when I came.3. 将来进行时He will be writing a letter when I come. 完成进行时123451. 现在完成进行时He has been writing a letter for two hours.2. 过去完成进行时When I called, he had been writing a letter for two hours. 使用过去时的时机主语+一般过去时动词 1234567891011121314151. 表过去的动作、习惯、状态：I saw him yesterday.He always rose early in the morning.He was born in 19702. 表过去的经验，也可用现在完成时表示： Did you ever see her before?=Have you ever seen her before?3. 句中若有after/until/before等连词引导的从句，由于时间先后很清楚，可用一般过去时代替过去完成时： After I had finished the work, I went home.=After I finished the work, I went home.#我搬来这里之前曾在日本住过很长一段时间 Before I moved here, I had lived in Japan for a long time.=Before I moved here, I lived in Japan for a long time. 使用现在完成时的时机主语+have+过去分词(have译成“已经”或“曾经”)： 123456789101112131415a. 用以表示到现在为止完成的动作：I have just finished my homework.b. 用以表示到现在为止的经验：#我见过他几次I have met him several times.#我从未见过像他这样的人I have never met such a man as he.c. 表曾去过的经历用：have been，表已经到了某个地方：have gone#他已经到香港去了(还没有回来)He has gone to Hong Kong.#他曾去过香港He has been to Hong Kong. 使用过去完成时的时机12345主语+had+过去分词(had译成“已经”或“曾经”)，用以表示截至过去某时为止所完成的动作或经历：He had studied English for 10 years before he left for the States.He had already left when I came. 过去完成时不能单独存在，要与另一个一般过去时从句或表过去的副词短语连用： 先发生的动作—&gt;用过去完成时表示后发生的动作—&gt;用一般过去时表示 1I lost the book which my father had given me. 使用将来完成时的时机主语+will+have+过去分词(will have译成“将已经”), 用以表示到将来某时为止所完成或仍在继续的动作或经历等： 1234#他将已抵达芝加哥He will have arrived in Chicago by this time tomorrow.#他就在此住满10年了He will have lived here for 10 years by the end of this month. 使用现在进行时的时机主语+be动词+现在分词(正在…)，不过也可以表示即将发生的动作，常与表将来的副词连用：此时be动词译成：即将… 123456#他即将于今天来 He is coming today.=He will be coming today. #他们很快就要离开 They are leaving soon.=They will be leaving soon. 使用现在完成进行时的时机主语+have/has been+现在分词(have/has been译成“一直都在…”), 用以表示一直继续到现在且仍将继续下去的运作： 12She has been working with this company for 5 years.They have been standing here since 7 this morning. 使用过去完成进行时的时机主语+had been+现在分词(had been译成“一直都在”)，与过去完成时一样，过去完成进行时不能单独存在，须与一般过去时连用。 12I had been sleeping when he came.He told us that he had been studying German since 2001. 使用将来完成进行时的时机主语+will have been+现在分词(will have been译成“将一直在”)，表一直继续到将来某时，而仍将继续进行的动作： 12到今年年底，我教英语将已有10年了I will have been teaching English for ten years by the end of this year. “for+一段时间”与完成时的关系12345678910for the |past| five years 过去5年来during |last|inthroughdown throughover=since five years ago 自从5年前起这些短语与现在完成时或现在完成进行时连用#过去两个星期来，我都一直在学英语I have been studying English for the past two weeks. since与完成时的关系since与”for+一段时间”使用差不多，不过注意： since之后须接明确的某段时间作宾语，不要接一段时间作宾语，若之后有一段时间，则须在一段时间之后接ago:12Peter has lived here since 2002.Peter has lived here since two years ago. 现在完成时或现在完成进行时的动词与“for+一段时间”连用时，该动词所表示的动作必须可接续进行，否则就不可与“for+一段时间”连用：1234567891011I have lived here for five years.He has died for two months.(x)=He has died.=It is two months since he died.=Two months have passed since he died.He has married Jane for twenty years.(x)=He has married Jane.=It is twenty years since he married Jane.=Twenty years have passed since he married Jane. 语态语态有两种：主动与被动。形成被动语态一定是及物动词： 123原句的宾语作主语+be动词+过去分词Everyone loves him.He is loved by everyone. 虚拟语气123456789101112131415a. 纯条件的虚拟语气---用现在式如果他在这儿，我会揍他If he is here, I will beat him.b. 与现在事实相反的虚拟语气---用过去时如果他现在在这儿，我会揍他If he were here, I would beat him.(But he is not here.)c. 与过去事实相反的虚拟语气---用过去完成时如果他当时在这儿，我会揍他If he had been here, I would have beaten him.(But he was not here.)d. 与将来状况相反的虚拟语气---if从句要用助动词should，译成“万一”，主句则用过去时或现在时助动词万一他在这儿，我会揍他If he should be here, I would(或will) beat him. 纯条件虚拟语气123456789If+主语+一般时动词，主语+will(may/can/should/shall/must/ought to)+原形动词If he comes, you should tell him the truth.once/when/before/as soon as/unless等副词也可以构成条件句，与if用法相同One I have money, I will buy a car.When he finishes it, he may come.I will not do anything before he arrives. 与现在事实相反的虚拟语气123456789101112131415If+主语+一般过去时动词， 主语+would(could/might/should/ought to)+原形动词If he lived in my town, I could see him every day.be动词统一使用wereIf I were rich, I would help you.因为我不是很有钱，所以我不能帮你=As I am not rich, I can&apos;t help you.主句中的助动词一定为过去时助动词will-&gt;wouldmay-&gt;mightcan-&gt;couldshall-&gt;should或者ought tomust只能表示现在或将来的状况，只用于纯条件的虚拟语气中。 与过去事实相反的虚拟语气1234567891011121314If+主语+had+过去分词，主语+would(could/might/should/ought to)+have+过去分词要是他当时听了你的劝告，就可能会避免这个错误了If he had taken your advice, he might have avoided the mistake.If he had had money, he would have bought a car.如果if从句与过去事实相反，用过去完成时；主句与现在事实相反，则置助动词过去时：If+主语+had+过去分词，主语+would(could/might/should/ought to)+原形动词+now(today)If I had started saving then, I would be able to buy a car now.If I had started saving then, I would have been able to buy a car now(x)If I had met her five years ago, she might be my wife today. 与将来状况相反的虚拟语气123456789101112131415161718192021译成&quot;万一...&quot;a. 低可能性：主句要用过去时助动词If+主语+should+原形动词，主语 would(could/could/might/should/ought to)+原形动词万一你生病，会议将延期---你不太可能生病，会议不太可能延期If you should fall ill, the meeting would be put offIf it should rain, I would stay home.b. 高可能性：主句要用现在时助动词If+主语+should+原形动词，主语+will(can/may/should/ought to)+原形动词万一你生病，会议将延期---你可能会生病，而会议也可能延期If you should fall ill, the meeting will be put off.If it should rain, I will stay home.也可能与祈祷句形成的主句连用万一我迟到，务必要等我---我可能会迟到If I should be late, be sure to wait for me. 使用虚拟语气的注意事项 表示强烈与真理相反的虚拟语气句型： 12If the cat were to smile, I would pass out.The cat can&apos;t smile. if的省略： 12345678910111213if从句中，若有过去完成时助动词had/should/were(何秀华)时，可将这3个词置于主语前，而将if省略a. had If he had done it, he would have felt sorry.=Had he done it, he would have felt sorry.b. should If he should tell lies, I would punish him.=Should he tell lies, I would punish him.c. were If he were lazy, he might fail.=Were he lazy, he might fail. 可取代if的其他连词： 123456789101112in case(that): 万一on condition that: 如果provided/providing(that): 如果so long as/as long as: 只要只要书有趣，哪本书都可以Any book will do as long as it is interesting.in case +that从句 万一...in case of+名词 万一... In case an accident happens, don&apos;t panic.=In case of an accident happens, don&apos;t panic. but for 123456789101112131415161718192021222324252627=without 若非/要不是...a. 与现在事实相反：若非...就... If it were not+that从句(一般现在时)，主语+could(would/might/should/ought to)+原形动词=But for+名词，要不是他努力工作，我才不会喜欢他 If it were not that he works hard, I wouldn&apos;t like him.=If it were not for his hard work, I...=Were it not for his hard work, I...=But for his hard work, I.. b. 与过去事实相反：若非当时...就... If had not been that从句(一般过去时)，主语+could(would/might/should/ought to)+have+过去分词=But for+名词， If it had not been that he lent me the money, I could not have bought that book.=If it had not been for the money (which) he lent me, I... =Had it not been for the money (which) he lent me, I... =But for the money (which) he lent me, I... But for his help, I couldn&apos;t have done it.=Without his help, I couldn&apos;t have done it.若非他及时警告，我可能已经没命了 But for his timely warning, I might have been killed.=Without his timely warning, I might have been killed. lest…(should)… 12345678910111213以免...=for fear that+主语+may+原形动词(表现在或者将来的状况) might+原形动词(表过去的状况) =for fear of+动名词你必须努力学习，以免考试不及格 You must study hard lest you (should) fail the exam.=You must study hard for fear that you may fail the exam.=You must study hard for fear of failing the exam. He did it carefully lest you make the same mistake again.=He did it carefully for fear that you might make the same mistake again.=He did it carefully for fear of making the same mistake again. as if 123456789101112=as though... 仿佛...(副词)a. 表极大的可能-&gt;动词使用一般时态看过来好像要下雨了It looks as if it is going to rain.b. 表与现在事实相反-&gt;动词使用一般过去时Mr. Wang loves me as if I were his own child.c. 表与过去事实相反-&gt;动词使用过去完成时It looks as if nothing had happended. What if…should…? 1234567891011121314151617要是...的话会怎样/要怎么办？ 表示对将来状况存疑的虚拟语气，if从句多置助动词should要是他来了，会怎样/怎么办？ What if he should come?=What might happen if he should come?=What could I do if he should come?要是下雨了，怎么办？What if should rain?What也可以用一般现在时： What if he comes?=What shall we do if he comes? What if rains?=What may happen if it rains? It’s time+that从句的一般过去时 123456789101112It&apos;s time(about time/hight time) + that从句的一般过去时 该时...的时候了该是他上床睡觉的时间了It&apos;s time he went to bed.该是我们行动的时候了It&apos;s time we did it.也可以改为不定式形式： It&apos;s time he went to bed.=It&apos;s time for him to go to bed.=It&apos;s time to go to bed. If only… 1234567891011要是...就好了只用于与现在事实或者过去事实相反的虚拟语气中，只能用一般过去时或者过去完成时：要是他在现在在这里就好了 If Only he were here.=I wish he were here.要是我早点知道这事就好了 If only I had known it earlier.=I wish I had known it earlier. wish的用法 123456789101112a. 接that从句，一定要用虚拟语气，若与现在事实相反，用一般过去时若与过去事实相反，用过去完成时：I wish (that) he is here(x)I wish he were here.=It&apos;s too bad that he is not here.b. I wish that... 真希望...就好了=I would rather(that)...真希望我当时更用功就好了I wish I had studied harder when young.=I would rather I had studied harder when young. hope的用法 123456789101112131415161718192021222324252627282930接that从句，使用一般时态，表示极大的可能性：a. 表示现在的状况I hope he is safe.b. 表示将来的状况I hope he will back.c. 表示进行的状况I hope they are having a good time.d. 表示完成的状况I hope they have finished the work.祈使句：祝你长命百岁 I hope he may live long.=May he live long! I hope you may pass the exam.=May you pass the exam. 上帝保佑你 May God bless you!=God bless you!国王万岁 May the king live long!=Long live the king! wish和hope的异同 123456789101112131415 I wish to travel abroad.=I hope to travel abroad.与for连用，表“期望获得”#我希望能有机会尝试一下I wish for a chance to try it.表祝福时，只能用wish，接两个名词或代词作宾语我祝你们玩的愉快I wish you a good time.I wish my parents longevity.I hope that you have a good time.I hope my partents can live long. 对过去事物的猜测的三种句型 12345678910111213141516a. must have+过去分词 一定曾经...他博学多才，一定受过良好的教育He is learned; he must have received a good education.b. may have+过去分词 可能曾经...他看起来很疲惫，可能昨晚熬夜很晚He looks tired; he may have stayed up late last night.c. cannot have+过去分词 不可能曾经...他是老实人，不可能偷了玛丽的钱He is honest; he cannot have stolen Mary&apos;s money.若表示猜测的结构为疑问句，要用“Can...have...?”他可能做了这样的事吗？May he have done it?(x)Can he have done it? 副词副词的功能 修饰动词： 123置于动词之后：He studies hard.I love you very much. 修饰形容词： 1234置于形容词之前：He is very kind.It is extremely dangerous to swim alone.This is probably true. 修饰副词： 123置于该副词之前：He studies very diligently.He does things too carefully. 修饰全句： 1234置于句首：Fortunately, he did not die.这答案显然是对的Evidently, this answer is right. 副词的位置一般规则 有be动词时，置于be动词之后: 12He is always kind.They are really able to do it. 有助动词时，置于助动词之后： 12He can hardly walk.He has never been to the US before. 有一般动词时，置于该动词之前，但修饰的动词的副词表示某种状态时，则置于动词之后： 12345678910111213The often go fishing.He really cares about you.修饰的动词的副词表示某种状态时，则置于动词之后：She dances beautifully.He droves his car fast.修饰动词的副词表示肯定、否定或频率的意味，则置于动词之前：肯定意味的动词：surely/certainly/absolutely/probably/reallyThey certainly hope to win the game.否定意味的动词：never/seldom/hardly ever(几乎不曾)/always/oftenHe often comes to school late. 状态副词在被动语态中的位置： 123置于过去分词前：He did the job well.The job was well done. 时间副词： 123句子短时，置于句尾，句子长时，置于句首Yesterday I saw him chatting very pleasantly with a blonde girl in the park.I saw him in the park yesterday. 地点副词： 123456789置于动词之后：here/there/home/downtown/upstairs/downstairs in/out/up/downHe come here.in/out/up/down等之后有宾语，则为介词He sat in the corner.He looked out of the windows.in/out/up/down等之后若无宾语，则为副词He come in. 副词短语 置于句尾，修饰句中的动词 He loves her with all his heart.he came by bus. 不定式短语当副词 除修饰动词外，还可置于形容词或副词后，修饰该形容词或副词: You will soon come to realize that you are mistaken. She went to Italy in order to sutdy music. I am sorry to have kept you waiting. 名词性从句修饰形容词 I am sorry that I cannot attend your party. I am not sure whether he’ll come. 重要的副词用法 very/much 很/非常 比较极的形容词或副词中，只能用much: 1234He walked much more slowly than I.He is much happier than Peter.修饰too时只能用muchHe is much too young. little/a little 123456little作副词，视为否定：一点儿都不，等于no:病人的情况并没比昨天好到哪儿去The patient&apos;s condition is little better than (it was) yesterday.a little作副词，译成：有点儿The patient&apos;s condition is getting a little better. no longer 123456789不再：=no more=not...any longer=not...anymore他不再在这儿了 He is no longer here.=No longer is he here.=He is not here any longer.=He is not here any more. sometimes/sometime/some time/some times 123456789101112131415161718192021222324252627a. sometime 有时候他有时候会来这儿走一下Sometimes he comes here for a visit.b. sometime 某时(不知何时)可用于一般过去时或一般将来时，使用时通常与另一明确的时间副词或副词短语连用：一般过来时：我昨天早上某个时间见过他I saw hime sometime yesterday morning.一般将来时：我明天某个时间会在这里I&apos;ll be here sometime tomororw.c. some time 一段时间我会在这儿待一阵子I&apos;ll stay here (for) some time.不久前他还在这里He was here some time ago.d. some times 有几次=a few times=several times.建议使用a few times或several times.我曾去过泰国几次I have been to Thailand a few times. ago/before/since/after 12345678910111213141516171819202122232425262728293031a. ago表“距现在若干时间以前”，时态用一般过去时他几一前来过这儿He came here a few days ago.How long ago did he come?b. before表“距过去某时若干时间以前”，时态用过去完成时:When I met Jim last week, He told me he had just gotten back from a trip to Lake Placid two days before.注意：ago只能用在一般过去时的结构。ago不能单独使用，但before可以作为单独副词单独使用：I met him ago(x)I me him two hours ago(ok)I met him before(ok)I have met him befoe(ok)I told him that I had met him before.(ok)c. since 自从...I have studied English Since I moved here in 2001.He told me that he had written two books since he moved here in 2001.since作副词，译成之后，置于句尾或have/has/had之后，修饰现在完成、进行时或者过去完成、进行时：我于2001年搬到此处，之后就一直在学习英文 I moved here in 2001 and have studied English since.=I moved here in 2001 and have since studied English.d. after 之后作副词译成“之后”，与since不同，since修饰完成时，而after则修饰过去时，此时等于later或afterwards.He fell ill on Monday and died three days after(later).The war ended in 1945, and they lived happily ever after. quite 十分地rather 相当…地 12345678910111213此二词可修饰原级之副词或形容词。他相当老了He is quite old.他蛮老的He is rather old.他相当用功He studies quite hard.修饰“形容词+单数名词”时，应在a或an之前：他是一个相当不错的学生He is quite a good student.They are quite good students. someday/one day/the other day/some other day 123456789101112131415161718192021a. someday 将来有一天(用于未来时)Keep on working hard, and someday you will be successful.b. one day 某日表前几天时，等于the other day，也可指几年前的某一天，两者均用过去时有一天我在城里看到他 I saw him downtown one day.=I saw him downtown the other day.表将来总有一天，等于someday，用于将来时总有一天你会后悔的 One day you&apos;ll be sorry.=Someday you&apos;ll be sorry.c. the other day 前些时间，用于一般过去时前几天我到公园玩了一趟I went to the park for a visit the other day.d. some other day 改天，用于一般将来时我现在很忙，改天再拜访你I&apos;m busy now, I&apos;ll visit you some ther day. somewhat/somehow/anyhow 12345678910111213141516171819202122232425262728a. somewhat 有一点儿，副词，置于形容词或副词前，以修饰形容词或副词： It&apos;s somewhat cold today.=It&apos;s a little cold today.=It&apos;s a bit cold today.他这件事做得有些粗心 He did the job somewhat carelessly.=He did the job a little(bit) carelessly. b. somehow 不知怎的，=for some unknown reason. 设法，=by some means.1) 表不知怎的，作独立副词，置于句首，修饰全句他人不错，但不知怎的，我就是不喜欢他He is nice; but somehow I don&apos;t like him.2) 表设法，作一般副词，修饰句中的动词，置于句尾我们必须设法找到这笔钱We must find the money somehow.c. anyhow 况且，而且，=besides/in addition 不管如何，=anyway/at any rate我没有时间去年电影，况且电影也太贵了I don&apos;t have time to go to the movie last year, they&apos;re too expensive anyhow.表“不管如何”时，anyhow为独立副词，通常置于句首，之后有逗号，修饰逗号之后整个主句：不管怎样，我们可以试试看Anyhow, we can try.anyway,at any rate, enough 12345678910a. 形容词，表足够/充分的，修饰名词他有足够的钱买一辆拉风的车He has enough money |to buy| a fance car. |for |b. 副词，表足以，修饰形容词或副词，置于形容词或副词之后他条件好，足以担任些职位He is good enough to fill the position.He is good enough for the position.他跑得够快而赶上了火车He ran fast enough to catch the train. likely/probably/possibly 可能地 1234567891011121314likely/probaly表示“很可能”，而possibly则表示“有可能但不确定”很有可能他会来: likely作副词时，之前可用very或most加以修饰He will(very/most) likely come.她很可能将要离开She will probaly leave.他们说不定会帮他They will possibly help him.likely也可以用形容词：probable不能修饰人 He is likely to come.=It&apos;s likely that he will come.=It&apos;s probable that he will come. Short Response 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859你是...， 我也是/你不是...，我也不是.a. 肯定句1). be动词，主语与be动词倒装他人很好，她也很好He is nice, and so is she. and she is, too.2). 助动词，主语与助动词倒装他要来，她也要来He will come, and so will she. and she will, too.3). 一般动词，主语与do/does/did倒装他来了，她也来了He came, and so did she. and she did, too.他有一本书，她也有He has a book, and so does she. and she does, too.同一个人时，不倒装He is stupid. So he is(=Indeed he is.)You can&apos;t do it. So I can&apos;t.b. 否定句用neither(或nor，使用neither或nor采用倒装句)或either. neither前要加and,当nor不用：1). be动词，主语与be动词倒装他不好，她也不好He is not nice, and neither is she. nor is she. and she isn&apos;t, either.他没有回家，她也没有回家He wasn&apos;t going home, and neither was she. nor was she. and she wasn&apos;t, either.2). 助动词，主语与助动词倒装他不来，她也不来He won&apos;t come, and neither will she. nor will she. and she won&apos;t, either.他没去，她也没去He hasn&apos;t gone, and neither has she. nor has she. and she hasn&apos;t, either.3). 一般动词，主语与do/does/did倒装他绝不说谎，她绝不说谎He never tells a lie, and neither does he. nor does he. and she doesn&apos;t, either.他没有书，她也没有He has no book, and neither does she. nor does she. and she doesn&apos;t, either.also只能又在肯定式中，置于be动词之后，或者一般动词之前：他很亲切，她也是He is kind, and she is also kind.He has a pen, and she also has one. 常见“名词+介词+名词”短语 123456789101112131415手拉手: hand in hand肩并肩: shoulder to shoulder/slide by slide臂挽着臂: arm in arm逐字地: word for word天天: day by day夜夜: night after night渐渐地: little by little一个一个地: one by one陆续地: one after another面对面地: face to face他们手牵手沿着那条路走下去They walked hand in hand down the road.他们陆续地走进去They came in one after another. not要置于不定式短语、分词、动名词之前 123456789101112由于有很多事要办，我决定不去旅行Since having a lot of work to do, I decided not to go on a trip.他假装不认识我He pretended not to know me.工作没完成，我决定留下来I don&apos;t have finished the work, I decided to stay.=Not having finished the work, I decided to stay.他很失望没把工作完成He was disappointed at not having finished the work. 助动词+副词十本动词 12我以前从没见过他I have never seen him before. 副词修饰被动语态时，通常置于过去分词之前 12345他演说得很好His speech was well presented.那件事终于解决了That case was finally settled. more than与倍数的关系 12345678910111213more than应置于倍数之前:a. 先造一个含有“倍数词+as ⋯ as ⋯ ”的句子他的钱是你的三倍:He has three times as much money as you do.b．再于“倍数词十as ⋯ as ⋯ ”的结构前置人more than，即告完成。He has more than three times as much money as you do．（他的钱超过你的三倍。）c. more than亦可置于动词double（增加一倍）之前。Our income has more than doubled in one year.（一年之内我们的收人增加了一倍多。） more than与over的关系 12345678over作介词表“超过”时，可用more than代替:他已待在此地一年多He has stayed here over a year.He has stayed here more than a year.这里的学生超过5人Over five students are here.More than five students are here. 避免双重否定 123456789101112not ever=never他从来没去过日本 He has not ever been to Japan before.=He has never been to Japan before.scarcely = hardly = almost not为否定副词:他几乎不会做这工作He can scarcely do this work.He can hardly do this work.他几乎没有钱He has hardly any money. 关系副词when/why/how/where 123456789101112131415这就是我们该出发的时候This is the time when we should set out.=This is when we should set out.这就是他为何迟到的理由This is the reason why he was late.=This is why he was late.这就是他成功的方法This is the way he succeeded.This is how he succeeded.那是我出生的地方That is the place where I was born.That is where I was born. I think so 12345678think 、 hope 、 believe 、 be afraid 、 imagine等动词之后加so，可代替肯定句，加not则代替否定句:A: 15 he nice?B: Yes, I think so. No, 1 think not.Will he come?B: I&apos;m afraid not. I&apos;m afraid so. all（三人以上一起）、both（两人一起） 1234567891011约翰、保罗和彼得明天都要来这里庆祝杰克的生日John、Paul and peter will all come here to celebrate Jack&apos;s birthday.这五位都是好学生These five students are all good.这两位人选都有资格做此工作The two candidates are both qualified for this work.他和我明天都要去美国He and I will both go to the American tomorrow. 容易混淆的副词 12345678910111213141516171819202122232425262728293031323334353637383940414243444546a . close（近）、 closely（仔细地，严密地）我住在他家附近I live close to his house.=I live near his house.好好盯着他Watch him closely.b . most（最）、 mostly（大部分，大都）在这些书中，这本最有趣Of all these books, This one is the most interesting.这些书大部分都过时了These books are mostly out of date.c. near（近）、 nearly(=almost，几乎）他住附近，不在远处He lives near, not far.他差点淹死了He was nearly drowned.near虽可作介词，但在下列短语中，near之后仍应置人to，形成固定短语come near to + V 一 ing差一点就⋯他差点淹死了 He came near to being drowned.=He came close to being drowned.=He was nearly drowned.d. high 表实际空间(可以测量)的 高 deep 深 wide 宽 highly 表程度而不指空间的 高 deeply 深 widely 宽飞机高高在天上飞着The airplane flew high in the sky.他大大地受到赞扬He was highly praised.我们赞扬他We spoke highly of him.我们很看重他We think highly of him.他潜人河中深处He dove deep into the river他伤得很重He was deeply hurt. greatly与highly之区别 123456789a . greatly表示&quot;大量地，大大地&quot;他大大地吃了一惊He was greatly surprised.车祸次数大幅增加The number of traffic accidents has increased greatly.b. highly表“高高地”，通常与“表扬” 、 “奖励” 、 “推荐”等动词（如praise 、recommend）连用:我们很看重他We think highly of him. nowhere（=in noplace）在任何地方都不 123此亦为否定副词，置于句首要采倒装句:这种人我在任何地方都找不到Nowhere could I find such a man. 倒装结构 否定倒装句： 123我没见过像她这么美的女孩I have never seen such a pretty girl as she (is).Never have I seen such a pertty girl as she. so/much倒装句： 123他人很好，值得我尊敬He is so kind that he deserves all my respect.So kind she that he deserves all my respect. 地方副词倒装句： 123那儿站着一个男孩A boy stood there.There stood a boy. 完全倒装句： 1234助人的人值得称赞It worth to praise for those who help others.Those who help others are praiseworthy.Praiseworthy are those who help others. as取代though的倒装句： 1234他虽然很好，我却不喜欢他Though he is nice, I don&apos;t like him.Nice as he is, I dislike him.As nice as he is, I dislike him. 否定倒装句123456789101112131415161718常用的否定副词：never: 从不hardly: 几乎不 = scarelyrarely: 很少seldom: 很少little: 一点儿也不常用的否定副词短语：by no means: 绝对不in no way: 绝对不on no accout: 绝对不under no circumstances: 绝对不in no situation: 绝对不常用的否定状语从句：not until+S+V: 直到...only when+S+V: 只有到...的时候only after+S+V: 只有到...之后 如何倒装： 123456789101112131415161718192021222324252627282930313233343536373839404142434445先将否定副词、否定副词短语或否定状语从句移到句首,再检查原结构主语之后的动词，采问句倒装。a. be动词be动词与主语倒装他从不快乐He is never happy.Never is he happy.他几乎从未安静过He is hardly ever quiet.Hardly is he ever quiet.他绝非我们需要的人You are by no means the person we need.By no means are you the person we need.直到他来，我才发现这件事I was not aware of it until he came.No until he came was I awae of it.只有玛丽跟他在一起的时候，他才快乐He is happy only when Mary is with him.b. 助动词助动词与主语要倒装他几乎不能动He can hardly move.Hardly can he move.他从来没做过这件事He has never done it before.Never has he done it before.c. 一般动词与疑问句一样，变成do/does/did+原形动词他很少唱歌He seldom sings.Seldom does he sing.他一点儿也没有想到玛丽的英语这么好He little knew that Mary&apos;s English could be so good.(x, 没有He little knew的说法)Little did he konw that Mary&apos;s English could be so good. only+介词短语及only+then 12345678910only then: 唯有在那时均视为否定副词短语，置于句首时，采倒装：只有这么做你才会成功Only by doing so can you succeed.有了他的帮忙，我才能克服困难Only with his help will be able to weather the fifficulties.到那时我才离开|Only then |did I leave.|Only at that time| not+a+单数可数名词 连一个…都没有 我在那儿一个人也没有看到I didn’t see a soul(=a person) over there.Not a soul did I see over there. I was not able to discover a clue to the murder.No a clue to the murder was I able to discover.这件谋杀案我连一点蛛丝马迹也没法找到 nowhere=in no place 在任何地方都不 这种人我在任何地方都找不到Nowhere could I find such a man. 一…就…的句型 12345678910111213141516171819a. 同时态的状语从句 As soon as she saw me, she passed out.=Once she saw me, she passed out.=The instant she...=The moment she ...b. On/Upon+N/V一ing, S+Vupon或on之后加名词或动名词，也表示&quot;一...就...&quot;的意思主语相同：我一做完工作就回家 As soon as I finished the job, I went home.=Upon/On finishing the job, I went home.不同主语：Upon/On用所有格他一到我就走了 As soon as he arrived, I left.=Upon/On his arriving, I left. so/such倒装句so…that或such…that倒装结构时，so或much及所引导的词类置于句首，与否定结构句完全相同 有be动词时： 123他这么乐于助人，因此我们都喜欢他 He is so helpful that we all like him.=So helpful is he that we all like him. 有助动词时： 123他做得这么好，所以值得我们尊敬 He has done so well that deserves our respect.=So well has he done that deserves our respect. 有一般动词时： 123他太晚到，而没有赶上火车 He came so late that missed the train.=So late did he come that missed the train. 地方副词倒装句三种倒装句型地方副词(there/here)或地方副词短语(in the room/at the station/by the window)置于句首时，亦可形成倒装： 第一型 123456789101112131415161718主语+不及物动词+地方副词 地方副词短语-&gt;地方副词 +不及物动词+主语地方副词短语 A girl sat there.=There sat a girl. A girl sat by the door.=By the door sat a girl. A book is on the desk.=On the desk is a book.玛丽站在门口 Mary stood in the doorway.=In the doorway stood Mary. 第二型 1234567891011主语+be动词+过去分词+地方副词或地方副词短语-&gt;地方副词或地方副词短语+be动词+过去分词+主语门后锁着一个人 A man was locked behind the door.=Behind the door was locked a man.那里坐落着一所学校 A school is located there.=There is located a school. 第三型 123456789101112主语+be动词+现在分词+地方副词或地方副词短语-&gt;现在分词+地方副词或地方副词短语+be动词+主语那儿有一个女孩在唱歌，她是我女友#A girl is singing over there, she is my girlfriend.Singing over there is a girl, who is my girlfriend.玛丽与大卫坐在大门前 Mary and David were sitting in front of the gate.=Sitting in front of the gate were Mary and David.=In front of the gate were sitting Mary and David. 主语必为普通名词或专有名词12345678910111213若主语为代词时，地方副词可放在句首，但句子不倒装：他站在那儿 He stood there.=There he stood.他站在房子前面 He stood in front of the house.=In front of the house he stood.他被锁在那里 He was locked in there.=There he was locked in. here与there的习惯用语置于句首为代词，不倒装： 1234567Here we are: 到了Here you are/Here you go: 拿去吧There you go again: 你又来这一套了There you are/There it is: 在那里 比较结构 原级比较：as 123456他和彼得一样用功He work as hard as Peter.He is as diligent as Peter.他没有Peter那么用功He doesn&apos;t study so hard as Peter. 一般比较：than 12345他比彼得用功He is more diligent than Peter.他没有彼特更用功He doesn&apos;t study harder than Peter. 一般比较结构 造句方式： 12345678910111213141516a. 先造一个含有形容词或副词的句子：他很小心He is prudent.他工作很热心He works enthusiastically.b. 再将句中形容词或副词变成比较级：他比较小心He is more prudent.他工作比较不热心He works less enthusiastically.c. 再设定比较对象，并在该比较对象之前置副词连词than：He is more prudent than Peter.He works less enthusiastically than Peter. than引导的状语从句结构变化： 12345678910111213若有be动词、助动词可保留，若有一般动词以do/does/did代替，当然这些也可以省略。a. be动词他没有你负责任He is less responsible than you (are).b. 助动词他唱歌比你悦耳He can sing more beautifully than you (can).c. 一般动词他工作比你仔细He worked more carefully than you (did). less之后的副词或形容词均使用原级： 12345678他没有John勤奋He is less hardworking than John.他没有John那么常来看我们He visits us less often than John.他做事没有John那么小心He does things less carefully than John. 与本身作比较： 1234567891011121314151617181920212223242526272829303132他比以前用功He studies hander than he did before.她看来比较实际年轻She looks younger than she is.a. 自行比较时，句中的时态若不同：1). be动词他比以前健康(不同时态)He is healthier than he was.2). 助动词他比以前唱歌好(不同时态)He can sing better than he could.3). 一般动词他比以前努力He work hander than he did. he used to.b. 此类中若有对等的介词短语,than之后的主语及动词或助动词都可以省略：1) 介词短语他和玛丽在一起比和简在一起快乐 He is happier with Mary than He is with Jane.=He is happier with Mary than with Jane.他对英文比对日文有兴趣He is more interesting in English than (he is) in Janpanese. 2) when从句他比我十年前看见他时瘦多了He is thinner than (he was) when I saw him ten years ago. 比较结构中相同动词的化简 1234我比他爱你 I love you more than he loves you.=I love you more than he does you.=I love you more than he does. 数量形容词的比较级变化：数量形容词：much/little/many/few 12345678910111213141516171819202122232425262728293031323334353637383940414243444546a. much(很多的)及little(很少的)修饰不可数名词：原级 比较级 最高级much more the mostlittle less the leastmany more the mostfew fewer the fewest他有很多钱 He has very much money.=He has a lot of money.(佳)肯定式中用very much或a lot of，否定式中则使用much他比John有钱He has more money than John (does).他是所有人中最有钱的He has the most money of all.他的时间很少He has little time.他做这工作的时间比我的少He has less time than I (do) for the job.他的时间最少He has the least time of all.b. many(很多的)及few(很少的)修饰可数的复数名词：他有很多朋友He has many friends.他的朋友比John的多He has more friends than John.他的朋友最多He has the most friends of all.他的朋友很少He has few friends.他的朋友比John的少He has fewer friends than John.他的朋友最少He has the fewest friends of all. 避免错误比较 12345这个篮子里的苹果比桌子上的那些要甜(与介词短语those比较)The apples in this basket are sweeter than those on the table.他的车子比他的朋友的好His car better than his friend&apos;s. 本身最高级意味的形容词： 1234567已经是最高级的意味，不能使用比较级：perfect: 完美的sublime: 卓越的noble: 高贵的extreme: 极度的superb: 极好的excellent: 特优的 变得愈来愈… 1234567891011 be getting more and more + adj.=be getting 比较级adj+and+比较级adj.=be getting increasingly+adj.那儿的生活越来越不容易Life over there are getting more and more difficult. increasingly difficult.那个男孩越来越胖了The boy is getting fatter and fatter. increasingly fat. 修饰比较级的副词 1234567891011置于被修饰的副词或形容词前：far/much/a lot/a great deal/still/even，其中far/much/a lot最常用这问题比那个要难多了/甚至更难This question is (far/much/a lot) more fifficult than than one.他比他们之中任何一个都用功得多He studies much harder than anyone of them.far/much/a lot可修饰副词too：他太老了，无法做这差事He is far/much/a lot too old to do the work. 最高级的副词 12345678910111213141516171819有两个：most及leasta. 形容词她是我见过最美的女孩She is the most beautiful girl that I&apos;ve never seen.三个女孩中，她最不好看Of the three girls, she is the least beautiful.b. 副词他待人最有礼貌He treats people most politely.c. 动词我最需要你I need you most. 最高级一定要是三者以上： 123456789101112131415161718192021a. Of/among the three/four... 在这三个/四个...之中这五本书中，我最喜欢那本Of five books, I like that one best.他是所有学生中最好的一个He is the best student of all.of the two则使用比较级这两个学生中，John比较好Of the two students, John is the better.这两本书中，我比较喜欢那本Of the two books, I like that one better.b. 最高级形容词+n+关系代词that(不可使用who/whom/which)+定语从句完成时：在我所认识的人中，John是最有责任感的一位John is the most responsible man (that) I&apos;ve ever known.在我教过的学生中，他是最好的一位He is the best students that I&apos;ve ever taught. the与最高级的关系： 1234567891011121314最高级副词修饰动词或副词时，不一定要加the，但最高级形容词修饰名词时，则须加the:a. 副词这四个学生中，他待人最有礼Of the four students, he treats people (the) most politely.b. 动词我所认识的人中，我最尊敬他Of all the people I know, I respect him (the) most.I respect him most that I&apos;ve ever known.c. 名词他是所有学生当中最好的一个He is the best student of all. most可作very解： 123加形容词修饰名词时，不必于most之前加the:她真是个很美的女孩子She is a most/very beautiful girl. by far可修饰最高级： 123by far及much修饰the+最高级副词或形容词：她是镇上最美的女孩子She is by far/much the most beautiful girl in town. 原级比较结构 造句方式： 1234567891011121314151617as...as...: 和...一样地...a. 先造一个含有形容词或副词的句子：He is prudent.He runs fast.b. 再于句中的形容词或副词之前置as一词as为副词，译成“一样地” ：他一样地小心He is as prudent.He runs as fast.c. 再设定比较对象，并在该比较对象之前置副词连接词as即告完成；此as译成“和” ：他和约翰一样小心He is as prudent as John.他跑得和约翰一样快He runs as fast as John. as从句的变化 1234567891011a. be动词他和约翰一样小心He is as prudent as John (is)b. 助动词他唱得和约翰一样好He can sing as well as John (can)c. 一般动词他跑得和约翰一样快He runs as fast as John. as从句或than从句也可采倒装结构： 12345678他比我的好友约翰谨慎He is as prudent is John, who is a good friend of mine.他和我妹妹玛丽一样用功He studies as hand as does Mary, who is my sister. 他比我妹妹玛丽用功多了He studies much hander than does Mary, who is my sister. “as…as…”用于肯定或否定句，而”so…as…”则只能用在否定句中 12345678肯定句：He studies as hard as Mary.他和栏杆一样瘦He is as thin as a rail.否定句：He doesn&apos;t study as(so) hard as Mary.He isn&apos;t as(so) polite as John. 其他有关as/than的重要用法 “as…as…”可与单数可数名词连用： 1234567891011121314151617181920212223242526272829303132333435363738#将第一个as置于动词之后：He is as handsome a boy as John.I have as fancy a car as that one.1). 类似此种只能与单数名词连用的结构还有：so...that...: 如此...以致/所以...too...to... 太...而不...how... 多么地...a. so...that...他是个这么好的男孩子，所以我非常喜欢他 He is so a good boy that I like him very much.=He is such a good boy that I like him very much.b. too...to...他年纪太大，没法做此事He is too old to do it.c. how...他是个伟大的人物 What a great man he is!=How great man he is!2) what与such之后可修饰单复数或不可数名词：他真是个伟大的人物What a greate man he is!这真是好听的音乐(不可数)What beautiful music it is!他们真是好孩子(复数)What good boys they are!He is such a good boy that I like him.They are such good boys that I like them.这类只能与单数可数名词连用的结构记忆法：从前有个书呆子(so...that...)，他有两个朋友，一个叫阿西(as...as...),一个叫土土(too...to...)，三人相聚时，最爱念一个字：好(how...) as…as one can 尽可能地…=as…as possible 1234567891011121314151617a. 造句方法：1) 先造一个含有形容词或副词的句子：他一定要用功You must study hard.他保持沉默He remained quiet.2). 在hard或quiet之前置入第一个as(用副词，中文译为&quot;一样地&quot;)：You must study as hard.He remained as quite.3). 句尾添加as one(you/he...) can或possible:你要尽可能地用功You must study as hard as you can. as possible.He remained as quiet as he could. as possible. as…as any 与任何人一样… 12他和任何其他人一样快乐He is as happy a man as any. as…as ever 与往常一样… 12虽然我和他分开已久，但他还是和以前一样年轻Though it has been a long time since I parted for him, he is as young as ever. as…as ever lived 古今最…之一=one of the+最高级…that ever lived 123他是古今最伟大的英雄之一 He is as great a heo as ever lived.=He is one of the greatest heroes that ever lived. 两个形容词的相互比较： 1234more...that结构中两个形容词比较，一律用原级： He is more good than bad.=He is not so bad as good.=He is good rather than bad. 倍数词造句法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596a. 本结构一共有四种：倍数词+as⋯as⋯ 是⋯的几倍morethan+倍数词+as⋯as⋯ 是⋯的几倍还不止倍数词+the（或所有格）+名词是⋯ ⋯的几倍more than+倍数词+the（或所有格）+名词 是...的几倍还不止他的体重是我的三倍 He is three times as heavy as I.=He is three times my weight.他的体重是我的三倍还不止 He is more than three times as heavy as I.=He is more than three times my weight.b. 倍数词+as...as... 是...的几倍含有as...as...的短语，句中必须要用形容词或副词：他用功的程度是的3倍He studies hard.He studies as hard as I.将倍数词置于as...as...之前He studies three times as hard as I.I am three times as old as he.c. more than+倍数词+as...as... 是...的几倍还不止 =倍数词+more...than...他的用功是我的三倍还不止He studies hard.He studies as hard as I.He studies three times as hard as I.He studies more than three times as hard as I.He studies three times harer than I. He is more than three times as diligent as I.=he is three times more diligent than I. I am more than three times as old as he.=I am three times more older than he. d. 倍数词+the(that/his/those/these/所有格)+名词 是...的几倍 本结构必须要与名词连用他是钱是我的两倍1. 先译&quot;他有我的钱&quot;He has my money.2. 再将倍数置于所有格之前He has twice my money.3. more than+倍数词+the(或所有格)+名词 是...的几倍还不止他的钱是我的两位还不止1. He has my money.2. He has twice my money.3. He has more than twice my money.f. 下列词类均视为倍数词：one-third 三分之一two-thirds 三分之二three-fourths 四分之三我的年龄只有你的一半I am half as old as you.我的用功程度仅及他的三分之一I study only one-third as hard as he.她的体重有我的两倍She is twice my weight.g. 特殊结构：1. twice as⋯as⋯（常用）是⋯ ⋯的两倍他的年纪是我的两倍He is twice as old as I.2. one and a half times as⋯as... 是⋯的一倍半He 15 eight years old, and I am one and a half times as old as he·3. as many as+数词+复数名词 多达⋯这次空难中多达100人丧生As many as one hundred people were killed in the air crash.桌上的苹果有5个之多There are as many as five apples on the table.as much as+金钱名词: 他身上有50块钱之多He has as much as fifty dollars with him.as long as+数词十名词 长达...as wide as+数词十名词 宽达...as high as+数词十名词 高达...as early as+数词+名词 早在...as late as+数词+名词 迟至...我学英文已有25年之久I&apos;ve been learning English as long as 25 years.这条河宽达100米This river is as wide as 100 meters.我早在星期五就把信寄了I mailed the letter as early as Friday.他迟至清晨两点才回家he came home as late as two in the morning. the+比较级…, the+比较级… 愈…就愈… 1234567891011121314151617181920212223242526272829使用本句型的先决条件为：句中要有副词或形容词：a. 造句法：你愈用功，你就愈能成为一个好学生1. 加入The..., the...: The____..., the____...2. 将中文句子中的“愈”去掉，变成： 你用功，你就能成为一个好学生： You study hard, you&apos;ll become a good student.(x)3. 将第二步中的英文句子置于第一步中的空格后： The____you study hard, the____you&apos;ll become a good student.4. 将第三步中的形容词或副词移位到空格中，并变成比较级。若形容词之后有名词，该名词也要移位， 而原有的a或an则删除： The harder you study, the better student you&apos;ll become.你愈谨慎，你犯的错就愈少1. The___..., the___...2. You are careful, you&apos;ll make few mistakes.(x)3. The___you are careful, the___you&apos;ll make few mistakes.4. The more careful you are, the fewer mistakes you&apos;ll make.b. 使用“愈...就愈...”的结构时，要注意两点：1. 若句中没有副词或形容词时，则在The或the之后置副词more或less:你愈爱我，我就对你愈好The___you love me, the___I&apos;ll be nice to you.The more you love me, the nicer I&apos;ll be to you.d. 比较级+and+比较级 愈来愈...It is getting warmer and warmer every year.The girl became more and more beautiful. 代词it作形式主语 代替不定式 12 It is good to study.=To study is good. 代替that名词性从句 123他想读书，这是件好事It&apos;s good that he wants to study.That he wants to study is good. 代替动名词 1234整天看小说是没用的 It&apos;s no use/good reading novels all day long.=It&apos;s of no use to read novels all day long.=There is no use (in) reading novels all day long. it作形式宾语 但动词虽加宾语，意思却不完整，而需要补充说明，谓不完全及物动词，其补充语一定是名词或形容词（包括分词变成的形容词）： 1234我认为他人很好I consider him good.I consider him a good man.I consider her beautiful. 不定式或that从句有名词的功能，可作主语，亦可作宾语: 123456789101112131415161718192021I think that he is good.To sutdy abroad has always been my dream.I want to go.That he doesn&apos;t study is true.但两者却不能直接作不完全及物动词之宾语，必须用it代替:I think that he is good true.(x)I think it true that he is good.I think to get up early good.(x)I think it good to get up early.以下是常用的不完全及物动词：think 认为⋯ ⋯是⋯ ⋯consider 认为⋯ ⋯是⋯ ⋯deem 认为⋯ ⋯是⋯ ⋯believe 相信⋯ ⋯是⋯ ⋯find 发现⋯ ⋯是⋯ ⋯I consider it necessary to excercise on a daily basis. it亦可用以强调介词短语或状语从句： 12345我是在1974年开始学英文的It was in 1974 that I began to study English.他是因为懒惰的关系而失败的It was because he was lazy that he failed. 数量代词的用法： 123456789101112131415161718192021222324most of the(或my/your...)+不可数名词+单数动词all 复数名词+复数动词somehalfpartthe restone-thirdtwo-thirdsnone大部分的钱都被窃了(money前不得加the)Most of money was stolen.部分学生在这儿Some of the students are here.所有学生都在这儿All of the students are here.他所有的演播都在这儿All of his students are here.有三分之一的学生通过了测验One third of the students have passed the test.70％的水被污染了Seventy percent of the water is contaminated.70％的学生没考及格Seventy percent of the students have failed the test. almost/most/all的关系： 1234567891011121314almost 为副词表“几乎”, 不能作代词。most 为形容词（大多数的，最多的）、副词（最），亦可作代词，表“大多数”all 为形容词（所有的），亦可作代词（全部），可用almost修饰。大多数学生在这儿(代词)Most of the students are here.所有的学生都在这儿(代词)All of the students are here.几乎所有的学生都在这儿(adv)Almost all of the students are here.大多数学生都喜欢音乐(adj)Most students love music.所有学生都喜欢音乐(adj)All students love music. 人称代词 1234她自杀了She killed herself.她的书和我的一样有趣her books are as interesting as mine. 反身代词的强势用法 123他亲自做这件事He himself did it.He did it himself. a friend of mine/this book of hers的用法 1234567891011他是我的一位朋友He is a friend of mine.你的这位朋友是个好人This friend of yours is good.上述结构使用到“ of十所有格代词”时，均可与不定冠词（a 、 an）、指示代词( this 、 that 、 these 、 those 、which）或数词（some 、 a few 、 ten 、 many 、most ⋯）连用，但不可与定冠词the连用，换言之，无下列用法：他是我最好的朋友He is the best of mine.(x)He is my best friend.他是我最好的朋友其中一个He is one of my best friends. those who/those whom 1234叫那些迟到的人来见我Tell those who are late to come and see me.虚无主义者对伊凡的画感兴趣Evan&apos;s paintings are interesting to those who believe in nihilism. each other/one another 123456789101112131415a. each other 两者彼此 one another 三者或三者以上彼此这两位同学彼此都很喜欢对方The two students like each other.这五位同学彼此互相讨厌The five student hate on another.b. one after the other 两者相继地 one after another 三者或三者以上相继地他相继地举起左右手He raised his hands one after the other.所有学生一个接一个地进来All the students came in one after another. either/neither/both/all/any/none作代词的用法 12345678910111213141516either（两者中任一） any（三者或以上任一）neither（两者皆不） none（三者或以上皆不）both（两者皆） all（三者或以上皆）你上咖啡也好，茶也好，我都不在乎随便哪样都行I don&apos;t care whether you serve coffee or tea; either will do.这四把椅子没有一把是我喜欢的I don&apos;t like any of the four chairs.这两本书都不好Neither of the two books is good.三个学生中没有一个是用功的None of the three students is(或are) diligent.那两个学生都懒惰Both of the two students are lazy.这五个苹果都烂掉了All of the five apples are rotten. one…the other… 一个…另一个…(限定的两者) 12他有两个儿子。一个是老师, 另一个则是军人。He have two sons. One is a teacher, and the other is a soldier. one…another…the other… 一个…一个…另一个…(限定的三者) 12他有三个儿子。一个是老师，一个是军人，另一个则是律师He have three sons. One is a teacher, another is a soldier, and the other is a lawyer. one…another… 一个…另一个…(非限定的两者) 12嗜好因人而异。某甲可能喜欢游泳，某乙则可能喜欢远足Hobbies vary with people. One may enjoy swimming, while another may love hiking. some…others… 一些…另一些…(非限定的两群) 12嗜好因人而异。有些人可能喜欢游泳，有些人则可能喜欢远足Hobbies vary with people. Some may enjoy swimming, while others may love hiking. some…others…still others… 一些…一些…另一些… 12嗜好因人而异。有些人可能喜欢游泳，有些人可能喜欢蹦极，也有些人则喜欢远足Hobbies vary with people. Some may enjoy swimming, others may be fond of bungee jumping, and still others may love hiking. 明确数词…the others/the rest… 若干…其他/其余 12我班上只有两个学生通过考试，其余全不及格Only two students in my class passed the test. The others failed. 复合形容词 数词+名词 1234这个计划会持续一年，但那个计划则会持续五年This program will last one year, but that program will last five years.这项五年计划执行起来得超过五年(five一year中的名词为单数)This five一year plan may take more than five years to carry out. 数词+名词+形容词 1234约翰五岁了John is five years old.约翰是五岁的男孩John is a five一year一old boy. 名词+现在分词 1234玛丽是个喜欢伤男孩子心的女孩Mary is a girl who likes to break boys&apos; hearts.玛丽是个会伤人心的女孩Mary is a heart一breaking girl. 副词+现在分词 1234这块低洼地区淹水了The area which lay low was flooded.这块低洼地区淹水了The low一lying area was flooded. 名词+过去分词 1234我为那个心碎的男孩子感到难过I feel sorry for the boy whose heart is broken.我为那个心碎的男孩感到难过I feel sorry for the heart一broken boy. 形容词+名词变成的过去分词 12那个大眼睛的女孩子是谁Who is that big一eyed girl? well/ill+过去分词 1234567891011121314151617181920212223well一behaved 很守规矩的well一bred 很有教养的well一built 体格很棒的well一chosen 慎选的well一done 表现很棒的；（牛排）全熟的well一dressed 穿着体面的well一known 很出名的well一mannered 彬彬有礼的well一read 书读得很多的，饱读诗书的well一traveled 去过很多地方的，老马识途的ill一bred 没有教养的111一considered 考虑不周的111一fated 命运不好的，不幸的ill一gotten 用不正当手段获得的illjudged 判断不当的111一mannered 态度不好的111一natured 心地不好的，本性坏的good一natured 本性善良的（非well一attired )我喜欢这孩子，因为他很乖I like the child because he is well一behaved.这位命苦的年轻人最后自杀了The ill一fated young man committed suicide in the long run. 介词用法at at+建筑物 12345我们将在车站碰面We will meet at the station.a．城市、国家等则与介词in连用b．若强调“在某建筑物内”则介词仍须使用in at+人对某人（尤指近距离常与yell/point/shout/laugh等动词连用。此处at表“朝向” ) 1234567891011121314yell/shout at 人 对某人吼叫／咆哮laugh at 人 嘲笑某人point at 人 指着某人那个男人生气时总是对他的太太咆哮The man always shouts at his wife when he is angry.强尼，不要嘲笑那个可怜的乞丐Don&apos;t laugh at the poor beggar, Johnny.老师指着他说；“我看到你考试时作弊。 ”The teacher pointed at him and said, &quot;I saw you cheat on the test.&quot;point to... 指着远处的...他指着山顶上的房子说：“那是我的家。 ”He pointed to the house on the hilltop and said, &quot;It&apos;s my house.&quot; at与下列名词连用表“从事某活动” 123456789be atwork 上班，做事be at church 做礼拜be at school 上学be at rest 休息莎琳娜正在工作，不要打扰她Selena is at work now; don&apos;t bother her.你现在可以去问经理；他正在大厅里休息You can go ask the manager now; he is at rest in the lobby. at亦与度数／程度／价格／年龄等与数词有关的名词连用 12345678910at the price/cost/expense of+数词 以⋯ ⋯的价格at the age of+年龄 在⋯ ⋯的岁数时at the speed of+速度 以⋯ ⋯的速度我以450美元的价格买了这台CD随身听I bought the portable CD player at the price of $450珊蒂在32岁时生了第一个小孩Sandy had her first child at the age of 32.她当时以时速28英里的速度开车She drove at the speed of 28 miles an hour. by1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071721. 表“在⋯ ⋯之旁＂相当于beslde:by the window 在窗边by the door 在门边by my side 在我旁边他走过来站在我旁边He came over and stood by my side.2. 表“凭借”: judge A by B 借由B来评鉴、判断A tell A by B 借由B知道是A不要以貌取人Don&apos;t judge a person by his or her looks.3. by与下列表“抓”有关的动词连用，仍表’ ‘借由”:catch/grab/grasp/hold/seize/take+人+by the+身体部位或衣物的部分 抓住某人的⋯那个男人抓住他的衣领叫他闭嘴The man seized him by the collar and told him to shut up.注意：by后面不能接所有格，只能接the.我抓住了他的手I caught him by the hand.4. by亦表“经过”之意:pass by my house 经过我家drive by my house 开车经过我家walk by my house 走路经过我家我经过家门而没有走进去I passed by my house and didn&apos;t walk in.5. by亦表“差距”之意尤用于下列用法：be older than+人+by two years 比某人大两岁be younger than+人+ by two years 比某人小两岁她比我大五岁She is older than I by five years.我们以两分之差赢了另一队We beat the other team by two points.6. by亦表“按照”我的表现在是5点20It is five twenty by my watch.7. by亦表”取道”come back by way of Hong Kong 取道香港回来=come back via Hong Kong我们取道香港前往北京We went to Beijing by way of Hong Kong.8. by亦表&quot;乘坐（交通工具）&quot;:come back by bus/train/ship/air/bicycle/motorcycle乘公车／火车／船／飞机／（骑）自行车／（骑）摩托车回来他每天乘出租车上班He goes to work by taxi every day.9. by用于被动语态表“被...”他被车撞死了he was killed by a car.10. by亦可用于主动语态by+动名词 用/借由...他借由努力而成功He succeeded by working hard.那个老太太以卖水果为生The old woman makes a living by selling fruit.11. by+时间最迟不超过某时间Come back by ten=Come back no later than ten.十点以前要回来 for 与表“动身”之意的动词连用，表“前往（某地）” 12345678910set out/set off/depart for+地点 动身前往某地head for/to+地点 朝某地前进leave A地 for B地 离开A地前往B地她昨晚动身前往美国She set out for American last night.看完电影后我们就前往酒吧We headed to the pub after the movie.他昨天离开澳大利亚前往新西兰He leave Australia for New Zealand yesterday. 表“为⋯的目的”: 1234567do it for you 为了你做这件事for the sake of..． 为了⋯的缘故不要责怪他。毕竟，他这件事是为你做的Don&apos;t blame on him. After all, he did it for you.他做每件事都是为了钱He doe everything for the sake of money. 表”赞成” 12345 be for+事 赞成某事=be in favor of+事你赞成他的想法吗？ Are you for his idea?=Are you in favor of his idea? 表“为了⋯的原因”： 123reward+人+for+事 因为某事而奖励某人村民报答他救了那位小女孩The villagers rewarded him for saving the girl. 表”当作”： 123456use A for B 把A当作B使用take A for B （误）把A当作B那些原住民以一些昆虫为食The natives use insects for food.我把约翰误当作女孩子，因为他留长发I took John for a girl because he wears long hair. 表”找寻”： 12345be hard pressed for time 时间不够用（而须寻找时间）look for... 寻找...那位女士扮演多重角色。她的时间总是不够用The lady wears many hats. She is always hard pressed for time. 表”持续（一段时间）”: 12345我已经学了三年的日文了I have been learning Japanese for three years.我好长一段时间没见到约翰了I haven&apos;t seen John for a long time. for ages. for亦用于下列句型中 1234567It is+非表人之本性或气质的形容词+for sb to+原形动词⋯对某人来说（做）⋯ ⋯是⋯ ⋯的。学英文对你来说是有必要的It is necessary for you to study English.你能帮我真是太好了It is very kind for you to help me. 表“开往（某地点）帕勺交通工具）”： 12往芝加哥的班机将在2点30分起飞The flight for Chicago will depart at 2:30. 表“以…价格” : 12这本书我10元就买到了I bought this book for only $10. from 表”从…”: 1234567891011121314151617from A to B 从A到Bfrom what he said 从他所说的话go from bad to worse 每况愈下从上海飞往香港要花一个多小时It takes more than an hour to fly from Shanghai to Hong Kong.从他的话中我无法了解他的意思I couldn&apos;t figure out what he meant from his words. 从那次事件后，他们的生意就每况愈下Their business has gone from bad to worse since that incident.be made from... 由...做的（成品不能还原成原料）这酒是葡萄酿造的This wine is made from grapes.be made of... 由...做的（制成的成品其原料本质不变）这张桌子是木头打造的This table is made of wood. from亦可与表“禁止、阻止、保护”有关的动词连用此处from表“免于” 123456789101112131415ban/prohibit/restrict/restrain sb from+动名词 禁止某人（做）prevent/stop/keep sb from+动名词 阻止某人（做）⋯protect sb from+动名词 保护某人免于⋯他被禁止在大厅里抽烟He was prohibited from smoking in the lobby.那位女士及时赶来阻止了她的儿子自杀The lady came in time to prevent her son from killing himself.王子保护公主免受盗匪攻击The prince protected the princess from being attacked by the bandit.我们老师禁止我们带漫画书到学校Our teacher forbid us to bring comic books to school. in 表“穿/戴着…” 123456789be dressed in⋯ 身穿/戴着⋯wear...那个穿着红衣服的女孩是谁?Who is the girl that is dressed in red?Who is the girl that is wearing a red dress?她出门前穿上她的外套She put on her coat before going out. “in＋一段时间”表“在一段时间之后”，相当于“一段时间+later”，亦可表“在一段时间之内”此时in相当于within: 12345我两分钟后就会回来I&apos;ll be back in two minutes.I&apos;ll be back two minutes later.我想我在两分钟内就可以完成这份工作I think I can finish the work within 2 minutes. 表“以⋯ ⋯（颜料、工具等）” 123456write the letter in ink/pencil 用墨水/铅笔写信write the word in chalk 用粉笔写字钢笔写字write the words in ink=Write the words with a pen 表“按…次序”: 1234567in alphabetical order 按字母顺序keep...in order 将⋯整理好/整齐他把文件按照字母顺序放人档案夹中He put ine papers in the folders in alphabetical order.客人来之前把你的房间整理好Keep your room in order before the guests come. 表“以⋯语言/声音”: 123456789101112write in Chinese 用中文写speak in a low voice 低声说话talk in a bitter tone 以尖刻的语调说话这老外会写中文，真令人惊讶It&apos;s surprising that the foreigner can write in Chinese.凯特正低声在和她的朋友说话Ketty is speaking to her friend in a low vice.我母亲凝视着提姆，并以尖刻的语调说话My monther stared at Tom and talked in a bitter tone.那个男子说话带有浓重的日本口音The man spoke with a heavy Japanese accent. on 与表“距离”的名词连用： 123456go on a trip/journey 去旅行go on an expedition 去探险／远征go on an excursion （尤指团体）去远足我先生喜欢在每年的这个时候去日本旅行My husband likes to go on a trip to Japan at this time of the year. 与表“差遣”的名词连用: 1234567be on an errand 跑腿，办差事be on a mission 身负使命他出去办点差事He is on an errand.那些军人被派往前线作战The soldiers were sent to the front on a mission. 与表“约会”的名词连用 1234be on a date with sb 跟某人约会你昨晚和那边那位男士约会吗?Were you on a date with the man over there last night? 与表“特别饮食”的名词连用： 1234be on a diet 节食辛蒂不会吃这些蛋糕的。她正在节食Cindy won&apos;t eat these cakes. She is on a diet. 与表“平地”或“大陆”的名词连用： 12345678910on campus 在校园内on the mainland 在大陆on the farm/ranch 在农场/牧场上他们在牧场上饲养牛羊They rear cattle and sheep on the ranch.off campus 在校外学生即使在校外也应该守规矩The students should behave themselves even off campus. 与表“线状／线条”意味的名词连用： 12345678live on that street 住在那条街上walk on the beach 在海滩散步（海滩为线状）on the coast 在海岸上on the brim/edge/verge of... 在⋯的边缘on the frontier 在边境我喜欢和男友在海滩散步I like to walk on the beach with my boyfriend. 表“借由”之意: 1234on foot 步行（非by foot)你每天早上都是走路上学吗？Do you go to school on foot every morning? 表“凭借、遵照”： 1234567act on one&apos;s advice 按照某人的建议行事act on one&apos;s order 遵照某人的命令行事我会按照你的建议行事，试着去达成目标I&apos;ll act on your advice and try to achieve the goal.不要问太多。只要遵照我的命令行事就对了Don&apos;t ask too much. Just act on my order. 与表有关“无线电器材”之名词连用： 123456on/over the telephone 在打电话on television 在电视屏幕上（非on the television)on/over the radlo 在广播中史蒂芬正在通电话Steven is talking on the telephone. 与表“赞美、恭贺”的名词连用，表“为了/因为…”: 1234congratulate sb on（代）名词／动名词 因⋯而恭贺某人他的家人恭喜他通过了考试His family congratulated him on passing the exam. 与“日子”连用： 123456789101112on Sunday 在星期天on December 11 在12月11日爱弥尔星期天都上教堂Emile goes to church on Sundays.in the moming/aftemoon/evening 在早上/下午/傍晚at ninght 在晚上但moming/afternoon/evening如与日子连用时，介词用onon the aftemoon of Auguest 16 在8月16日下午on Mondy moming 在星期一早上on Saturday night 在星期六晚上 On的其他重要用法： 1234567on call 随时待命on duty 值班/上班时间中消防队员随时为紧急情况待命The firefighters are on call for emergenies.警卫的值班时间是从早上7点到晚上9点The guard is on duty from 7:00 a.m. to 9:00 p.m. to 通常表“到达、往”之意： 12345678go the station 到车站去be moved to tears 感动得落泪up to+数词 多达（若干数词）他被《泰坦尼克号》这部电影感动得落泪He was moved to tears by the move Titanic.他每个月赚将近5万美元He makes up to $50,000 per month. 与“人”连用表“对某人而言”： 1234be everything to sb 是某人的一切她的子女是她的一切Her children is everything to her. 下列名词连用，形成固定用法，表“针对”之意： 1234567891011the key to success 成功之钥（非the key of success)the answer to the question 问题的答案（非the answer ofthe question)the solution to the problem 解决问题之道（非the solution of the problem)成功之钥就是勤劳The key to success is diligence.这个问题的答案没人知道The answer to this question is unknown. to亦与情绪名词连用，形成“To one‘s＋情绪名词⋯” 令某人的是⋯ 1234567891011121314To one&apos;s surise,... 令某人惊讶的是...To one&apos;s joy,... 令某人高兴的是...To one&apos;s satisfaction,... 令某人满意的是...To one&apos;s dismay,... 令某人沮丧的是...To one&apos;s astoinshment,... 令某人惊奇的是...令我惊讶的是，他什么也没说就离开了To my surpise, he left without saying anything.令我们高兴的是，他通过了入学考试，进人一所好大学就读To our joy, he passed entrance exam and enterd a good university.表“就某人所知”亦与介词to连用:就我所知，这个月初约翰已经移居香港了To my understanding/knowledge, John moved to Hong Kong early this month. 表“酉己合（音乐、曲调、节拍等）” 12345678910dance to the music 随着音乐跳舞＊不可说dance with the music 抱着音乐一起跳舞扭(x)dance to the melody 随着旋律跳舞老师放音乐后,大家便开始随着音乐起舞The teacher had the music on and everyone started to dance to the music.dance with sb 与某人跳舞我想和你跳舞I&apos;d like to dance with you. “be upto＋事”表“从事某事”，通常指令人怀疑或不好的事，常用于问句： 123456你在搞什么鬼What are you up to?lt&apos;s up to sb. 则表“由某人做决定”由你决定吧It&apos;s up to you. with 表“使用（某物）”之意： 1234567cut it with a knife 用刀切它do it with care 以谨慎的心做这件事妈用刀子切马铃薯My monther cut the potato with a knife.她谨慎地做每一件事She does everything with care. with亦可放句首，表“有⋯ ⋯；随着⋯ ⋯ ” 1234随着时间的流逝，他的年纪愈来愈大With the passing of time, he&apos;s getting older.他是个有大鼻子的人He is a man with a big nose. with也可形成复台结构，通常为大动作附带小动作时，大动作以本动词表示，小动作则使用with复合结构，其结构为: 1234567当时他双臂交叉着坐在那里He was sitting there with his arms folded.上句中“坐”为主要动词，“双臂交叉”为附带动作，故后者使用“ with his anns folded”当时他嘴里含着烟斗跟我说话 He was talking to me with a pipe in his mouth.=He was talking to me pipe in mouth. 上句中“讲话”为主要动词，“嘴里含烟斗”为附带动作。 With的其他常态用法： 1234567891011leave sth with sb 将某物留给某人compare A with B 将A与B作比较be popular with+一群人 受某一群人欢迎be wrong with sb 某人出了差错他的父母死后留给他一笔财富His parents left a fortune with him after they died.她有点不对劲Something&apos;s wrong with her.你怎么了What&apos;s the matters with you? about 表“有关”，相当于on/concerning/regarding: 1234写一往篇有关空气污染的文章 write an article about air pollution=write an article on/concerning/regarding air pollution.about多为口语用法，on则在文笔中使用 after 在“追逐”，通常与动词连用 1234567 be after 人 追逐/追捕某人=run after 人=chase (after) 人警方仍在追缉这句小偷The police are still after the thief. running after the thief. chasing the thief. 表“像、模仿” 1234567 take after 像...=resemble...=look like...他长得像他爸爸 He takes after his father.=He resembles his father.=He looks like his father. toward 表“朝向” 1234567891011121314151617toward sb 朝某人的方向toward the station 朝车站的方向toward evening 快傍晚时那个女孩带着灿烂的笑容跑向他The gril ran toward him with a bright smile.forward 往前(adv)backward 往后、倒退(adv)wayward 任性的、四面八方的(adj)那个小孩子正摇摇晃晃地向前走The little child is todding forward.学生们被要求倒退着跑The students were asked to run backward.她是个任性的女孩，从不会为别人着想She is a wayward girl, who never thinks about others. 表“对于” 123one&apos;s attitude toward/to... 某人对...的态度你对于安乐死的看法如何？What&apos;s your attitude toward mercy killing? except 除了…之外 12345678除了他之外，所有的人都会去 All except him will go.=All save him will go.Everybody can do it except Peter. He enjoys nothing except music.=He enjoys nothing but music. except+ that从句(或for+名词)，表示“只可惜…”或“只不过是…” 123他人蛮不错，只不过是脾气不好She is quite nice except that he is bad一tempered. for his bad一tempered. behind 在…之后 1234房子后面有一棵树There is a tree behind the house.他正坐在我后面He is sitting behind me. 落后 12345678910behind schedule 比预定时间落后他总是比预定时间落后完成工作He always finished his work behind schedule.on shedule 按预定时间ahead of shedule 比预定时间提前我们会尽量使每件事按预定时间进行We&apos;ll try to keep everything running on schedule.火车比预定时间提前到达The train arrived ahead of shedule. into 进入 12345walk into the classroome 走进教室run into the room 跑进房间 run into+人 与某人巧遇=bump into+人 转变成 12345change/transform/turn A into B 将A转变成Btranslate A into B 将A翻译成B那个女巫把自己变成了一个小女孩The witch changed herself into a little girl. 与be动词连用，形成”be into”的短语，表“热衷于…” 12他热衷于古典音乐He is into classical music. within 表“在…之内”，尤其指在某空间或某时间之内，相当于in 1234 within the country=in the country within an hour 在一小时之内=in an hour 也可指范围，表“在…范围内” 123act within the law 在法律范围内行事守法的公民应该依法行事A law一abiding citizen should act within the law. without 表“没有/无…” 123456coffee without sugar 无糖的咖啡without his help 没有他的帮助Robert likes to drink coffee without sugar.若没有他的帮助，我当时就无法成功I couldn&apos;t have succeeded without his help. 与without有关的短语 12345678can do without... 没有...也行cannot do without... 没有...就不行It goes without saying+that从句 不用说，...我可以没有钱，但不能没有你的爱I can do without money, but I cannot do without your love.不用说，金钱不能买到每一样的东西It goes without saying that money can&apos;t buy everything. above 表“在…上方” 12Some birds are flying above(或over) the city.Nanjing is located above Hangzhou on the map. 在下列短语中，above表“不屑”之意： 1234567be above＋动名词不屑／不愿（做）⋯他不屑于诈骗别人He is above cheating.但be not above＋动名词愿意（做）⋯他总是不耻下问He is not above asking questions. over 表“在…这上”，相当于above，但通常为悬空的状态: 12 be over the city=be above the city 表“覆盖在⋯ ⋯之上”，为有接触物体的状态： 123put one&apos;s hands over one&apos;s faee 把某人的手放在某人的脸上他把手放在她的脸上，告诉她不要担心He puts his hands over her face, telling her not to worry. 表“（悬空）略过”，尤与动词iump连用： 123jump over the wall 跳过这座墙那名窃贼跳过墙跑到街上去了The thief jumped over the wall and ran down the street. 与地方名词连用，表“在某地方的另一头”: 123somewhere over there 在那儿的某处我住在马路那头I live over the road. 与数词连用表“超过”之意： 12345 over+数词 超过某数词=more than+数词在这场车祸中有超过20个人受到重伤 Over twenty people were seriously injured in the traffic accident.=More than twenty were seriously injured in the traffic accident. 与表“驾驭”同义之名词或动词连用： 1234567have authority/control/power over... 有支配、指挥⋯ ⋯的权力rule over... 统治...总统有权指挥军队The president has authority over the army.这个国王已经统治人民十年了The king has ruled over the people for ten years. 与咖啡、茶、三餐等连用，表“吃／喝⋯ ⋯了一段时间”，常与talk连用: 1234我们谈了一杯茶的光景We talked over a cup of tea.他们聊了一顿晚饭的时间They talked over dinner. 词辨 123456789101112131415161718a. 当描述某物的位置高于另一物时，above与over均可使用:房子上方有一个阁楼 There is an attic above the house.=There is an attic over the house.b. 但若表“从某物的一边移动到另一边”之动态动作，则只可用over:那个小孩把球丢到墙外The child threw the ball over the wall.c. 另外above与over均可表“超越”，但above多与表示最小等级或一固定点之名词连用，over则与数词、时间、年龄、金钱等连用:玉山的高度为海拔3952米Mt. Jade is 3952 meters above seal level.这个男孩身高超过同年龄其他小孩The boy is above average height for his age.这个男人30多岁了The man is over thirty.我花了500多元买这本书I spent over five hundred dollars buying this book. below 表“在（某物体）的下方” : 12345write below the line 写在横线下方这家百货公司下面有一家超市There is a supermarket ablow the department store.离桥下远处有个瀑布There is a waterfall far below the bridge. 表“（数量等）少于…；（地位等）低于…”: 12他是个不到50岁的男子He is a man below fifty. beneath 表‘在（某空间）之正下方”: 12他们住在同一个屋檐下The live beneath the same roof. 表“（身份、地位、智力等）低于⋯”，相当于below： 12法兰克的智力远不及丹尼尔Frank is far beneath Daniel in intelligence. under 表“在（某物体）的下面”: 12345under the tree 在树下under the bed 在床下我在床底下找到我的乌龟I found my turtle under the bed. 表“低于⋯；少于⋯”: 123under seven years old 7岁以下在美国，21岁以下的人不可以买酒People under 21 are not allowed to buy alcohol in America. 表“在⋯ ⋯支配/控制/影响下”： 123456under the president 由总统统治under the influence of drugs 在药物的影响下这个国家现在由一位仁慈的皇后所统治The country is now under a benevolent queen.他在酒醉后吐露了实情He told the truth under the influnnce of wine. 表“承受着（重担，压力等）；在⋯的情况/状态下”: 12345678under the heavy pressure of work 在沉重的工作压力下under any cireumstances 在任何情况下under construction 施工中在任何情况下都不要告诉别人这个秘密Don&apos;t tell others the secret under any circumstances.这座桥仍在施工中The bridge is still under construction. 表“接受（考验、刑罚、手术等）”: 1234567under an eye operatinn 接受眼部手术under penalty 受到处罚他在那家医院接受眼部手术He is under an eye operation at that hospital.凡是违反规定的人就会受到处罚People who disobey the regulations will be under penalty. 表“依照/根据（约定、法令等）”： 12根据美国宪法，人皆生而平等All people are equal under the U.S. comstitution. 表“属于⋯项目” : 12小说属于文学类Novels come under literature. underneath表“在…之下”，相当于under或beneath： 123456 underneath the table 在桌子下=under the table=beneath the table那只猫正在桌子底下睡觉The cat is sleeping underneath the table. against 表“对抗、抵制、反对” : 123be against a plan 反对一个计划所有的成员都反对他提出的计划All members were against the plan he brought forward. 表“顶着”： 123lean against the wall 靠墙站小心那个靠墙站着的男子Be careful of the man leaning against the wall. 表“以…为背景”: 12那个颜色在你皮肤的衬托下看起来不错That color looks good against your skin. along 表“沿着⋯”: 123walk along the river 沿着河走沿着街走,你会看到很多书店Walk along the stree and you&apos;ll see many bookstores. 表“顺畅”： 123get along with 人 某人相处得很好辛西亚和她的同学相处得很好Cindy gets along with her classmates. before表“在⋯ ⋯之前”，之后可接表空间或时间的名词： 12345stand before the car 站在车前before June 在6月之前站在那辆黑色汽车前的男孩是我哥哥。The boy standing before the black car is my brother. beyond表“超越”之意： 12345678910be beautiful beyond description 美得难以形容beyond one&apos;s imagination 超乎某人的想象beyond one&apos;s expectation 出乎某人意料之外那里的风景美得难以形容The scenery there is beautiful beyond description.他的聪明超乎我们的想像His is so smart beyond our imagination.珍妮出人意料地赢得了比赛Jenny won the competition beyond everyone&apos;s expectation. during表“在（一段时间）期间” : 12345during his stay here 在他待在这里期间during the two months 在这两个月中这两个月期间我都不会在这里I won&apos;t be here during the two months. through 表“经过”： 123walk through the crowd 穿过人群那位歌手穿过人群离开了The singer walked through the crowd and left. 表“取道／通过”: 123travel through Hong Kong 取道香港去旅游我们取道香港去加拿大We went to Canada through Hong Kong. 表“经由”： 1234567through one&apos;s help 经由某人的帮助through the magazine 透过这本杂志吉娜经由琳达的帮忙而获得这份工作Gina got the job through Linda&apos;s help.我是透过这本杂志认识汤姆的I knew Tom through the magazine. 表“从（活动、场合等）开始到结束”: 1234through the whole meeting 会议开始到结束从会议开始到结束经理一句话都没说The manager didn&apos;t say a word through the whole metting. 表“整段时间”: 1234through the day 一整个白天through thei nght 一整个晚上他白天一整天都在街上游荡He wandered around on the street through the day. besides表“除了…之外”，相当于“in addition to”： 123456 besides +（代）名词/动名词 除了⋯之外=in addition to +（代）名词/动名词除了游泳，他也会溜冰 Besides swimming, he can also roller一skate.=In addition to swimming, ... till/untill表“直到…”: 1234567891011121314151617till与until可互通使用，不过until较till正式。 stay here till ten 在这里待到十点=stay here until tenuntil另常与not连用，形成：not...ntil... 直到...才...他直到九点才来 He didn&apos;t come until nine o&apos;clock.=Not until nine o&apos;clock did he come.till有时与from连用，形成： from...till 从⋯到⋯=from...to...那场电影我们从8点看到10点 We watched the movie from eight till ten.=We watched the movie from eight to ten. since表“自从…”，通常与完成时连用： 12345since+时间名词/动名词 自从⋯他从星期二就开始生病了He has been ill since Tuesday.自从大学毕业之后，她就一直在这家公司工作Since graduating from college, she has been working in this company. beside 表“在…之旁”．相当于by之意： 12345 sit beside me 坐在我旁边=sit by my side凯伦，过来坐在我旁边Come and site beside me, Karen. 表“与⋯相较”，相当于“compared with”或“in comparison with”： 1234跟他相比，我便微不足道了Beside him, I&apos;m nothing.Comparedd with him, I&apos;m nothing.In comparison with him, I&apos;m nothing. 反意疑问句 反意疑问句的使用规则: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546陈述句为肯定时，接否定反意疑问句，陈述句为否定时，接肯定反意疑问句:a. 陈述句有匕e动词，反意疑问句沿用be动词:他人不错，是不是？He is nice, isn&apos;t he?我没病，是不是?I wasn&apos;t sick, was I?b. 陈述句有助动词，反意疑问句沿用助动词:他会来，是不是?He will come, won&apos;t he?他们不能做这件事，是不是?They can&apos;t do it, can they?你做完了，是不是？You have done it, haven&apos;t you?c. 陈述句有一般动词，反意疑问句则使用do、does、did:他来了，是不是？He came, didn&apos;t he?他起得很早，是不是？He gets up early, doesn&apos;t he?他没来，是不是？He didn&apos;t come, did he?d. 与祈使句使用时，反意疑问句一律用will you:来这里，好不好?Come here, will you?别再抽烟，好不好?Stop smoking, will you?e. 与祈使句“ Let&apos;s...”（咱们⋯句型使用，反意疑问句一律用shall we:咱们走吧，好不好?Let&apos;s go, shall we?咱们别做那件事，好不好?Let&apos;s not do it, shall we?但：请你让我们／他们／他／⋯ ⋯／约翰走吧，好不好？let us/them/him/.../John go, will you? 句中有否定副词 123456789陈述句含有scarcely/hardly/rarely/no doubt/little/never/by no means等否定副词．陈述句视为否定句，须接肯定反问句:他很少抽烟是不是？He |scarcely ever| smokes, does he? |hardly ever | |rarely |毫无疑问他是个好孩子，是不是？He is no doubt a good boy, is he? 句中有助动词短语: 12345678910陈述句含有would rather、had better等助动词短语．反意疑问句中用其第一个词:你宁愿去，是不是？You would rather go? wouldn&apos;t you?你最好做这事，是不是？You had better do it? hadn&apos;t you?他应当来，是不是？He ought to come, shouldn&apos;t he? 反意疑问句须用人称代词: 123456789反意疑问句一定要用人称代词，但叙述句首为Thereis/are/was/were等时，则须用thereJohn is fine, isn&apos;t he?This is not good, is it?That is good, isn&apos;t it?These are not good, are those?Those are good, aren&apos;t they?那里有个人，是不是？There is a man there, isn&apos;t there? 陈述句中，主语若为第一人称单数I表示意见或观点时，不可能对自己反问，应以其后的that从句形成反问: 123456我认为大卫人不错，是不是？I think that David is nice, isn&apos;t he?但主语若为I以外的主语，则反问句仍以主句为依据形成反问句:我们认为这个新老师很好，是不是？We think that the new teacher is nice, don&apos;t we? need作一般动词及助动词的反问句： 123456need可作一般动词，也可作为助动词（仅限于否定句，须与not并用，无论第几人称need之后都不加s）:他需要过去，是不是？He needs to go,doesn&apos;t he?他不需要过去，是不是？He need not go, need he?","categories":[{"name":"English","slug":"English","permalink":"http://blog.gcalls.cn/categories/English/"}],"tags":[{"name":"English","slug":"English","permalink":"http://blog.gcalls.cn/tags/English/"}]},{"title":"别错过孩子英语学习敏感期","slug":"别错过孩子英语学习敏感期","date":"2019-07-28T06:19:24.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2019/07/别错过孩子英语学习敏感期.html","link":"","permalink":"http://blog.gcalls.cn/2019/07/别错过孩子英语学习敏感期.html","excerpt":"遵循语言自然习得规律，让孩子像学母语一样学英语。从英语零基础到小学毕业，自主阅读原版&lt;&lt;哈利.波特&gt;&gt;！不让孩子再走英语学习长征路！","text":"遵循语言自然习得规律，让孩子像学母语一样学英语。从英语零基础到小学毕业，自主阅读原版&lt;&lt;哈利.波特&gt;&gt;！不让孩子再走英语学习长征路！ 概述 从0岁开始听英语童谣、进行指物训练 从1岁起坚持亲子共读英文绘本 从2岁起观看英语动画 从3岁起字母和自然拼读 从5岁起开启分级阅读 0岁+：指物训练和英语童谣直接用英语说给孩子听，和小宝宝玩英语指物训练游戏，如： This is your be. Where is your bed? 听英语童谣： 40首必听必唱的经典英语童谣： Q&amp;A不需要翻译成中文，家长可以通过图画示意和肢体大动作以及精细动作的演示，让孩子主动了解图画代表的意思。童谣的意义不是让孩子理解意思，而是熟悉语言的拼读。 建议家长在童谣输入上看重质量而不是数量，可以建立一个自己的英语童谣库，按照动作或主题分次念给孩子听，玩亲子互动游戏。一周只一两首童谣进行互动，每天重复同样的童谣，日积月累，输入效果也会非常明显。切记，孩子学语言最重要的方式是模仿和重复。 1岁+：亲子共读英文绘本 每天至少共10分钟为孩子快乐地大声朗读 每天至少读三个故事，可以是同一个故事重复读三遍。孩子在开始学会阅读前，需要聆听一千个故事。 生动活波地大声朗读。 读宝宝喜欢的故事，一遍以一遍地反复读，每本书保持同样的朗读风格。 玩和书本内容相关的游戏。 绝对不要刻意教孩子怎么读，也不要一靠近书就紧张。 每天给孩子大声读书。 父母尽可以大量地经孩子读英文绘本，不必怕把孩子的口音带偏了，即使带偏以后还是能回到路上的。 亲子共读时，前面几次可以一遍英文朗读加一遍中文翻译，之后就可以省去中文。如果图文对应关系很强则不必翻译。 每天坚持30分钟左右： 选对书 用各种方法刺激孩子英文绘本的兴趣 用心给孩子读书 坚持每天读 只要孩子对英文绘本感兴趣，开不开口说英语不要主过介意。只纠正孩子弄错的概念即可。家长应先“预习好功课”，把绘本中每个单词的发单都搞明白，再模仿句子的语音语调，尽量让自己读得更加生动地道。在孩子与家长共读了几遍绘本、对书的内容已经有了初步了解后，家长就可以给孩子播放音频、取代真人阅读了。 2岁+：看英语动画片建议当孩子2岁以后，看动画片可以逐步成为每天的例行活动，动画片要与阅读英文绘本、听英语童谣互相配合，一起构成孩子日常的英语输入环境。 家长最好陪孩子一起看 首选纯英文不带字幕的 不要贪多，要能耐住性子“反复观看” 收集与孩子喜欢的动画片相关的绘本、童谣等资源 不要一边玩玩具一边看 2-3岁：10-15分钟，每天不超过30分钟 3-4岁：15-20分钟，每天不超过1小时 4岁+：30分钟，每天不超过两个小时 经典童谣动画动画片 车宝四兄弟 小猪佩琪 车轮转呀转 恰克大冒险 汽车镇的故事 火车宝宝 托马斯小火车 天线宝宝 花园宝宝","categories":[{"name":"English","slug":"English","permalink":"http://blog.gcalls.cn/categories/English/"}],"tags":[{"name":"English","slug":"English","permalink":"http://blog.gcalls.cn/tags/English/"}]},{"title":"Jenkins移动端自动构建","slug":"Jenkins移动端自动构建","date":"2019-06-13T08:46:34.000Z","updated":"2025-02-05T05:50:33.234Z","comments":true,"path":"/2019/06/Jenkins移动端自动构建.html","link":"","permalink":"http://blog.gcalls.cn/2019/06/Jenkins移动端自动构建.html","excerpt":"记录一下jenkins自动构建android与ios安装包。","text":"记录一下jenkins自动构建android与ios安装包。 安装Jenkins123456789101112brew install openjdk@11sudo ln -sfn /opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-11.jdkbrew install jenkins-ltsbrew services start jenkins-lts#Generating for QR codebrew install qrencode#brew install md5sha1sum#brew services list#第一次运行需要通过以下命令查看启动日志：brew services stop jenkins-lts/opt/homebrew/opt/openjdk@11/bin/java -Dmail.smtp.starttls.enable\\=false -jar /opt/homebrew/opt/jenkins-lts/libexec/jenkins.war --httpListenAddress\\=0.0.0.0 --httpPort\\=8080 --prefix=/ci 注意：2.263.4为正确运行的版本，如果通过uninstall后，可以在安装后手动替换掉war文件：123#cp -a /Developer/jenkins-war-2.263.4.war /usr/local/opt/jenkins-lts/libexec/jenkins.war#rsync -avP jenkins@192.168.101.83:/usr/local/opt/jenkins-lts/libexec/jenkins.war /opt/homebrew/opt/jenkins-lts/libexec/cp -a ~/Developer/jenkins-war-2.263.4.war /opt/homebrew/opt/jenkins-lts/libexec/jenkins.war 新版本的workspace默认为${ITEM_ROOT}/workspace，位于job目录下。需要修改~/.jenkins/config.xml文件的内容：1&lt;workspaceDir&gt;$&#123;JENKINS_HOME&#125;/workspace/$&#123;ITEM_FULL_NAME&#125;&lt;/workspaceDir&gt; 修改配置： #/usr/local/Cellar/jenkins-lts/2.289.1/homebrew.mxcl.jenkins-lts.plist:/opt/homebrew/opt/jenkins-lts/homebrew.mxcl.jenkins-lts.plist: 123456789101112... &lt;string&gt;/opt/homebrew/opt/openjdk@11/bin/java&lt;/string&gt; &lt;string&gt;-Dmail.smtp.starttls.enable=false&lt;/string&gt; &lt;string&gt;--httpPort=8080&lt;/string&gt; &lt;string&gt;--prefix=/ci&lt;/string&gt;... &lt;key&gt;RunAtLoad&lt;/key&gt; &lt;true/&gt; &lt;key&gt;StandardOutPath&lt;/key&gt; &lt;string&gt;/Users/jenkins/.jenkins/logs/stdout.log&lt;/string&gt; &lt;key&gt;StandardErrorPath&lt;/key&gt; &lt;string&gt;/Users/jenkins/.jenkins/logs/error.log&lt;/string&gt; 系统配置没有采用jenkins插件方式安装，而是通过命令行shell脚本直接运行，所以需要事先在这台电脑上安装配置好相关的环境，并确保手动构建、打包功能正常，手动发布到appstore能成功。 android gradle配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859ext &#123; defaultIsJenkins = \"false\" isJenkins = project.hasProperty('IS_JENKINS') ? IS_JENKINS : defaultIsJenkins&#125;def loadSigningConfig() &#123; if (\"true\".equals(isJenkins)) &#123; // Create a variable called keystorePropertiesFile, and initialize it to your // keystore.properties file, in the rootProject folder. def keystorePropertiesFile = rootProject.file(\"/Users/test/.jenkins/scripts/signingConfigs.properties\") // Initialize a new Properties() object called keystoreProperties. def keystoreProperties = new Properties() // Load your keystore.properties file into the keystoreProperties object. keystoreProperties.load(new FileInputStream(keystorePropertiesFile)) android.signingConfigs.config.keyAlias = keystoreProperties['keyAlias'] android.signingConfigs.config.keyPassword = keystoreProperties['keyPassword'] android.signingConfigs.config.storeFile = file(keystoreProperties['storeFile']) android.signingConfigs.config.storePassword = keystoreProperties['storePassword'] &#125;&#125;android &#123; ... signingConfigs &#123; config &#123; &#125; &#125; loadSigningConfig() defaultConfig &#123; ... versionCode rootProject.ext.versionCode versionName rootProject.ext.versionName ... &#125; ... buildTypes &#123; release &#123; ... if (\"true\".equals(isJenkins)) &#123; signingConfig signingConfigs.config &#125; &#125; debug &#123; if (\"true\".equals(isJenkins)) &#123; signingConfig signingConfigs.config &#125; &#125; &#125; applicationVariants.all &#123; variant -&gt; variant.outputs.all &#123; output -&gt; if (\"true\".equals(isJenkins)) &#123; outputFileName = new File(\"xwallet-\"+rootProject.ext.versionName+\".\"+rootProject.ext.versionCode + \"-\"+BUILD_TYPE+\".apk\") &#125; ... &#125; &#125; nginx配置安装： 12brew install nginxbrew services start nginx #/usr/local/etc/nginx/nginx.conf:/opt/homebrew/opt/nginx/nginx.conf: 1234567891011121314151617181920212223242526272829303132333435363738394041server &#123; listen 443 ssl; server_name appbuild.zerofinance.hk; server_tokens off; client_max_body_size 0; charset utf-8; # ssl on; ssl_certificate /Users/jenkins/.jenkins/ssl/zerofinance.hk/zerofinance.crt; ssl_certificate_key /Users/jenkins/.jenkins/ssl/zerofinance.hk/zerofinance.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ECDH:AESGCM:HIGH:!RC4:!DH:!MD5:!aNULL:!eNULL; ## Individual nginx logs for this GitLab vhost access_log /Users/jenkins/works/logs/www_access.log; error_log /Users/jenkins/works/logs/www_error.log; location ^~ /ci/appDownloads &#123; alias /Users/jenkins/works/; autoindex on; &#125; location ^~ /eclipse &#123; alias /Users/jenkins/works/eclipseupdate/; autoindex on; &#125; location / &#123; return 301 /ci; &#125; location /ci &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_redirect off; proxy_pass http://127.0.0.1:8080; &#125;&#125; jenkins配置安装Build Name and Description Setter插件，并Set Build Name的Build Name为： 1#$&#123;BUILD_NUMBER&#125;_$&#123;PLATFORM&#125;_$&#123;BUILD_TYPE&#125;_$&#123;ENV&#125;_$&#123;GIT_BRANCH&#125; 先安装Environment Injector Plugin插件，位置位于”Excute Shell”之后，配置Properties File Path为： 1/tmp/$&#123;JOB_NAME&#125;-$&#123;ENV&#125;-$&#123;BUILD_NUMBER&#125;.properties Build-&gt;Execute shell: 123456if [[ \"$GIT_BRANCH\" == \"\" || \"$BUILD_TYPE\" == \"\" || \"$PLATFORM\" == \"\" || \"$ENV\" == \"\" ]]; then echo \"Parameters must not be empty!\" exit -1else bash $&#123;JENKINS_HOME&#125;/scripts/buildApp.shfi 参考脚本: 自动上传到app store请参考：https://testerhome.com/topics/10507 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516#!/bin/bash######Only work for macos######source /Users/jenkins/.zshrc#BUILD_TYPE=$1#PLATFORM=$2#WORKSPACE=$3#ENV=$4#GIT_BRANCH=$5#The signer tool for signing a apk ileAPKSIGNER=/Users/jenkins/Library/Android/sdk/build-tools/28.0.3/apksignerZIPALIGN=/Users/jenkins/Library/Android/sdk/build-tools/28.0.3/zipalign#ios altool path, only for uploading ipa to app store#ALTOOLPATH=/Applications/Xcode.app/Contents/Applications/Application\\ Loader.app/Contents/Frameworks/ITunesSoftwareService.framework/Versions/A/Support/altoolALTOOLPATH=/Applications/Xcode.app/Contents/SharedFrameworks/ContentDeliveryServices.framework/Versions/A/Frameworks/AppStoreService.framework/Versions/A/Support/altool#Script homeSCRIPTS_HOME=/Users/jenkins/.jenkins/scripts#Base path for geneating filesSAVE_BASE_PATH=/Users/jenkins/worksJIAGU_JAVA=$&#123;SCRIPTS_HOME&#125;/merchantjiagu/java/bin/javaJIAGU_JAR=$&#123;SCRIPTS_HOME&#125;/merchantjiagu/jiagu.jarBUNDLETOOL=$&#123;SCRIPTS_HOME&#125;/bundletool-all-1.15.6.jar#The qrcode tool for generating a qrcode from urlQRENCODE=/usr/local/bin/qrencode#A key for uploading apk to google playPROXY_HOST=scloud.aaa.comPROXY_PORT=1082APK_UPLOAD=$&#123;SCRIPTS_HOME&#125;/apk_upload/bin/apk_uploadAPKUP_KEY=$&#123;SCRIPTS_HOME&#125;/xpaymerchantapp-dfec2cb36528.jsonAPKUP_APPID=111.222.merchant# googleServicesJson=$&#123;SCRIPTS_HOME&#125;/google-services.json#The temporary variable between this script and email jelly fileTMP_PROPERTIES_FILE=/tmp/$&#123;JOB_NAME&#125;-$&#123;ENV&#125;-$&#123;BUILD_NUMBER&#125;.properties#App nameecho \"APPNAME = $&#123;APP_NAME&#125;\"APP_NAME=merchantAppK8S_FILE_NAME=xpay-merchant-app#The environment variables of git folderEVN_FOLDER=/tmp/xpay-merchant-app-config/k8s-$&#123;ENV&#125;-config#The base release name of gitRELEASE_NAME=$&#123;GIT_BRANCH//origin\\//&#125;#RELEASE_NAME=$&#123;GIT_BRANCH//refs\\/heads\\//&#125;#The app host urlAPP_HOST_URL=$&#123;JENKINS_URL//ci\\//&#125;#The download base urlAPP_BASE_URL=$&#123;JENKINS_URL&#125;appDownloads#the external base url, only for reinforcing apk#EXTERNAL_BASE_URL=http://218.17.1.146:45682/ci/appDownloads#The substring pathSUB_PATH=$&#123;JOB_NAME&#125;/$&#123;ENV&#125;/$&#123;RELEASE_NAME&#125;#The final app urlAPP_URL=$&#123;APP_BASE_URL&#125;/$&#123;SUB_PATH&#125;/$&#123;BUILD_NUMBER&#125;#The history pathHISTORY_URL=$&#123;APP_BASE_URL&#125;/$&#123;JOB_NAME&#125;/$&#123;ENV&#125;#The final external url, only for reinforcing apk#EXTERNAL_URL=$&#123;EXTERNAL_BASE_URL&#125;/$&#123;SUB_PATH&#125;/$&#123;BUILD_NUMBER&#125;#The path for geneated files, only for copying geneated files to antoher directoryAPP_SAVE_PATH=$&#123;SAVE_BASE_PATH&#125;/$&#123;SUB_PATH&#125;/$&#123;BUILD_NUMBER&#125;#Developement plistDEV_PLIST=$&#123;SCRIPTS_HOME&#125;/developement.plist#Release plistRELEASE_PLIST=$&#123;SCRIPTS_HOME&#125;/ExportOptions.plist#the plist file which for itms-servicesPLIST_TEMPLATE=$&#123;SCRIPTS_HOME&#125;/mechantApp.template#App Released history list#For Android: $&#123;SCRIPTS_HOME&#125;/apphistory.android#For IOS: $&#123;SCRIPTS_HOME&#125;/apphistory.iosAPP_HISTORY_LIST=$&#123;SCRIPTS_HOME&#125;/mechantApphistory#verification for buildingsigningConfigs=$&#123;SCRIPTS_HOME&#125;/xpaySigningConfigs.propertiesappId=$(cat $&#123;signingConfigs&#125; |grep appId|awk -F '=' '&#123;print $2&#125;')appPassword=$(cat $&#123;signingConfigs&#125; |grep appPassword|awk -F '=' '&#123;print $2&#125;')#apphostToken=$(cat $&#123;signingConfigs&#125; |grep apphostToken|awk -F '=' '&#123;print $2&#125;')#secretId=$(cat $&#123;signingConfigs&#125; |grep secretId|awk -F '=' '&#123;print $2&#125;')#secretKey=$(cat $&#123;signingConfigs&#125; |grep secretKey|awk -F '=' '&#123;print $2&#125;')storePassword=$(cat $&#123;signingConfigs&#125; |grep storePassword|awk -F '=' '&#123;print $2&#125;')keyAlias=$(cat $&#123;signingConfigs&#125; |grep keyAlias|awk -F '=' '&#123;print $2&#125;')keyPassword=$(cat $&#123;signingConfigs&#125; |grep keyPassword|awk -F '=' '&#123;print $2&#125;')storeFile=$(cat $&#123;signingConfigs&#125; |grep storeFile|awk -F '=' '&#123;print $2&#125;')jiaguuser=$(cat $&#123;signingConfigs&#125; |grep jiaguuser|awk -F '=' '&#123;print $2&#125;')jiagupwd=$(cat $&#123;signingConfigs&#125; |grep jiagupwd|awk -F '=' '&#123;print $2&#125;')#Make sure APP_SAVE_PATH existedmkdir -p $APP_SAVE_PATH#passing variable to email jelly fileecho \"appUrl=$APP_URL\" &gt;&gt; $TMP_PROPERTIES_FILEecho \"historyUrl=$HISTORY_URL\" &gt;&gt; $TMP_PROPERTIES_FILEecho \"appHostUrl=$APP_HOST_URL\" &gt;&gt; $TMP_PROPERTIES_FILE#Uploading app to APP_HOST which is a APP publish websitefunction uploadApp() &#123; platformParam=$1 filePath=$2 appJenkinsUrl=$3 iosIpaUrl=$4 universalApkPath=$5 universalAppJenkinsUrl=$6 #echo \"Uploading app for $platformParam...\" echo \"filePath=$filePath\" echo \"appJenkinsUrl=$appJenkinsUrl\" #abb to apk: only for android if [[ \"$iosIpaUrl\" == \"\" &amp;&amp; \"$APK_TYPE\" == \"aab\" &amp;&amp; \"$AAB_TO_APK\" == \"Y\" ]]; then echo \"universalApkPath=$universalApkPath\" echo \"$&#123;platformParam&#125;_universalUrl=$universalAppJenkinsUrl\" echo \"$&#123;platformParam&#125;_universalUrl=$universalAppJenkinsUrl\" &gt;&gt; $TMP_PROPERTIES_FILE $&#123;QRENCODE&#125; -o $APP_SAVE_PATH/$&#123;platformParam&#125;_qrcode.png -s 8 -m 1 \"$universalAppJenkinsUrl\" else $&#123;QRENCODE&#125; -o $APP_SAVE_PATH/$&#123;platformParam&#125;_qrcode.png -s 8 -m 1 \"$appJenkinsUrl\" fi qrcodeGetUrl=\"$&#123;APP_URL&#125;/$&#123;platformParam&#125;_qrcode.png\" echo \"$&#123;platformParam&#125;_downloadUrl=$qrcodeGetUrl\" &gt;&gt; $TMP_PROPERTIES_FILE #echo \"$appJenkinsUrl\" | grep \"itms-services\" &gt; /dev/null if [[ \"$iosIpaUrl\" != \"\" ]]; then #ios directUrl=$iosIpaUrl else #android directUrl=$appJenkinsUrl fi echo \"directUrl=$directUrl\" echo \"$&#123;platformParam&#125;_directUrl=$directUrl\" &gt;&gt; $TMP_PROPERTIES_FILE return 0&#125;#Building android appfunction buildAndroid() &#123; #for android #Checking if the version existed #android/gradle.properties #appVersionCode=300 #appVersionName=5.0 android_history_list=\"$&#123;APP_HISTORY_LIST&#125;.android\" echo \"android_history_list=$android_history_list\" if [[ -f \"$&#123;android_history_list&#125;\" &amp;&amp; \"$UPLOAD_GOOGLEPLAY\" == \"Y\" ]]; then currAppVersionCode=$(cat android/gradle.properties|grep \"appVersionCode\"|awk -F '=' '&#123;print $2&#125;') currAppVersionName=$(cat android/gradle.properties|grep \"appVersionName\"|awk -F '=' '&#123;print $2&#125;') historyAppVersionCode=$(cat $&#123;android_history_list&#125;|grep \"appVersionCode\"|awk -F '=' '&#123;print $2&#125;') historyAppVersionName=$(cat $&#123;android_history_list&#125;|grep \"appVersionName\"|awk -F '=' '&#123;print $2&#125;') echo \"currAppVersionCode=$currAppVersionCode\" echo \"currAppVersionName=$currAppVersionName\" echo \"historyAppVersionCode=$historyAppVersionCode\" echo \"historyAppVersionName=$historyAppVersionName\" #if [ `echo \"$currAppVersionCode &lt;= $historyAppVersionCode\" | bc` -eq 1 -a `echo \"$currAppVersionName &lt;= $historyAppVersionName\" | bc` -eq 1 ]; then if [[ `echo \"$currAppVersionCode &lt;= $historyAppVersionCode\" | bc` == 1 || `echo \"$currAppVersionName &lt;= $historyAppVersionName\" | bc` == 1 ]]; then echo \"Android: The current version you build is less than or equal to the version of histroy, please increase the number of version.\" exit -1 fi fi BUILD_PLATFORM=\"android\" rm -fr $APP_SAVE_PATH/Android.bundle.zip $APP_SAVE_PATH/*.aab $APP_SAVE_PATH/android_qrcode.png cd $&#123;WORKSPACE&#125; # sed -i \"\" \"s;^ *export const ENV *= *'.*';export const ENV = '$&#123;ENV&#125;';g\" $&#123;WORKSPACE&#125;/component/Common/Environment.js # if [[ \"$ENV\" == \"prod\" ]]; then # /bin/cp -a $googleServicesJson android/app/ # fi echo \"Building android: nvm use 16.15.1 npm install --legacy-peer-deps\" nvm use 16.15.1 npm install --legacy-peer-deps &gt; /dev/null echo \"Building android: npm run build-android\" npm run build-android &gt; /dev/null buildResult=$? cd android if [[ \"$ONLY_HOTUPDATE\" == \"N\" ]]; then buildLog=$APP_SAVE_PATH/build-android.log if [[ \"$APK_TYPE\" == \"apk\" ]]; then echo \"Building android: ./gradlew clean assemble$BUILD_TYPE\" output=app/build/outputs/apk/release/ rm -fr $output/*.apk echo \"./gradlew clean assemble$BUILD_TYPE --stacktrace -PIS_JENKINS=true -PBUILD_TYPE=$&#123;BUILD_TYPE&#125; &gt; /dev/null 2&gt; $buildLog\" ./gradlew clean assemble$BUILD_TYPE --stacktrace -PIS_JENKINS=true -PBUILD_TYPE=$&#123;BUILD_TYPE&#125; &gt; /dev/null 2&gt; $buildLog else ##aab echo \"Building android: ./gradlew clean bundle\" output=app/build/outputs/bundle/release/ rm -fr $output/*.aab echo \"./gradlew clean bundle --stacktrace -PIS_JENKINS=true -PBUILD_TYPE=$&#123;BUILD_TYPE&#125; &gt; /dev/null 2&gt; $buildLog\" ./gradlew clean bundle --stacktrace -PIS_JENKINS=true -PBUILD_TYPE=$&#123;BUILD_TYPE&#125; &gt; /dev/null 2&gt; $buildLog fi buildResult=$? fi if [[ $buildResult == 0 ]]; then # hot update package currPwd=`pwd` cd $&#123;WORKSPACE&#125;/android/app/src/main/assets zip -rq $&#123;APP_SAVE_PATH&#125;/Android.bundle.zip * cd $currPwd apkFile=`ls $output|grep $&#123;APK_TYPE&#125;` # APK only exist when non-hotfix if [[ $? == 0 &amp;&amp; -f $output/$apkFile ]]; then filePath=$output/$apkFile universalApkFile=universal.apk singedApkFile=$&#123;APP_NAME&#125;-$&#123;RELEASE_NAME&#125;.$&#123;APK_TYPE&#125; #singedApkPath=$APP_SAVE_PATH/$&#123;apkFile&#125; singedApkPath=$APP_SAVE_PATH/$&#123;singedApkFile&#125; appJenkinsUrl=\"$&#123;APP_URL&#125;/$singedApkFile\" universalApkPath=$APP_SAVE_PATH/$&#123;universalApkFile&#125; universalAppJenkinsUrl=\"$&#123;APP_URL&#125;/$universalApkFile\" #abb to apk if [[ \"$APK_TYPE\" == \"aab\" &amp;&amp; \"$AAB_TO_APK\" == \"Y\" ]]; then apksName=$&#123;APP_NAME&#125;-$&#123;RELEASE_NAME&#125;.apks java -jar $BUNDLETOOL build-apks \\ --mode=universal \\ --bundle=$filePath \\ --output=$APP_SAVE_PATH/$apksName \\ --ks=$storeFile \\ --ks-pass=pass:$storePassword \\ --ks-key-alias=$keyAlias \\ --key-pass=pass:$keyPassword java -jar $BUNDLETOOL extract-apks \\ --device-spec=/Users/jenkins/.jenkins/scripts/device-spec.json \\ --apks=$APP_SAVE_PATH/$apksName \\ --output-dir=$APP_SAVE_PATH/ fi if [[ \"$SKIP_REINFORCE\" == \"N\" ]]; then echo \"Reinforcing the $&#123;APK_TYPE&#125;...\" proxy_off #login $&#123;JIAGU_JAVA&#125; -jar $&#123;JIAGU_JAR&#125; -login $&#123;jiaguuser&#125; $&#123;jiagupwd&#125; echo \"$&#123;JIAGU_JAVA&#125; -jar $&#123;JIAGU_JAR&#125; -jiagu $&#123;APK_TYPE&#125; $&#123;filePath&#125; $&#123;APP_SAVE_PATH&#125;/ -auto-sign\" $&#123;JIAGU_JAVA&#125; -jar $&#123;JIAGU_JAR&#125; -jiagu $&#123;APK_TYPE&#125; $&#123;filePath&#125; $&#123;APP_SAVE_PATH&#125;/ -auto-sign proxy_on if [[ $? == 0 ]]; then echo \"Reinforcing the $&#123;APK_TYPE&#125; successfully...\" jiaguApk=`ls $&#123;APP_SAVE_PATH&#125; | grep jiagu_sign` &gt; /dev/null if [[ \"$jiaguApk\" != \"\" ]]; then mv $&#123;APP_SAVE_PATH&#125;/$&#123;jiaguApk&#125; $singedApkPath else echo \"Not found the reinfored $&#123;APK_TYPE&#125;.\" exit -1 fi else echo \"Reinforcing the apk failed...\" exit -1 fi else cp -a $filePath $singedApkPath fi if [[ \"$UPLOAD_GOOGLEPLAY\" == \"Y\" ]]; then echo \"Uploading file to google play, it may make a few minutes, please wait...\" #https://github.com/eventOneHQ/apkup/blob/master/src/cli/upload.ts #Track can be 'internal', 'alpha', 'beta', 'production' or 'rollout'. Default: 'internal' # nvm use v10.16.0 # #npm i -g apkup # apkup --key \"$&#123;APKUP_KEY&#125;\" --apk \"$&#123;singedApkPath&#125;\" \\ # --release-notes \"en-US=Bug fixes and minor enhancements\" \\ # --release-notes \"zh-HK=錯誤修正及少量更新\" \\ # --track \"$&#123;TRACK&#125;\" # nvm use v16.15.1 #https://github.com/stasheq/google-play-apk-upload bash $&#123;APK_UPLOAD&#125; $&#123;APKUP_APPID&#125; \"$&#123;singedApkPath&#125;\" $&#123;APKUP_KEY&#125; $&#123;PROXY_HOST&#125; $&#123;PROXY_PORT&#125; if [[ $? != 0 ]]; then echo \"Uploading apk to google play failed!\" exit -1 fi echo \"Uploading apk to google play successfully!\" #Recorded the last version in the history when app was uploaded to google play successfully. echo \"appVersionName=$&#123;currAppVersionName&#125;\" &gt; $&#123;android_history_list&#125; echo \"appVersionCode=$&#123;currAppVersionCode&#125;\" &gt;&gt; $&#123;android_history_list&#125; fi uploadApp $BUILD_PLATFORM $singedApkPath $appJenkinsUrl \"\" $universalApkPath $universalAppJenkinsUrl if [[ $? != 0 ]]; then echo \"Building android is successful, but upload failed, maybe caused by establishing network, it exited, please try again.\" exit -1 fi fi else cat $buildLog echo \"Building android is failed. it exited.\" exit -1 fi&#125;#Building ios appfunction buildIos() &#123; # for ios #Checking if the version existed #/* Debug */ #/* Release */ #CURRENT_PROJECT_VERSION = 1; #MARKETING_VERSION = 4.9; ios_history_list=\"$&#123;APP_HISTORY_LIST&#125;.ios\" echo \"ios_history_list=$ios_history_list\" if [[ -f \"$&#123;ios_history_list&#125;\" &amp;&amp; \"$UPLOAD_APPSTORE\" == \"Y\" ]]; then currMarketingVersion=`cat ios/$&#123;APP_NAME&#125;.xcodeproj/project.pbxproj|grep \"MARKETING_VERSION\"|sed -n '2p'|sed 's;\\;$;;'|awk -F ' = ' '&#123;print $2&#125;'` #historyProjectVersion=`cat $&#123;ios_history_list&#125;|grep \"CURRENT_PROJECT_VERSION\"|awk -F ' = ' '&#123;print $2&#125;'` historyMarketingVersion=`cat $&#123;ios_history_list&#125;|grep \"MARKETING_VERSION\"|awk -F ' = ' '&#123;print $2&#125;'` #echo \"currProjectVersion=$currProjectVersion\" echo \"currMarketingVersion=$currMarketingVersion\" #echo \"historyProjectVersion=$historyProjectVersion\" echo \"historyMarketingVersion=$historyMarketingVersion\" if [[ `echo \"$currMarketingVersion &lt;= $historyMarketingVersion\" | bc` == 1 ]]; then echo \"IOS: The current version you build is less than or equal to the version of histroy, please increase the number of version.\" exit -1 fi fi BUILD_PLATFORM=\"ios\" rm -fr $APP_SAVE_PATH/IOS.bundle.zip $APP_SAVE_PATH/*.ipa $APP_SAVE_PATH/*.plist $APP_SAVE_PATH/ios_qrcode.png cd $&#123;WORKSPACE&#125; #sed -i \"\" \"s;^ *export const ENV *= *'.*';export const ENV = '$&#123;ENV&#125;';g\" $&#123;WORKSPACE&#125;/component/Common/Environment.js echo \"Building ios: nvm use 16.15.1 npm install --legacy-peer-deps\" nvm use 16.15.1 npm install --legacy-peer-deps &gt; /dev/null echo \"Building ios: npm run build-ios\" npm run build-ios &gt; /dev/null cd ios output=build/outputs/ rm -fr $output/*.ipa xcarchiveBackUpFolder=$output/xcarchive-`date +%Y%m%d%H%M%S` mkdir $xcarchiveBackUpFolder mv $output/*.xcarchive $xcarchiveBackUpFolder/ if [[ \"$ONLY_HOTUPDATE\" == \"N\" ]]; then proxy_on echo \"Building ios: pod install\" /usr/local/bin/pod install proxy_off echo \"Building ios: xcodebuild clean\" echo \"xcodebuild \\ -workspace \\\"$&#123;APP_NAME&#125;.xcworkspace\\\" \\ -scheme \\\"$&#123;APP_NAME&#125;\\\" \\ -configuration \\\"$&#123;BUILD_TYPE&#125;\\\" \\ clean &gt; /dev/null\" xcodebuild \\ -workspace \"$&#123;APP_NAME&#125;.xcworkspace\" \\ -scheme \"$&#123;APP_NAME&#125;\" \\ -configuration \"$&#123;BUILD_TYPE&#125;\" \\ clean &gt; /dev/null echo \"Building ios: xcodebuild archive\" echo \"xcodebuild archive -workspace \\\"$&#123;APP_NAME&#125;.xcworkspace\\\" \\ -scheme $&#123;APP_NAME&#125; \\ -configuration \\\"$&#123;BUILD_TYPE&#125;\\\" \\ -archivePath \\\"$&#123;output&#125;/$&#123;APP_NAME&#125;-$&#123;BUILD_TYPE&#125;.xcarchive\\\" &gt; /dev/null\" security unlock-keychain -p 'xxxxxx' xcodebuild archive -workspace \"$&#123;APP_NAME&#125;.xcworkspace\" \\ -scheme $&#123;APP_NAME&#125; \\ -configuration \"$&#123;BUILD_TYPE&#125;\" \\ -archivePath \"$&#123;output&#125;/$&#123;APP_NAME&#125;-$&#123;BUILD_TYPE&#125;.xcarchive\" &gt; /dev/null if [[ $? != 0 ]]; then echo \"Archive failed!\" exit -1 fi echo \"Building ios: xcodebuild export archive to ipa\" if [[ \"$ENV\" == \"prod\" ]]; then echo \"xcodebuild -exportArchive -archivePath \\\"$&#123;output&#125;/$&#123;APP_NAME&#125;-$&#123;BUILD_TYPE&#125;.xcarchive\\\" \\ -exportPath \\\"$&#123;output&#125;/$&#123;APP_NAME&#125;-$&#123;BUILD_TYPE&#125;.ipa\\\" \\ -exportOptionsPlist $RELEASE_PLIST &gt; /dev/null\" xcodebuild -exportArchive -archivePath \"$&#123;output&#125;/$&#123;APP_NAME&#125;-$&#123;BUILD_TYPE&#125;.xcarchive\" \\ -exportPath \"$&#123;output&#125;/$&#123;APP_NAME&#125;-$&#123;BUILD_TYPE&#125;.ipa\" \\ -exportOptionsPlist $RELEASE_PLIST \\ -allowProvisioningUpdates YES &gt; /dev/null if [[ $? != 0 ]]; then echo \"Exporting archive to ipa failed!\" exit -1 fi if [[ \"$UPLOAD_APPSTORE\" == \"Y\" ]]; then IPAPATH=$&#123;output&#125;/$&#123;APP_NAME&#125;-$&#123;BUILD_TYPE&#125;.ipa/$&#123;APP_NAME&#125;.ipa echo \"Uploading file to app store, it may make a few minutes, please wait...\" echo \"\\\"$&#123;ALTOOLPATH&#125;\\\" --upload-app -f \\\"$&#123;IPAPATH&#125;\\\" -u \\\"$&#123;appId&#125;\\\" -p \\\"******\\\" --output-format xml\" uploadResult=`\"$&#123;ALTOOLPATH&#125;\" --upload-app -f \"$&#123;IPAPATH&#125;\" -u \"$&#123;appId&#125;\" -p \"$&#123;appPassword&#125;\" -t ios --output-format xml` echo \"uploadResult=$uploadResult\" #\"$&#123;ALTOOLPATH&#125;\" --upload-app -f \"$&#123;IPAPATH&#125;\" -u \"$&#123;appId&#125;\" -p \"$&#123;appPassword&#125;\" --output-format xml | grep \"No errors uploading\" echo \"$uploadResult\" | grep \"No errors uploading\" &gt; /dev/null if [[ $? != 0 ]]; then echo \"Building ios successfully, but uploading to app store failed!\" exit -1 else echo \"Uploading file to app store successfully!\" #Recorded the last version in the history when app was uploaded to google play successfully. #echo \"CURRENT_PROJECT_VERSION = $&#123;currAppVersionName&#125;\" &gt; $&#123;ios_history_list&#125; echo \"MARKETING_VERSION = $&#123;currMarketingVersion&#125;\" &gt; $&#123;ios_history_list&#125; fi fi else xcodebuild -exportArchive -archivePath \"$&#123;output&#125;/$&#123;APP_NAME&#125;-$&#123;BUILD_TYPE&#125;.xcarchive\" \\ -exportPath \"$&#123;output&#125;/$&#123;APP_NAME&#125;-$&#123;BUILD_TYPE&#125;.ipa\" \\ -exportOptionsPlist $DEV_PLIST \\ -allowProvisioningUpdates YES &gt; /dev/null fi fi if [[ $? == 0 ]]; then currPwd=`pwd` cd $&#123;WORKSPACE&#125;/ios/bundle zip -rq $&#123;APP_SAVE_PATH&#125;/IOS.bundle.zip * cd $currPwd ipaFile=`ls $output|grep ipa` # IPA only exist when non-hotfix if [[ $? == 0 &amp;&amp; -d $output/$ipaFile ]]; then filePath=$output/$ipaFile/$&#123;APP_NAME&#125;.ipa ipaFile=$&#123;APP_NAME&#125;-$&#123;RELEASE_NAME&#125;.ipa cp -a $filePath $APP_SAVE_PATH/$&#123;ipaFile&#125; iosIpaUrl=$&#123;APP_URL&#125;/$&#123;ipaFile&#125; pngUrl=$&#123;APP_BASE_URL&#125;/xpay-merchant-app.png md5Size=`ls -l $filePath | awk '&#123;print $5&#125;'` cat $PLIST_TEMPLATE | sed \"s;#&#123;downloadFile&#125;;$&#123;iosIpaUrl&#125;;g\" | sed \"s;#&#123;md5Size&#125;;$&#123;md5Size&#125;;g\"| sed \"s;#&#123;pngFile&#125;;$&#123;pngUrl&#125;;g\" | sed \"s;#&#123;releaseName&#125;;$RELEASE_NAME;g\" &gt; $APP_SAVE_PATH/$&#123;APP_NAME&#125;.plist appJenkinsUrl=\"itms-services://?action=download-manifest&amp;url=$&#123;APP_URL&#125;/$&#123;APP_NAME&#125;.plist\" uploadApp $BUILD_PLATFORM $filePath $appJenkinsUrl $iosIpaUrl if [[ $? != 0 ]]; then echo \"Building ios is successful, but upload failed, maybe caused by establishing network, it exited, please try again.\" exit -1 fi fi else echo \"Building ios is failed, it exited.\" exit -1 fi&#125;function proxy_off() &#123; unset http_proxy unset https_proxy echo -e \"The proxy has been closed!\"&#125;function proxy_on() &#123; export no_proxy=\"127.0.0.1,localhost,*.abc.net,192.168.100.88,10.0.0.0/8,192.168.0.0/16,172.16.0.0/12\" export http_proxy=\"http://$&#123;PROXY_HOST&#125;:$&#123;PROXY_PORT&#125;\" export https_proxy=$http_proxy echo -e \"The proxy has been opened!\"&#125;#Enabled proxy# proxy_on#Init the projectcd $&#123;WORKSPACE&#125;/init./update.sh#SSL Pinningcd $&#123;WORKSPACE&#125;if [[ -f pinset.json ]]; then echo \"npx pinset gen -f\" npx pinset gen -f if [[ $? != 0 ]]; then echo \"npx pinset gen failure!\" exit -1 fifi#Replacing Environment variablesif [[ -d $&#123;EVN_FOLDER&#125; ]]; then echo \"Destination path '$&#123;EVN_FOLDER&#125;' already exists, delete first...\" rm -fr $&#123;EVN_FOLDER&#125;figit clone http://gitlab.xxx.net/x-pay/k8s-$&#123;ENV&#125;-config/ $&#123;EVN_FOLDER&#125;if [[ ! -d $&#123;EVN_FOLDER&#125;/$&#123;K8S_FILE_NAME&#125; ]]; then echo \"Replacing Environment variables failed: Destination path '$&#123;EVN_FOLDER&#125;/$&#123;K8S_FILE_NAME&#125;' doesn't exist.\" exit -1fi#Starting buildcd $&#123;WORKSPACE&#125;# sed -i \"\" \"s;^ *export const ENV *= *'.*';export const ENV = '$&#123;ENV&#125;';g\" $&#123;WORKSPACE&#125;/component/Common/Environment.js/bin/cp -a $&#123;EVN_FOLDER&#125;/$&#123;K8S_FILE_NAME&#125;/Constants.js $&#123;WORKSPACE&#125;/src/if [[ \"$PLATFORM\" == \"android\" ]]; then /bin/cp -a $&#123;EVN_FOLDER&#125;/$&#123;K8S_FILE_NAME&#125;/google-services.json $&#123;WORKSPACE&#125;/android/app/ buildAndroidelif [[ \"$PLATFORM\" == \"ios\" ]]; then# /bin/cp -a $&#123;EVN_FOLDER&#125;/$&#123;K8S_FILE_NAME&#125;/GoogleService-Info.plist $&#123;WORKSPACE&#125;/ios/$&#123;APP_NAME&#125;/ buildIoselif [[ \"$PLATFORM\" == \"both\" ]]; then /bin/cp -a $&#123;EVN_FOLDER&#125;/$&#123;K8S_FILE_NAME&#125;/google-services.json $&#123;WORKSPACE&#125;/android/app/ buildAndroid# /bin/cp -a $&#123;EVN_FOLDER&#125;/$&#123;K8S_FILE_NAME&#125;/GoogleService-Info.plist $&#123;WORKSPACE&#125;/ios/$&#123;APP_NAME&#125;/ buildIosfiif [[ \"$TAG_DESC\" == \"\" ]]; then TAG_DESC=\"For prod version $&#123;newTag&#125; based on $releaseBranch via jenkins\"fi#Disabled proxyproxy_off#Tagging for the prod git release versionif [[ \"$ENV\" == \"prod\" ]]; then echo \"Tagging the release version for prod...\" releaseBranch=$&#123;GIT_BRANCH//origin\\//&#125; #releaseBranch=$&#123;GIT_BRANCH//refs\\/heads\\//&#125; newTag=$&#123;releaseBranch&#125;-`date +%Y%m%d%H%M` git tag -a $newTag -m \"$&#123;TAG_DESC&#125;\" echo \"Pushing the release version to the origin...\" git push origin $&#123;newTag&#125;fi 自动清理过期文件: vim cleanHistoryFiles.sh 12345678910#!/bin/bashdays=90#Cleaning the files before days of the works directoryfind /Users/jenkins/works/xpayapp-* -type f -mtime +$&#123;days&#125; -exec rm -rf &#123;&#125; \\;find /Users/jenkins/works/xwallet-* -type f -mtime +$&#123;days&#125; -exec rm -rf &#123;&#125; \\;find /Users/jenkins/works/xpaymerchant-* -type f -mtime +$&#123;days&#125; -exec rm -rf &#123;&#125; \\;#cleaning the files before days of the app_host directory#find /opt/app-host/shared/public/uploads/pkg -type f -mtime +$&#123;days&#125; -exec rm -rf &#123;&#125; \\;echo \"Cleaning files is done! 参考 https://github.com/pluosi/app-host https://github.com/rock-app/fabu.love http://zhangzr.cn/2018/07/27/iOS%E5%BC%80%E5%8F%91-%E8%87%AA%E5%8A%A8%E6%89%93%E5%8C%85%E5%88%9D%E6%AD%A5%E6%8E%A2%E7%A9%B6/ https://www.jianshu.com/p/38b2e17ced73 https://juejin.im/post/5b6a542b5188251a9e171bf2 https://blog.csdn.net/li530893850/article/details/70889763 https://blog.csdn.net/zrina1314/article/details/80102199 https://github.com/eventOneHQ/apkup https://stackoverflow.com/questions/14665518/api-to-automatically-upload-apk-to-google-play https://developers.google.com/android-publisher/getting_started","categories":[],"tags":[{"name":"app","slug":"app","permalink":"http://blog.gcalls.cn/tags/app/"}]},{"title":"React基础","slug":"React基础","date":"2019-05-08T03:11:44.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2019/05/React基础.html","link":"","permalink":"http://blog.gcalls.cn/2019/05/React基础.html","excerpt":"记录一下React基础相关的知识点。","text":"记录一下React基础相关的知识点。 ES6Promise123456789101112131415161718192021222324test = async () =&gt; &#123; // return 'xxx' // in then // in then, add async necessary // return new Error('error!') // in catch, add async necessary // throw new Error('error!') // in then, add async unnecessary // return Promise.resolve('xxx') // in catch, add async unnecessary // return Promise.reject(new Error('error!')) // add async unnecessary /* return new Promise((resolve, reject) =&gt; &#123; // in then // resolve('yyy') // in catch // throw new Error('error!') // in catch reject(new Error('error!')) &#125;) */&#125; async12345678910111213141516171819202122232425262728293031323334353637383940414243async fetch() &#123; this.props.loadList() this.fetchPerson('Billy') // .then(this.fetchOrders) .then(person =&gt; this.fetchOrders(person)) .then(orders =&gt; &#123; orders.forEach(order =&gt; &#123; console.log(order) &#125;) &#125;) .catch(console.error) // let person = await this.fetchPerson('Billy') // let orders = await this.fetchOrders(person) // orders.forEach(order =&gt; &#123; // console.log(order) // &#125;)&#125;// callthis.test().then( x =&gt; &#123; alert('x-&gt;' + x) &#125;/* , err =&gt; &#123; alert('e-&gt;' + err) &#125; */ // catch与then中的第二个参数效果一样 ).catch(err =&gt; &#123; alert('e-&gt;' + err) &#125;))async fetchOrders(person) &#123; const orders = person.orderIds.map(id =&gt; (&#123; id &#125;)) return orders&#125;async fetchPerson(name) &#123; return &#123; name, orderIds: ['A', 'B'] &#125;&#125; React生命周期 初始化（无子组件）: 1constructor-&gt;componentWillMount-&gt;render-&gt;componentDidMount 初始化（有子组件）: 123父组件：constructor-&gt;componentWillMount-&gt;render子组件：-&gt;constructor-&gt;componentWillMount-&gt;render-&gt;componentDidMount父组件：-&gt;componentDidMount state改变时(无子组件): 1shouldComponentUpdate-&gt;componentWillUpdate-&gt;render-&gt;componentDidUpdate state改变时(有子组件): 123父组件：shouldComponentUpdate-&gt;componentWillUpdate-&gt;render子组件：-&gt;componentWillReceiveProps-&gt;shouldComponentUpdate-&gt;componentWillUpdate-&gt;render-&gt;componentDidUpdate父组件：-&gt;componentDidUpdate 离开页面时: 1componentWillUnmount 如果父组件的state改变时，所有子组件或者子组件在render()中的prop值都会改变： 1子组件render()方法：&lt;MyTextArea defaultValue=&#123;this.props.test&#125; /&gt; 如果将子组件的props值定义为state的话，则需要在componentWillReceiveProps设置state: 123456789101112constructor(props) &#123; super(props) this.state = &#123; test: props.test &#125;&#125;componentWillReceiveProps = nextProps =&gt; &#123; this.setState(&#123; test: nextProps.test &#125;)&#125;子组件render()方法：&lt;MyTextArea defaultValue=&#123;this.state.test&#125; /&gt; 如何避免这些不必要的render： 使用shouldComponentUpdate()以让React知道当前状态或属性的改变是否不影响组件的输出，默认返回ture，返回false时不会重写render，而且该方法并不会在初始化渲染或当使用forceUpdate()时被调用，我们要做的只是这样: 123shouldComponentUpdate(nextProps, nextState) &#123; return nextState.someData !== this.state.someData&#125; react路由Switch12345&lt;Switch&gt; &lt;Route path=&quot;/demo/list&quot; component=&#123;List&#125; exact /&gt; &lt;Route path=&quot;/demo/detail/:id&quot; component=&#123;Detail&#125; /&gt; &lt;Redirect to=&quot;/demo/list&quot; /&gt;&lt;/Switch&gt; Switch表示有多少Route时，只加载找到的第一条Route。 exact表示精确匹配。没有加exact模糊匹配。 子组件配置路由123456pages/home/index.js(第一次会加载这个页面):&lt;Switch&gt; &lt;Route path=&quot;/&quot; component=&#123;MainPage&#125; exact /&gt; &lt;Route path=&quot;/demo&quot; component=&#123;Demo&#125; /&gt; &lt;Route path=&quot;*&quot; component=&#123;NotFoundPage&#125; /&gt;&lt;/Switch&gt; 注意：不能加exact，否则子组件中的路由无法加载。 123456/demo/index.js:&lt;Switch&gt; &lt;Route path=&quot;/demo/list&quot; component=&#123;List&#125; /&gt; &lt;Route path=&quot;/demo/detail/:id&quot; component=&#123;Detail&#125; /&gt; &lt;Redirect to=&quot;/demo/list&quot; /&gt;&lt;/Switch&gt; react-route必须要在系统第一次加载路由。通过点击进来的页面如果配置的路由在第一次没有加载，就报错。/demo在第一次就载了，所以子组件配置/demo开头的路由可以。但如果子组的路由不是以/demo的就有问题。估计是react router的限制。 /demo必须要先定义在router.js中，各个组件中可以定义自己的路由，不过前提必须要是/demo开头才行。 所以home下的Index.js中的这行不能加exact，否则路由就加载不了。 /demo/list模糊匹配到了/demo，然后/demo中配置了子组件的路由，就能够成功加载。 参考 https://dvajs.com/knowledgemap/#javascript-%E8%AF%AD%E8%A8%80 https://es6.ruanyifeng.com/ https://ithelp.ithome.com.tw/articles/10201276 https://ithelp.ithome.com.tw/articles/10201420?sc=iThelpR https://segmentfault.com/a/1190000016494335","categories":[{"name":"react","slug":"react","permalink":"http://blog.gcalls.cn/categories/react/"}],"tags":[{"name":"react","slug":"react","permalink":"http://blog.gcalls.cn/tags/react/"}]},{"title":"Git如何永久删除文件与历史记录","slug":"Git如何永久删除文件与历史记录","date":"2019-05-05T01:36:43.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2019/05/Git如何永久删除文件与历史记录.html","link":"","permalink":"http://blog.gcalls.cn/2019/05/Git如何永久删除文件与历史记录.html","excerpt":"记录一下Git怎么永久删除文件(包括该文件的历史记录)。","text":"记录一下Git怎么永久删除文件(包括该文件的历史记录)。 bfg-repo-cleaner最快速方便的操作可以使用bfg-repo-cleaner工具操作，具体操作如下： 123456789wget https://repo1.maven.org/maven2/com/madgag/bfg/1.13.0/bfg-1.13.0.jar#清理大于10M以上的大文件java -jar bfg-1.13.0.jar --strip-blobs-bigger-than 10M some-big-repo.git#清理和回收空间cd some-big-repo.gitgit reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive#推送修改后的repogit push --force --allgit push --force --tags bfg-repo-cleaner不会删除真实存在的文件。如果想手动执行可以参考以下的方式。 查看哪些历史提交过文件占用空间较大使用以下命令可以查看占用空间最多的五个文件： 1git rev-list --objects --all | grep \"$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -5 | awk '&#123;print$1&#125;')\" rev-list命令用来列出Git仓库中的提交，我们用它来列出所有提交中涉及的文件名及其ID。 该命令可以指定只显示某个引用（或分支）的上下游的提交。 123--objects：列出该提交涉及的所有文件ID。--all：所有分支的提交，相当于指定了位于/refs下的所有引用。verify-pack命令用于显示已打包的内容。 重写commit，删除大文件使用以下命令，删除历史提交过的大文件： 1git filter-branch --force --index-filter 'git rm -rf --cached --ignore-unmatch big-file.zip' --prune-empty --tag-name-filter cat -- --all 上面脚本中的big-file.zip请换成你第一步查出的大文件名，或者这里直接写一个目录。操作后注意看一下是否有删除真实存在的文件。 123filter-branch命令可以用来重写Git仓库中的提交--index-filter参数用来指定一条Bash命令，然后Git会检出（checkout）所有的提交， 执行该命令，然后重新提交。–all参数表示我们需要重写所有分支（或引用）。 在重写提交的过程中，会有以下日志输出: 12Rewrite 6cdbb293d453ced07e6a07e0aa6e580e6a5538f4 (266/266)# Ref 'refs/heads/master' was rewritten 如果显示 xxxxx unchanged, 说明repo里没有找到该文件, 请检查路径和文件名是否正确，重复上面的脚本，把所有你想删除的文件都删掉。 清理和回收空间虽然上面我们已经删除了文件, 但是我们的repo里面仍然保留了这些objects, 等待垃圾回收(GC), 所以我们要用命令彻底清除它, 并收回空间，命令如下: 123456cd 仓库目录#rm -rf .git/refs/original/#git reflog expire --expire=now --all#git gc --prune=now#git gc --aggressive --prune=nowgit reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive 推送修改后的repo以强制覆盖的方式推送你的repo, 命令如下: 123#git push origin master --forcegit push --force --allgit push --force --tags 参考 https://rtyley.github.io/bfg-repo-cleaner/ https://www.hollischuang.com/archives/1708 http://www.cnblogs.com/shines77/p/3460274.html","categories":[{"name":"git","slug":"git","permalink":"http://blog.gcalls.cn/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://blog.gcalls.cn/tags/git/"}]},{"title":"vscode的使用与插件开发","slug":"vscode的使用与插件开发","date":"2019-04-28T07:06:57.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2019/04/vscode的使用与插件开发.html","link":"","permalink":"http://blog.gcalls.cn/2019/04/vscode的使用与插件开发.html","excerpt":"Visual Studio Code（VS code）是开发神器，通过插件配置不仅可以开发前端，还可以开发后端(java/go等)，下面介绍一下vscode的常用插件与插件如何开发一个自己的插件。","text":"Visual Studio Code（VS code）是开发神器，通过插件配置不仅可以开发前端，还可以开发后端(java/go等)，下面介绍一下vscode的常用插件与插件如何开发一个自己的插件。 git-bash12345678#https://code.visualstudio.com/docs/editor/integrated-terminal \"terminal.integrated.profiles.windows\": &#123; \"Cygwin\": &#123; \"path\": \"D:\\\\Developer\\\\Git\\\\bin\\\\bash.exe\", \"args\": [\"--login\"] &#125; &#125;, \"terminal.integrated.defaultProfile.windows\": \"Cygwin\", nodejs1234567#curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.34.0/install.sh | bashcurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash. ~/.bashrc#显示有远端的版本nvm ls-remote#安装对应的版本nvm install 12.22.7 安装常用工具： 1234567891011npm install hexo-cli -gnpm install hexo-server -gnpm install hexo-deployer-git -gnpm install yarn -gnpm install http-server -g#yarn global add servenpm config set registry https://registry.npmmirror.com --globalnpm config set disturl https://npmmirror.com/dist --globalyarn config set registry https://registry.npmmirror.com --globalyarn config set disturl https://npmmirror.com/dist --global 常用插件安装以下插件：1234567891011121314151617181920212223242526272829303132333435#javascripteslintColor PickernpmDebugger for ChromeEclipse Keymap#reactES7 React/Redux/GraphQL/React-Native snippets#vueVeturVue VSCode Snippets#gitGitLenszerofinance-git#其他公共插件Local HistoryXML ToolsPrettier#https://zhuanlan.zhihu.com/p/54031899koroFileHeaderAutoFileNameImport Cost#javaJava Extension PackSpring Boot Extension PackJava Code GeneratorsTomcat for JavaDocker#android/ios pluginAndroid iOS EmulatorReact Native Tools#see debug:https://github.com/Microsoft/vscode-react-native/blob/master/doc/debugging.md#debugging-on-ios-device#Install ios-deploy: npm install -g ios-deploy 可以在项目的.vscode目录下添加extensions.json，并添加以下内容，当第一次打开时会提示是否需要安装这些推荐的插件： 123456789101112131415161718&#123; \"recommendations\": [ \"dbaeumer.vscode-eslint\", \"anseki.vscode-color\", \"eg2.vscode-npm-script\", \"msjsdiag.debugger-for-chrome\", \"alphabotsec.vscode-eclipse-keybindings\", \"dsznajder.es7-react-js-snippets\", \"eamodio.gitlens\", \"zerofinance.zerofinance-git\", \"xyz.local-history\", \"DotJoshJohnson.xml\", \"esbenp.prettier-vscode\", \"OBKoro1.korofileheader\", \"JerryHong.autofilename\", \"wix.vscode-import-cost\" ]&#125; 可以通过命令行批量安装： 123456789101112131415161718192021222324252627282930313233343536373839404142434445#前端------code --install-extension dbaeumer.vscode-eslintcode --install-extension anseki.vscode-colorcode --install-extension eg2.vscode-npm-scriptcode --install-extension msjsdiag.debugger-for-chromecode --install-extension alphabotsec.vscode-eclipse-keybindings#reactcode --install-extension dsznajder.es7-react-js-snippets#vuecode --install-extension octref.veturcode --install-extension sdras.vue-vscode-snippets#gitcode --install-extension eamodio.gitlenscode --install-extension zerofinance.zerofinance-git#otherscode --install-extension xyz.local-historycode --install-extension DotJoshJohnson.xmlcode --install-extension esbenp.prettier-vscodecode --install-extension OBKoro1.korofileheadercode --install-extension JerryHong.autofilenamecode --install-extension wix.vscode-import-cost#前端------#java------code --install-extension vscjava.vscode-java-packcode --install-extension Pivotal.vscode-boot-dev-packcode --install-extension sohibe.java-generate-setters-getters#java------#docker------code --install-extension PeterJausovec.vscode-docker#docker------#android/ios plugin------code --install-extension DiemasMichiels.emulatecode --install-extension vsmobile.vscode-react-native#android/ios plugin------#其它命令说明，不需要执行#列出已经安装的插件code --list-extensions#安装某个插件code --install-extension ms-vscode.cpptools#卸载某个插件code --uninstall-extension ms-vscode.csharp 代码注释koroFileHeader添加注释，在全局的settings.json中添加： 参考： https://code.visualstudio.com/docs/editor/emmet https://www.cnblogs.com/summit7ca/p/6944215.html 12345678910111213141516171819202122232425262728\"editor.fontSize\": 14,\"terminal.integrated.fontSize\": 14,\"emmet.triggerExpansionOnTab\": true,\"emmet.includeLanguages\": &#123; \"javascript\": \"javascriptreact\", \"vue-html\": \"html\", \"razor\": \"html\", \"plaintext\": \"jade\"&#125;,// 注释\"fileheader.configObj\": &#123; // 将该选项设置为true即可开启 \"autoAdd\": false&#125;,// 头部注释\"fileheader.customMade\": &#123; \"Author\": \"dave.zhao\", \"Date\": \"Do not edit\", \"LastEditors\": \"dave.zhao\", \"LastEditTime\": \"Do not edit\", \"Description\": \"\"&#125;,// 函数注释\"fileheader.cursorMode\": &#123; \"Date\": \"Do not edit\", \"description\": \"\", \"param\": \"\"&#125; 注意：Author和LastEditors填写自己的名字 文件头注释快捷键：window：ctrl+alt+i,mac：ctrl+cmd+i 函数注释快捷键：window：ctrl+alt+t,mac：ctrl+cmd+t 常用配置可以放在全局的settings.json中，也可以放在各个项目的settings.json中： 1234567891011121314151617181920&#123; \"eslint.validate\": [\"javascript\", \"javascriptreact\"], \"javascript.updateImportsOnFileMove.enabled\": \"always\", // 代码缩进修改成4个空格 \"editor.detectIndentation\": false, \"editor.tabSize\": 4, \"editor.formatOnSave\": true, // 每次保存的时候将代码按eslint格式进行修复 \"eslint.autoFixOnSave\": true, // 让prettier使用eslint的代码格式进行校验 \"prettier.eslintIntegration\": true, // 去掉代码结尾的分号 \"prettier.semi\": false, // 使用带引号替代双引号 \"prettier.singleQuote\": true, \"prettier.tabWidth\": 4, \"prettier.printWidth\": 250, // 让函数(名)和后面的括号之间加个空格 \"javascript.format.insertSpaceBeforeFunctionParenthesis\": true&#125; Javahttps://code.visualstudio.com/docs/languages/javahttps://code.visualstudio.com/docs/remote/wslhttps://code.visualstudio.com/docs/java/java-tutorialhttps://code.visualstudio.com/docs/java/java-projecthttps://code.visualstudio.com/docs/java/java-debugginghttps://code.visualstudio.com/docs/java/extensionshttps://code.visualstudio.com/docs/java/java-spring-boot 插件： 1234567https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-packhttps://marketplace.visualstudio.com/items?itemName=Pivotal.vscode-boot-dev-packhttps://marketplace.visualstudio.com/items?itemName=adashen.vscode-tomcat#wget http://mirrors.aliyun.com/apache/tomcat/tomcat-8/v8.5.61/bin/apache-tomcat-8.5.61-windows-x64.zip#https://tomcat.apache.org/tomcat-8.5-doc/appdev/sample/#WSLhttps://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack Settings.json terminal.integrated.shell.windows设置为git-bash的话不能启动java项目。 123456789101112131415161718192021222324252627282930&#123; \"editor.fontSize\": 16, # \"terminal.integrated.shell.windows\": \"D:\\\\Developer\\\\Git\\\\bin\\\\bash.exe\", \"git.autofetch\": true, \"terminal.integrated.fontSize\": 16, \"java.home\": \"D:\\\\Developer\\\\java\\\\jdk-11.0.9\", \"java.configuration.runtimes\": [ &#123; \"name\": \"JavaSE-1.8\", \"path\": \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_271\", \"default\": true &#125;, &#123; \"name\": \"JavaSE-11\", \"path\": \"D:\\\\Developer\\\\java\\\\jdk-11.0.9\" &#125; ], \"files.exclude\": &#123; \"**/.classpath\": true, \"**/.project\": true, \"**/.settings\": true, \"**/.factorypath\": true &#125;, \"maven.executable.path\": \"D:\\\\Developer\\\\apache-maven-3.3.9\\\\bin\\\\mvn.cmd\", \"java.configuration.maven.globalSettings\": \"D:\\\\Developer\\\\apache-maven-3.3.9\\\\conf\\\\settings.xml\", \"java.configuration.maven.userSettings\": \"D:\\\\Developer\\\\apache-maven-3.3.9\\\\conf\\\\settings.xml\", \"editor.suggestSelection\": \"first\", \"vsintellicode.modify.editor.suggestSelection\": \"automaticallyOverrodeDefaultValue\", \"java.project.importOnFirstTimeStartup\": \"automatic\"&#125; WSL for Settings.json 12345678910111213141516171819202122232425262728&#123; \"editor.fontSize\": 16, \"git.autofetch\": true, \"terminal.integrated.fontSize\": 16, \"java.home\": \"/Developer/java/jdk-11.0.9\", \"java.configuration.runtimes\": [ &#123; \"name\": \"JavaSE-1.8\", \"path\": \"/Developer/java/jdk1.8.0_271\", \"default\": true &#125;, &#123; \"name\": \"JavaSE-11\", \"path\": \"/Developer/java/jdk-11.0.9\" &#125; ], \"files.exclude\": &#123; \"**/.classpath\": true, \"**/.project\": true, \"**/.settings\": true, \"**/.factorypath\": true &#125;, \"maven.executable.path\": \"/Developer/apache-maven-3.3.9/bin/mvn\", \"java.configuration.maven.globalSettings\": \"/Developer/apache-maven-3.3.9/conf/settings.xml\", \"java.configuration.maven.userSettings\": \"/Developer/apache-maven-3.3.9/conf/settings.xml\", \"editor.suggestSelection\": \"first\", \"vsintellicode.modify.editor.suggestSelection\": \"automaticallyOverrodeDefaultValue\"&#125; Remote Debug: 1234567891011121314151617181920212223#launch.json&#123; \"configurations\": [ // &#123; // \"type\": \"java\", // \"name\": \"Spring Boot-TelepresenceK8sApplication&lt;telepresence-k8s&gt;\", // \"request\": \"launch\", // \"cwd\": \"$&#123;workspaceFolder&#125;\", // \"console\": \"internalConsole\", // \"mainClass\": \"com.ctl.telepresencek8s.TelepresenceK8sApplication\", // \"projectName\": \"telepresence-k8s\", // \"args\": \"\" // &#125;, &#123; \"type\": \"java\", \"name\": \"Debug (Attach)-Spring Boot-TelepresenceK8sApplication&lt;telepresence-k8s&gt;\", \"request\": \"attach\", \"projectName\": \"telepresence-k8s\", \"hostName\": \"localhost\", \"port\": 8000 &#125; ]&#125; Developing inside a Container参考： https://code.visualstudio.com/docs/remote/containers https://code.visualstudio.com/docs/remote/create-dev-container https://code.visualstudio.com/docs/remote/containers-advanced https://github.com/microsoft/vscode-dev-containers 首先要安装vscode-remote-extensionpack插件： https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack .devcontainer/devcontainer.json 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// For format details, see https://aka.ms/vscode-remote/devcontainer.json or this file's README at:// https://github.com/microsoft/vscode-dev-containers/tree/v0.137.0/containers/java&#123; \"name\": \"employee\", \"build\": &#123; \"dockerfile\": \"Dockerfile\", \"args\": &#123; \"VARIANT\": \"8\" &#125; &#125;, // Set *default* container specific settings.json values on container create. \"settings\": &#123; \"terminal.integrated.shell.linux\": \"/bin/bash\", \"java.home\": \"/Developer/java/jdk-11.0.9\", \"maven.executable.path\": \"/Developer/apache-maven-3.3.9/bin/mvn\", \"java.configuration.maven.globalSettings\": \"/Developer/apache-maven-3.3.9/conf/settings.xml\", \"java.configuration.maven.userSettings\": \"/Developer/apache-maven-3.3.9/conf/settings.xml\", \"java.configuration.runtimes\": [ &#123; \"name\": \"JavaSE-1.8\", \"path\": \"/Developer/java/jdk1.8.0_241\", \"default\": true &#125;, &#123; \"name\": \"JavaSE-11\", \"path\": \"/Developer/java/jdk-11.0.9\" &#125; ] &#125;, // Add the IDs of extensions you want installed when the container is created. \"extensions\": [ \"vscjava.vscode-java-pack\" ], #If you do have login access, you can use a remote filesystem bind mount instead: \"workspaceMount\": \"source=$&#123;localWorkspaceFolder&#125;,target=/Developer/workspace/employee,type=bind,consistency=cached\", \"workspaceFolder\": \"/Developer/workspace/employee\", \"mounts\": [ \"source=F:\\\\docker,target=/Developer/docker,type=bind,consistency=cached\", // \"source=try-node-node_modules,target=$&#123;containerWorkspaceFolder&#125;/node_modules,type=volume\", \"source=F:\\\\apache-tomcat-7.0.50,target=/Developer/apache-tomcat-7.0.50,type=bind,consistency=cached\", \"source=C:\\\\Users\\\\Dave.zhao\\\\.kube,target=/root/.kube,type=bind,consistency=cached\" ], \"remoteEnv\": &#123; \"KUBERNETES_NAMESPACE\": \"default\" &#125;, // Use 'forwardPorts' to make a list of ports inside the container available locally. // \"forwardPorts\": [], // Use 'postCreateCommand' to run commands after the container is created. // \"postCreateCommand\": \". /etc/profile &amp;&amp; java -version\", // Uncomment to connect as a non-root user. See https://aka.ms/vscode-remote/containers/non-root. \"remoteUser\": \"dev\"&#125; .devcontainer/Dockerfile 12345678910111213141516171819202122232425262728# FROM java:8-jdkFROM centos:7RUN mkdir /Developer /configWORKDIR /DeveloperENV APPNAME=employee \\ VERSION=3.0.0-SNAPSHOT \\ CONFIG=/config/# RUN ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo Asia/Shanghai &gt; /etc/timezoneRUN yum install -y wget sudo# RUN groupadd dev &amp;&amp; useradd -r -g dev devARG USERNAME=devARG USER_UID=1000ARG USER_GID=$USER_UID# Create the user# [Optional] Add sudo support. Omit if you don't need to install software after connecting.RUN groupadd --gid $USER_GID $USERNAME \\ &amp;&amp; useradd --uid $USER_UID --gid $USER_GID -m $USERNAME \\ &amp;&amp; echo $USERNAME ALL=\\(ALL\\) NOPASSWD:ALL &gt; /etc/sudoers.d/$USERNAME \\ &amp;&amp; chmod 0440 /etc/sudoers.d/$USERNAMERUN chown -R $USERNAME.$USERNAME /Developer /configADD apache-maven-3.3.9.tar.gz /Developer/RUN mkdir -p /Developer/java /Developer/workspaceADD java/* /Developer/java/ADD script.sh /Developer/RUN bash /Developer/script.sh script.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#!/bin/bash#关闭内核安全(如果是vagrant方式，第一次完成后需要重启vagrant才能生效。)sed -i 's;SELINUX=.*;SELINUX=disabled;' /etc/selinux/configsetenforce 0getenforcecat /etc/NetworkManager/NetworkManager.conf|grep \"dns=none\" &gt; /dev/nullif [[ $? != 0 ]]; then echo \"dns=none\" &gt;&gt; /etc/NetworkManager/NetworkManager.conf systemctl restart NetworkManager.servicefiln -sf /usr/share/zoneinfo/Asia/Chongqing /etc/localtime#logined limitcat /etc/security/limits.conf|grep 100000 &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF* soft nofile 100000* hard nofile 100000* soft nproc 100000* hard nproc 100000EOFfised -i 's;4096;100000;g' /etc/security/limits.d/20-nproc.conf#systemd service limitcat /etc/systemd/system.conf|egrep '^DefaultLimitCORE' &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/systemd/system.conf &lt;&lt; EOFDefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000EOFfi#echo \"vm.swappiness = 10\" &gt;&gt; /etc/sysctl.confcat /etc/sysctl.conf|grep \"net.ipv4.ip_local_port_range\" &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 300net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.ip_local_port_range = 1024 65535net.ipv4.ip_forward = 1EOFsysctl -pfisu - root -c \"ulimit -a\"# yum -y install gcc kernel-develmv -f /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup#wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repowget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.reposudo mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backupsudo mv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backupyum -y install bind-utils bridge-utils ntpdate setuptool iptables \\ system-config-securitylevel-tui system-config-network-tui \\ ntsysv net-tools lrzsz telnet lsof vim dos2unix unix2dos zip unzipcat &gt;&gt; /etc/profile &lt;&lt; EOFexport JAVA_HOME=/Developer/java/jdk1.8.0_241export M2_HOME=/Developer/apache-maven-3.3.9export GRADLE_USER_HOME=/Developer/.gradleexport PATH=\\$JAVA_HOME/bin:\\$M2_HOME/bin:\\$PATHEOF. /etc/profile# /home/$USERNAME/.bashrc 参考以下文件：devcontainer.zip 或者：https://github.com/zhaoxunyong/java-k8s/tree/1.0.4.RELEASE/spring-k8s-feign/spring-cloud-k8s-account-service/.devcontainer 注意： 如果有变动，需要执行”Remote-Containers: Rebuild Container”。 另外不支持远程docker(2375端口)方式打开本地的目录(因为无法远程mount目录)，但可以通过”Remote-Containers: Clone Repository in Container Volume”的方式或者： https://code.visualstudio.com/docs/remote/containers-advanced#_converting-an-existing-or-predefined-devcontainerjson If you do not have login access to the remote host, use a Docker “volume” for your source code.Update .devcontainer/devcontainer.json as follows (replacing remote-workspace with a unique volume name if desired):workspaceMount”: “source=employee-workspace,target=/Developer/workspace/employee,type=volume”, If you used a volume instead of a bind mount, use Ctrl+Shift+` to open a terminal inside the container. #You can run git clone from here to pull down your source code and use File &gt; Open… / Open Folder… to open the cloned repository. Docker-from-docker.devcontainer/devcontainer.json 1234567\"extensions\": [ \"ms-azuretools.vscode-docker\"],\"mounts\": [ \"source=/var/run/docker.sock,target=/var/run/docker.sock,type=bind\", \"source=$&#123;env:HOME&#125;$&#123;env:USERPROFILE&#125;/.kube,target=/home/dev/.kube,type=bind\"], script.sh 1234567891011#https://www.cnblogs.com/763977251-sg/p/11837130.html#Docker installation#https://aka.ms/vscode-remote/samples/docker-from-dockersudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposudo yum makecache fastsudo yum -y install docker-cetouch /var/run/docker.socksudo gpasswd -a dev dockerchown dev.dev /var/run/docker.sock 插件开发参考代码https://github.com/zhaoxunyong/vs-code-git-plugin，具体细节待补充。 参考 https://segmentfault.com/a/1190000008968904 https://www.cnblogs.com/virde/p/vscode-extension-input-and-output.html https://github.com/steveukx/git-js https://www.jianshu.com/p/2b096d8ad9b8 https://github.com/Microsoft/vscode-extension-samples https://www.jianshu.com/p/520c575e91c3 https://segmentfault.com/a/1190000017279102 https://segmentfault.com/a/1190000014758981 https://www.cnblogs.com/liuxianan/p/vscode-plugin-publish.html https://www.cnblogs.com/virde/p/vscode-extension-input-and-output.html https://www.cnblogs.com/virde/p/vscode-extension-input-and-output.html http://nodejs.cn/api/fs.html#fs_fs_unlinksync_path","categories":[{"name":"vscode","slug":"vscode","permalink":"http://blog.gcalls.cn/categories/vscode/"}],"tags":[{"name":"vscode","slug":"vscode","permalink":"http://blog.gcalls.cn/tags/vscode/"}]},{"title":"黑苹果安装过程","slug":"黑苹果安装过程","date":"2019-04-09T00:40:52.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2019/04/黑苹果安装过程.html","link":"","permalink":"http://blog.gcalls.cn/2019/04/黑苹果安装过程.html","excerpt":"记录一下安装黑苹果的全过程。","text":"记录一下安装黑苹果的全过程。 准备硬件配置Lenovo Xiaoxin Air 13 Pro, 对应的硬件配置如下： CPU 内存 显卡 声卡 Intel Core i7 6500U @ 2.50GHz 8G Intel HD Graphics 520 Intel Corporation Sunrise Point-LP HD Audio 操作系统版本操作系统版本：macOS-Catalina-10.15.4-19E266。 镜像制作下载etcher，打开镜像，选择U盘，点击Flash即可。 微PE工具箱从http://www.wepe.com.cn/download.html中下载对应的PE版本并刻录到U盘，用于备份EFI或者进入不了系统时的急救。 安装安装过程比较简单，详见黑果小兵macOS-Catalina-10.15.4-19E266安装过程中的步骤。 驱动安装安装完成后，用Clover Configurator加载EFI分区，然后复制EIF-&gt;CLOVER下的config_5700.plist为config.plist，将对应的驱动补丁下载, 并将kext目录下的几个文件放到EFI的CLOVER/kexts/other目录下。 声卡对应的inject id为3。 打开hidpi： 参考： https://www.sqlsec.com/2018/09/hidpi.html https://github.com/xzhih/one-key-hidpi 打开终端执行以下命令，按照文档中的说明一步步操作即可： 1sh -c \"$(curl -fsSL https://raw.githubusercontent.com/xzhih/one-key-hidpi/master/hidpi-zh.sh)\" 亮度调节： 参考：https://www.bilibili.com/video/av54305176/ 首先删除以前用过的亮度驱动，如ACPIBacklightInjector，删除ACPI/patched下的SSDT-PNLF.aml，然后打开clover configurator ，ACPI部分与视频一样勾上AddPNLF，然后Device部分勾选SetIntelBacklight和SetIntelMaxBacklight，然后你需要在你的kexts/other中放入较新版本的WhateverGreen.kext，重启生效。 Dell 9020显示驱动参考https://imac.hk/heipingguo-hd4400-hd4600-kext.html Dell 9020的驱动比较简单，不用上面这么麻烦，用Clover Configurator挂载EFI目录，再打开config.plist进行编辑: 12345678910#Graphics:ig-platform-id ---&gt; 0x0D220003或者从右边的下拉中选择0x0a260006#Devices：IntelGFX ---&gt; 0x04128086Audio inject ---&gt; 3#SMBIOS:Product Model ---&gt; iMac15,1#Boot:如果默认只启动MacOS，设置Default Boot Volume为对应的操作系统的Label 台式机打开hidpi时注意选择： 1231:开启hidpi2:iMac3:手动选择：1920x1080 1680x944 1440x810 显示器不能使用VGA，否则不能显示。 Dell 9020对应的EFI下载 打印机12345#https://warwick.ac.uk/fac/soc/wbs/central/issu/help/kb/hardware/printers/kyoceramac-win/wget https://warwick.ac.uk/fac/soc/wbs/central/issu/help/kb/hardware/printers/kyoceramac-win/macphase4.0_2018.01.19-eu.zip#install Kyocera OS X 10.8+ Web build 2018.01.05.dmg#参考上面的网址配置即可。如果是windows smb的话，地址为：smb://192.168.100.105/Kyocera02#驱动选择Kyocera FS-6525MFP 其他vscode可以在命令行直接输入code打开： 12#for macsudo ln -s '/Applications//Visual Studio Code.app/Contents/Resources/app/bin/code' /usr/bin/code 加载硬盘为自定义的目录： 12345678#https://blog.csdn.net/wangrui1573/article/details/82562253#先打开UUIDdiskutil info /dev/disk1s1#sudo diskutil umount /Volumes/Developer#sudo vifs (sudo vim /etc/fstab)#/dev/disk0s4 /data hfs rw,autoUUID=0C54584B-F516-42F8-88A4-4165E9D1E702 /Developer/ apfs rw 1 2#disk Utility会自动mount对应的point为/Develper 环境变量： 1234567891011121314151617181920212223242526alias ll='ls -l'alias code=\"/Applications/Visual\\ Studio\\ Code.app/Contents/Resources/app/bin/code\"export LANG=zh_CN.UTF-8function proxy_off()&#123; unset http_proxy unset https_proxy echo -e \"The proxy has been closed!\"&#125;function proxy_on() &#123; export no_proxy=\"127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net\" export http_proxy=\"http://127.0.0.1:1082\" export https_proxy=$http_proxy echo -e \"The proxy has been opened!\"&#125;export ANDROID_HOME=/Developer/Android/sdkexport PATH=$PATH:$ANDROID_HOME/toolsexport PATH=$PATH:$ANDROID_HOME/tools/binexport PATH=$PATH:$ANDROID_HOME/platform-toolsexport PATH=$PATH:$ANDROID_HOME/emulatorexport JAVA_HOME=$(/usr/libexec/java_home)export M2_HOME=/Developer/apache-maven-3.3.9export GRADLE_USER_HOME=/Developer/.gradleexport PATH=$JAVA_HOME/bin:$M2_HOME/bin:$PATH 下载m3u8： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#!/bin/bash#declare var#Allow how many threads to download.download_max_connection=10#The aria2c's max connectionaria2c_max_connection=16rootUrl='http://hong.tianzhen-zuida.com/20200102/17650_845edc1f/1000k/hls/'m3u8File=$1outputFile=output.mp4filelistFile=filelist.txttempFolder=downTempheader=\"--header='Referer: https://cdnx.stream/player/o2AWUrtSqQuo1Ma/'\"userAgent=\"Mozilla/5.0 (Macintosh\\; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36\"aria2cCmd=\"aria2c -x $aria2c_max_connection -U '$userAgent'\"if [ ! -d \"$tempFolder\" ]; then mkdir -p \"$tempFolder\"ficd $tempFolder#checkout either the finally file existedif [ -f \"$outputFile\" ]; then echo \"$outputFile already existed!\" exit -1fi# echo -n \"\" &gt; $filelistFileif [ -f \"$filelistFile\" ]; then rm -fr $filelistFilefiindex=0playlist=()while read linedo if [[ $&#123;line:0:1&#125; == \"#\" ]]; then continue fi if [[ $&#123;line&#125; != https://* ]]; then line=\"$&#123;rootUrl&#125;$&#123;line&#125;\" fi playlist[$index]=$line index=$((index+1))done &lt; $m3u8Fileindex=0fileindex=0playlistlength=$&#123;#playlist[@]&#125;echo \"playlistlength---&gt;$playlistlength\"while(( index &lt;= playlistlength ))do templength=$download_max_connection newplaylist=($&#123;playlist[@]:$index:$templength&#125;) for url in $&#123;newplaylist[@]&#125; do [[ $url == \"\" ]] &amp;&amp; continue # echo \"url---&gt;$url\" fileName=\"$&#123;fileindex&#125;.mp4\" if [[ ! -f \"$fileName\" || -f \"$&#123;fileName&#125;.aria2\" ]]; then echo \"$&#123;url&#125;\" | sed \"s;^;$aria2cCmd -o \\\"$&#123;fileName&#125;\\\" \\\";g\" \\ | sed \"s;$;\\\";g\" | sh +x &amp; if [[ $? == 0 ]]; then echo \"file '$&#123;fileName&#125;'\" &gt;&gt; $filelistFile fi else echo \"$fileName already existed!\" if [[ $? == 0 ]]; then echo \"file '$&#123;fileName&#125;'\" &gt;&gt; $filelistFile fi fi fileindex=$((fileindex+1)) done wait index=$((index+$templength))doneif [ -f \"$filelistFile\" ]; then ffmpeg -f concat -i $filelistFile -c copy $outputFileficd - 使用annie下载视频： 请参考：https://github.com/iawia002/annie 下载优酷vip视频： 先安装chrome插件： cookies.txt，然后使用以下命令下载： 123456#查看格式：#如果不是vip视频不用加-c参数：annie -i -c youku.txt &quot;url&quot;#下载指定格式视频：annie -f 格式类型 -c youku.txt &quot;url&quot; 参考 https://blog.daliansky.net/Intel-FB-Patcher-tutorial-and-insertion-pose.html https://www.sqlsec.com/2018/09/hidpi.html https://www.bilibili.com/video/av46767597?from=search&amp;seid=18234894269411097533 https://github.com/acidanthera/AppleALC https://bitbucket.org/RehabMan/os-x-acpi-battery-driver/downloads/ https://github.com/fishrong/ASUS-FL5500L-EFI/tree/master/EFI/CLOVER/kexts/Other/ApplePS2SmartTouchPad.kext https://www.jianshu.com/p/955ce6706ae2 https://imac.hk/heipingguo-hd4400-hd4600-kext.html","categories":[],"tags":[{"name":"mac","slug":"mac","permalink":"http://blog.gcalls.cn/tags/mac/"}]},{"title":"ubuntu os","slug":"ubuntu-os","date":"2018-12-03T06:14:00.000Z","updated":"2024-12-17T08:12:34.677Z","comments":true,"path":"/2018/12/ubuntu-os.html","link":"","permalink":"http://blog.gcalls.cn/2018/12/ubuntu-os.html","excerpt":"记录ubuntu的安装过程。","text":"记录ubuntu的安装过程。 安装操作系统https://ubuntu.com/download/desktop 优化apt登录时使用Ubuntu on X11 Org，否则Theme不能切换。因为Gnome Wayland不成熟。 Recommend: Software &amp; Update-&gt;Download from-&gt;Other去选择aliyun镜像。如果手动的话参考以下方式： 123456789101112131415sudo cp /etc/apt/sources.list /etc/apt/sources.list.baksudo vi /etc/apt/sources.list#Adding he following lists#For ubuntu 22.04deb http://mirrors.aliyun.com/ubuntu/ jammy main restricteddeb http://mirrors.aliyun.com/ubuntu/ jammy-updates main restricteddeb http://mirrors.aliyun.com/ubuntu/ jammy universedeb http://mirrors.aliyun.com/ubuntu/ jammy-updates universedeb http://mirrors.aliyun.com/ubuntu/ jammy multiversedeb http://mirrors.aliyun.com/ubuntu/ jammy-updates multiversedeb http://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ jammy-security main restricteddeb http://mirrors.aliyun.com/ubuntu/ jammy-security universedeb http://mirrors.aliyun.com/ubuntu/ jammy-security multiverse Update:12345sudo apt update#aptsudo add-apt-repository ppa:apt/stablesudo apt install apt-fastsudo apt upgrade 安装基础包123456789101112131415161718192021222324252627#sudo apt install software-properties-common#sudo apt install aria2sudo apt install vim unrar gdebi curl screen keepassxc git#sudo apt install vlc#Installing chromewget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.debsudo dpkg -i google-chrome-stable_current_amd64.deb#Installing albert#wget https://download.opensuse.org/repositories/home:/manuelschneid3r/xUbuntu_20.04/amd64/albert_0.17.2-0_amd64.debwget https://download.opensuse.org/repositories/home:/manuelschneid3r/xUbuntu_22.04/amd64/albert_0.17.3-0_amd64.debsudo dpkg -i albert_0.17.3-0_amd64.deb#添加application菜单sudo apt-get install alacarte#serch mainmenu就可以#mount NTFSsudo apt install ntfs-3g#Get UUIDls -l /dev/disk/by-uuidsudo vim /etc/fstabUUID=409AB21C9AB20F02 /data1 ntfs-3g rw 0 0UUID=5E98B57C98B552EF /data2 ntfs-3g rw 0 0 修改操作系统配置完整脚本: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273apt-get update#apt-get install make g++ init inetutils-ping sudo jq iproute2 net-tools wget htop vim screen curl lsof lrzsz zip unzip expect openssh-server -yapt-get install init inetutils-ping sudo iptables psmisc jq iproute2 net-tools wget htop vim screen curl lsof lrzsz zip unzip expect openssh-server -y#LANG=\"en_US.UTF-8\"#sed -i 's;LANG=.*;LANG=\"zh_CN.UTF-8\";' /etc/locale.confsystemctl disable iptablessystemctl stop iptablessystemctl disable firewalldsystemctl stop firewalld#ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimetimedatectl set-timezone Asia/Shanghai#logined limitcat /etc/security/limits.conf|grep \"^root\" &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOFroot - nofile 100000root - nproc 100000* - nofile 100000* - nproc 100000EOFfi#systemd service limitcat /etc/systemd/system.conf|egrep '^DefaultLimitNOFILE' &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/systemd/system.conf &lt;&lt; EOFDefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000EOFfi#user service limitcat /etc/systemd/user.conf|egrep '^DefaultLimitNOFILE' &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/systemd/system.conf &lt;&lt; EOFDefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000EOFficat /etc/sysctl.conf|grep \"net.ipv4.ip_local_port_range\" &gt; /dev/nullif [[ $? != 0 ]]; then cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.tcp_syncookies = 1net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 300net.ipv4.tcp_tw_reuse = 1net.ipv4.ip_local_port_range = 1024 65535net.ipv4.tcp_max_syn_backlog = 8192net.ipv4.tcp_max_tw_buckets = 500net.ipv4.ip_forward = 1fs.inotify.max_user_instances=1280fs.inotify.max_user_watches=655360vm.overcommit_memory=1fs.protected_regular=0EOFsysctl -pfisu - root -c \"ulimit -a\"echo \"140.82.112.4 github.com185.199.110.133 raw.githubusercontent.com\" &gt;&gt; /etc/hostssed -i 's;#PermitRootLogin.*;PermitRootLogin yes;g' /etc/ssh/sshd_configsystemctl enable sshsystemctl restart ssh Safe-RMSafe-RM.zip rm.sh:123456789101112sudo mkdir -p /works/shell /works/backupsudo chown -R dave.dave /works/chmod +x /works/shell/*#https://superuser.com/questions/192725/how-can-i-alias-a-command-for-sudocat &gt;&gt; /etc/profile &lt;&lt;EOFalias sudo='sudo 'alias rm=\"/works/shell/rm.sh\"EOF. /etc/profile GIT123456789git config --global user.name \"dave.zhao\"git config --global user.email dave.zhao@zerofinance.comgit config --global core.autocrlf falsegit config --global core.safecrlf warngit config --global core.filemode falsegit config --global core.whitespace cr-at-eolgit config --global credential.helper store#由于 Windows 版本的 Git 是使用 msys 编译的，它使用了旧版本的 Windows Api，限制文件名不能超过 260 个字符git config --global core.longpaths true 配置环境变量vim ~/.bash_profile1234567891011121314151617181920test -f ~/.profile &amp;&amp; . ~/.profiletest -f ~/.bashrc &amp;&amp; . ~/.bashrcalias ll=&quot;ls -l&quot;alias k=kubectlsource &lt;(kubectl completion bash | sed s/kubectl/k/g)function proxy_off()&#123; unset http_proxy unset https_proxy echo -e &quot;The proxy has been closed!&quot;&#125;function proxy_on() &#123; export no_proxy=&quot;127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net,*.aliyun.com,*.163.com,*.docker-cn.com,registry.gcalls.cn&quot; export http_proxy=&quot;http://127.0.0.1:1082&quot; export https_proxy=$http_proxy echo -e &quot;The proxy has been opened!&quot;&#125;source ~/.bash_profile rc.local1234567891011121314151617181920212223242526272829303132333435363738394041424344#http://www.atmcu.com/2256.html#rc-local.service#cat &gt; /etc/systemd/system/rc-local.service &lt;&lt; EOF#[Unit]#Description=/etc/rc.local Compatibility#ConditionPathExists=/etc/rc.local##[Service]#Type=forking#ExecStart=/etc/rc.local start#TimeoutSec=0#StandardOutput=tty#RemainAfterExit=yes#SysVStartPriority=99##[Install]#WantedBy=multi-user.target#EOF#sudo systemctl enable rc-local#sudo systemctl start rc-local.service#sudo systemctl status rc-local.service#rc.localcat &gt; /etc/rc.local &lt;&lt;EOF#!/bin/sh -e## rc.local## This script is executed at the end of each multiuser runlevel.# Make sure that the script will \"exit 0\" on success or any other# value on error.## In order to enable or disable this script just change the execution# bits.## By default this script does nothing.exit 0EOF#systemctlsudo chmod +x /etc/rc.local 搜狗输入法 for linux1234567891011121314#https://shurufa.sogou.com/linux#https://shurufa.sogou.com/linux/guidesudo apt install fcitx#按照第一步：1、添加中文语言支持 处理，并重启系统#安装wget https://ime.sogouimecdn.com/202208041520/03940f168eb3fa5819e568874dcc6a2f/dl/gzindex/1656597217/sogoupinyin_4.0.1.2800_x86_64.debsudo dpkg -i sogoupinyin_4.0.1.2800_x86_64.deb#重启系统#安装其他sudo cp /usr/share/applications/fcitx.desktop /etc/xdg/autostart/sudo apt purge ibussudo apt install libqt5qml5 libqt5quick5 libqt5quickwidgets5 qml-module-qtquick2sudo apt install libgsettings-qt1#重启电脑 系统快捷键1234567#sudo apt install deepin-screenshot# sudo apt install deepin-terminal#deepin-screenshot -&gt; ctrl+alt+Q#deepin-terminal -&gt; ctrl+alt+Tsudo apt install flameshotxdg-open . -&gt; Win+Eflameshot gui -&gt; ctrl+alt+Q NODEJS1234567#https://github.com/nvm-sh/nvmcurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash. ~/.bashrc#显示有远端的版本nvm ls-remote#安装对应的版本nvm install v12.22.6 安装常用工具：1234567891011npm config set registry https://registry.npmmirror.com --globalnpm config set disturl https://npmmirror.com/dist --globalnpm install hexo-cli -gnpm install hexo-server -gnpm install hexo-deployer-git -gnpm install yarn -gnpm install http-server -gyarn config set registry https://registry.npmmirror.com --globalyarn config set disturl https://npmmirror.com/dist --global#yarn global add serve JAVA12345678910sudo mkdir -p /Developer/java /Developer/workspacesudo chown -R dave.dave /Developersudo vim /etc/profile.d/java.shexport JAVA_HOME=/Developer/java/jdk1.8.0_202#export JAVA_HOME=$(/usr/libexec/java_home)export M2_HOME=/Developer/apache-maven-3.5.4export GRADLE_USER_HOME=/Developer/.gradleexport PATH=$JAVA_HOME/bin:$M2_HOME/bin:$PATHsource /etc/profile gnome-shell安装：1234567891011121314151617181920212223#For 22.04sudo apt install gnome-tweaks chrome-gnome-shell gnome-shell-extension-manager#open with firefox or chromehttps://extensions.gnome.org#Click: Click here to install browser extensionplugin:User Themes: https://extensions.gnome.org/extension/19/user-themes/#dock-from-dash: https://extensions.gnome.org/extension/4703/dock-from-dash/dash2dock-lite: https://extensions.gnome.org/extension/4994/dash2dock-lite/dash-to-panel: https://extensions.gnome.org/extension/1160/dash-to-panel/arc menu: https://extensions.gnome.org/extension/3628/arcmenu/Gnome Wayland不成熟，登录时使用Ubuntu on X11 Org，否则Theme不能切换。##Disable wayland on Ubuntu 22.04 Desktop##https://linuxconfig.org/how-to-enable-disable-wayland-on-ubuntu-22-04-desktop#echo $XDG_SESSION_TYPE#sudo nano /etc/gdm3/custom.conf##Within this file, look for the line that says #WaylandEnable=false. You can uncomment this line and either set it to true or #false, depending on whether you want Wayland enabled or not.#WaylandEnable=false##Restart#sudo systemctl restart gdm3 Wine企业微信需要用wine7安装才会有消息提醒，推荐使用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#https://www.cnblogs.com/hyaline-doc/p/18011541sudo dpkg --add-architecture i386 sudo mkdir -pm755 /etc/apt/keyringssudo wget -O /etc/apt/keyrings/winehq-archive.key https://dl.winehq.org/wine-builds/winehq.key# 注意 对应你的ubuntu版本# ubuntu 22.04 jammysudo wget -NP /etc/apt/sources.list.d/ https://dl.winehq.org/wine-builds/ubuntu/dists/jammy/winehq-jammy.sourcessudo apt updatesudo apt install --install-recommends winehq-stablewine --version#企业微信wine WeCom_3.1.16.3008.exe#微信wine WeChatSetup_V3.4.0.38.exe#卸载wine uninstaller#配置wine winecfg#输入框不能正常显示（不显示）输入的文字#https://blog.csdn.net/hongxiao2016/article/details/115156831sudo apt install winetricks# 安装微信需要的依赖，时间可能较长，我开了魔法的，还是下了挺久，wine在第一次使用是会问你是否安装 wine mono ,这是linux 上的.net，安装就好，但是可能比较慢，也可以后期手动装(后面补充)，好像不装wine mono也可以正常打开微信winetricks riched20 riched30 richtx32 msftedit ie8#英文模式下微信乱码解决：#https://blog.csdn.net/ysy950803/article/details/80326832#cp -a /Developer/Software/Themes/Fonts/* ~/.deepinwine/Deepin-WeChat/drive_c/windows/Fonts/cp -a /Developer/linux/software/Themes/Fonts/*.TTC ~/.wine/drive_c/windows/Fonts/#cp -a MSYH/* ~/.wine/drive_c/windows/Fonts/#vim msyh_font.reg#REGEDIT4# #[HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows NT\\CurrentVersion\\FontLink\\SystemLink]#\"Lucida Sans Unicode\"=\"msyh.ttc\"#\"Microsoft Sans Serif\"=\"msyh.ttc\"#\"MS Sans Serif\"=\"msyh.ttc\"#\"Tahoma\"=\"msyh.ttc\"#\"Tahoma Bold\"=\"msyhbd.ttc\"#\"msyh\"=\"msyh.ttc\"#\"Arial\"=\"msyh.ttc\"#\"Arial Black\"=\"msyh.ttc\"###deepin-wine6-stable regedit msyh_font.reg#wine regedit msyh_font.reg###vim /home/dave/.deepinwine/Deepin-WeChat/system.reg##vim ~/.wine/system.reg##查找关键词FontSubstitutes，把它下面挨着的“MS Shell Dlg”与“MS Shell Dlg2”的内容改为“msyh”#sed -i 's;\"MS Shell Dlg\"=\"Tahoma\";\"MS Shell Dlg\"=\"msyh\";g' ~/.wine/system.reg#sed -i 's;\"MS Shell Dlg 2\"=\"Tahoma\";\"MS Shell Dlg 2\"=\"msyh\";g' ~/.wine/system.reg VPN123sudo apt install openconnectsudo apt install network-manager-openconnectsudo apt install network-manager-openconnect-gnome SecureCRT1234567891011121314151617181920212223242526272829303132333435#libssl1.0.0sudo dpkg -i libssl1.0.0_1.0.2g-1ubuntu4.20_amd64.deb#libpython2.7#sudo apt-get install libpython2.7wget http://www.python.org/ftp/python/2.7.5/Python-2.7.5.tar.bz2tar -xvjf Python-2.7.5.tar.bz2cd Python-2.7.5./configure --prefix=/usr/local/python2.7 --with-threads --enable-sharedmakemake install altinstallln -s /usr/local/python2.7/lib/libpython2.7.so /usr/libln -s /usr/local/python2.7/lib/libpython2.7.so.1.0 /usr/libln -s /usr/local/python2.7/bin/python2.7 /usr/local/bin/sbin/ldconfig -v#libpng12#sudo add-apt-repository ppa:linuxuprising/libpng12#sudo apt update#sudo apt install libpng12-0sudo apt install build-essential zlib1g-dev#cd#mkdir srcwget https://ppa.launchpadcontent.net/linuxuprising/libpng12/ubuntu/pool/main/libp/libpng/libpng_1.2.54.orig.tar.xztar Jxfv libpng_1.2.54.orig.tar.xzcd libpng-1.2.54./configuremakesudo make installsudo ln -s /usr/local/lib/libpng12.so.0.54.0 /usr/lib/libpng12.sosudo ln -s /usr/local/lib/libpng12.so.0.54.0 /usr/lib/libpng12.so.0#Installsudo dpkg -i scrt-sfx-8.3.4-1699.ubuntu16-64.x86_64.deb Navicat Premium 151234567891011121314151617181920212223242526272829303132333435sudo apt install libfuse2#libcryptowget https://www.openssl.org/source/openssl-1.1.0k.tar.gztar xvf openssl-1.1.0k.tar.gzcd openssl-1.1.0l./configmake -j`nproc`sudo make installsudo ln -s /usr/local/lib/libcrypto.so.1.1 /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1sudo ln -s /usr/local/lib/libssl.so.1.1 /usr/lib/x86_64-linux-gnu/libssl.so.1.1#Crack./navicat-keygen --text ./RegPrivateKey.pemsudo cp -a libgio-2.0.so.0 /Developer/Navicat15/#/lib/x86_64-linux-gnu/libgio-2.0.so.0: undefined symbol: g_module_open_full的解决：#vim navicat15.sh#!/bin/bashexport LD_LIBRARY_PATH=/Developer/Navicat15:$LD_LIBRARY_PATH;/Developer/Navicat15/navicat15-premium-cs-patched.AppImage#Start/Developer/Navicat15/navicat15.sh#https://www.zssnp.top/2021/11/11/netease/#sudo mv /lib/x86_64-linux-gnu/libgio-2.0.so.0 /lib/x86_64-linux-gnu/libgio-2.0.so.0.bak##download libgio-2.0.so.0 from: https://wwx.lanzoui.com/iNxJNwdnkle#sudo cp -a libgio-2.0.so.0 /lib/x86_64-linux-gnu/##发现处理后settings打不开,恢复：#sudo ln -sf /lib/x86_64-linux-gnu/libgio-2.0.so.0.7200.1 /lib/x86_64-linux-gnu/libgio-2.0.so.0##libselinux.so.1: no version information available (required by /lib/x86_64-linux-gnu/libgio-2.0.so.0)#可以忽略，不影响使用 Cider123#https://cider.sh/wget https://github.com/ciderapp/cider-releases/releases/download/v1.5.0/cider_1.5.0_amd64.debsudo dpkg -i cider_1.5.0_amd64.deb 字体1234#wget \"https://dl-sh-ctc-2.pchome.net/25/rm/YosemiteSanFranciscoFont-master.zip\"#mv YosemiteSanFranciscoFont-master SanFranciscoFont#sudo cp -a SanFranciscoFont /usr/share/fonts/sudo git clone https://github.com/AppleDesignResources/SanFranciscoFont /usr/share/fonts/SanFranciscoFont San Francisco Text Medium WPS字体12345#https://blog.huzhifeng.com/2017/01/15/WPS/#https://www.dropbox.com/s/q6rhaorhsbxbylk/wps_symbol_fonts.zip?dl=0sudo mkdir -p /usr/share/fonts/wps_symbol_fontssudo unzip wps_symbol_fonts.zip -d /usr/share/fonts/wps_symbol_fontssudo chmod 755 /usr/share/fonts/wps_symbol_fonts 开机画面1234#https://www.gnome-look.org/browse/cat/109/ord/latest/git clone https://github.com/vinceliuice/grub2-themescd grub2-themes./install.sh SysMonitor12345678910111213#https://www.zhyong.cn/posts/f35/sudo apt install python3-psutil gir1.2-appindicator3-0.1git clone https://github.com/fossfreedom/indicator-sysmonitor.gitcd indicator-sysmonitorsudo make installcd ..rm -rf indicator-sysmonitor#配置方案一&#123;net&#125;║cpu:&#123;cpu&#125;/&#123;cputemp&#125;方案二&#123;net&#125;║CPU &#123;cpu&#125;/&#123;cputemp&#125;║MEM &#123;mem&#125;/&#123;fs///&#125; Zerotier123456#https://blog.csdn.net/awzs7758520/article/details/130127967curl -s https://install.zerotier.com | sudo bash#sudo apt install zerotier-onesudo zerotier-cli join network-idsudo systemctl enable zerotier-onesudo systemctl start zerotier-one 路由不通解决：123456789101112#https://zhichao.org/posts/zerotier通过上面的设置，已经实现了连接到 ZeroTier 的设备使用内网 IP 访问局域网，但是局域网内的设备仍然无法使用 ZeroTier 分配的 IP 来访问那些连接到 ZeroTier 的设备，我们还需要在路由器中配置静态路由:内部网络---&gt;路由设置：网络 / 主机 IP: 与 ZeroTier 网段保持一致： 192.168.195.0网络掩码: 与 ZeroTier 掩码保持一致 (/24 为 255.255.255.0)： 255.255.255.0网关: 安装 ZeroTier 设备的内网 IP： 192.168.3.2#OpenWRT的参考：https://www.douban.com/note/841817168/?_i=00754617wYVM7F,0086159YLqUxZi XRDP12345678910111213141516171819sudo apt install xrdpsudo systemctl enable xrdpsudo systemctl start xrdp#https://stackoverflow.com/questions/78074498/how-to-configure-xrdp-to-work-with-gnome-on-ubuntusudo apt updatesudo apt install xrdp gnome-sessionsudo adduser xrdp ssl-certecho &quot;gnome-session&quot; | tee ~/.xsessionecho &quot;export XAUTHORITY=$&#123;HOME&#125;/.Xauthority&quot; | tee ~/.xsessionrcecho &quot;export GNOME_SHELL_SESSION_MODE=ubuntu&quot; | tee -a ~/.xsessionrcecho &quot;export XDG_CONFIG_DIRS=/etc/xdg/xdg-ubuntu:/etc/xdg&quot; | tee -a ~/.xsessionrcecho &quot;export XDG_CURRENT_DESKTOP=ubuntu:GNOME&quot; | tee -a ~/.xsessionrcsudo systemctl restart xrdp#You must always be logged out locally in order to connect in remotely. 完美方案——解决XRDP连接黑屏，以及桌面优化: 123456#https://www.cnblogs.com/bruce1992/p/16535833.htmlsudo vim /etc/xrdp/startwm.shunset DBUS_SESSION_BUS_ADDRESSunset XDG_RUNTIME_DIRsudo systemctl restart xrdp.service Xrdp 体验优化 减少/解决画面卡顿： 1234567891011121314#https://blog.csdn.net/wu_weijie/article/details/116158271#编辑 /etc/xrdp/xrdp.initcp_send_buffer_bytes=4194304tcp_recv_buffer_bytes=6291456#将以下内容写入配置文件 /etc/sysctl.confnet.core.rmem_max = 12582912net.core.wmem_max = 8388608sudo sysctl -psudo systemctl restart xrdp EQ13 Ubuntu Bluetooth12345sudo dmesg|grep Bluetoothlsusbsudo apt install bluez-firmware 修复grub123456789101112131415161718192021#view UUIDblkid #Starting repair grub#root=(hd0,gpt8)#prefix=/boot/grubset root=(hd0,gpt8)set prefix=(hd0,gpt8)/boot/grubinsmod normalnormal#Logon Systemsudo update-grubsudo grub-install /dev/nvme0n1#或者sudo add-apt-repository ppa:yannubuntu/boot-repairsudo apt-get updatesudo apt-get install boot-repair#打开Dash，输入boot-repair，打开它，点击recommanded repair按钮。接下来按照提示修复即可。#Fixed fstab errormount -o remount, rw / MySQL二进制文件安装全自动安装： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#!/bin/bashsudo apt-get install libaio1 libaio-dev -ysudo apt-get install libnuma-dev -ysudo apt-get install libncurses5 -ycat &gt; /etc/my.cnf &lt;&lt; EOF[mysqld]bind-address=0.0.0.0port=3306socket=/Developer/mysql-5.7.37/data/mysql.sockpid-file=/Developer/mysql-5.7.37/logs/mysql.pidbasedir=/Developer/mysql-5.7.37datadir=/Developer/mysql-5.7.37/datamax_connections=20character-set-server=utf8collation-server=utf8_general_cilower_case_table_names=1EOFcat &gt; /usr/lib/systemd/system/mysqld.service &lt;&lt; EOF[Unit]Description=MySQL ServerAfter=syslog.targetAfter=network.target[Service]Type=simplePermissionsStartOnly=true#ExecStartPre=/bin/mkdir -p /var/run/mysqld#ExecStartPre=/bin/chown mysql:mysql -R /var/run/mysqldExecStart=/Developer/mysql-5.7.37/bin/mysqld_safe --defaults-file=/etc/my.cnfExecStop=/Developer/mysql-5.7.37/bin/mysql.server stopTimeoutSec=300PrivateTmp=trueUser=mysqlGroup=mysqlWorkingDirectory=/Developer/mysql-5.7.37EOFtar zxf mysql-5.7.37-linux-glibc2.12-x86_64.tar.gzmkdir -p /Developer/mysql-5.7.37/cp -a mysql-5.7.37-linux-glibc2.12-x86_64/* /Developer/mysql-5.7.37/rm -fr mysql-5.7.37-linux-glibc2.12-x86_64cp -a /Developer/mysql-5.7.37/support-files/mysql.server /Developer/mysql-5.7.37/bin/mysql.serversed -i \"s;^basedir=.*;basedir=/Developer/mysql-5.7.37;g\" /Developer/mysql-5.7.37/bin/mysql.serversed -i \"s;^datadir=.*;datadir=/Developer/mysql-5.7.37/data;g\" /Developer/mysql-5.7.37/bin/mysql.servergroupadd mysqluseradd -r -g mysql mysqlsudo chown -R mysql:mysql /Developer/mysql-5.7.37sudo mkdir -p /Developer/mysql-5.7.37/logs/sudo chown -R mysql.mysql /Developer/mysql-5.7.37/logs/sudo /Developer/mysql-5.7.37/bin/mysqld --initialize --user=mysql &gt; ./log 2&gt;&amp;1rootpwd=`cat ./log | grep \"temporary password\"|sed 's;^.*: ;;g'`echo \"root pwd is: $rootpwd\"#--basedir=/Developer/mysql-5.7.37 --datadir=/Developer/mysql-5.7.37/dataln -s /Developer/mysql-5.7.37/bin/mysql /usr/bin/mysqlln -s /Developer/mysql-5.7.37/data/mysql.sock /tmp/mysql.sock#Doesn't work for docker buildsystemctl daemon-reloadsudo systemctl enable mysqldsu - mysql -c \"/Developer/mysql-5.7.37/bin/mysqld_safe --defaults-file=/etc/my.cnf &amp;\"mkdir -p /var/run/mysqld/ln -s /Developer/mysql-5.7.37/data/mysql.sock /var/run/mysqld/mysqld.sockln -s /Developer/mysql-5.7.37/bin/mysqladmin /usr/bin/mysqladminsleep 3mysql -uroot -h127.0.0.1 -p$rootpwd --connect-expired-password -e \"alter user user() identified by '';FLUSH PRIVILEGES;\"echo \"The root password has been empty for localhost!\"echo \"Starting granting some privileges...\"mysql -uroot -h127.0.0.1 -e \" \\grant all privileges on *.* to root@'%' identified by 'Aa123#@!' WITH GRANT OPTION; \\grant all privileges on *.* to webase@'%' identified by 'Aa123#@!' WITH GRANT OPTION; \\FLUSH PRIVILEGES; \\\"mysqladmin -uroot -S /var/run/mysqld/mysqld.sock shutdown 手动安装： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#https://blog.csdn.net/weixin_36349646/article/details/102616914#https://www.cnblogs.com/ralap7/p/9034879.html#https://raw.githubusercontent.com/zhaoxunyong/stock-webapp/1.1.x/backend/README.mdsudo apt-get install libaio1 libaio-dev -ysudo apt-get install libnuma-dev -y#创建用户组mysqlgroupadd mysql #-r参数表示mysql用户是系统用户，不可用于登录系统，创建用户mysql并将其添加到用户组mysql中useradd -r -g mysql mysqlsudo chown -R mysql:mysql /Developer/mysql-5.7.37sudo mkdir -p /Developer/mysql-5.7.37/logs/sudo chown -R mysql.mysql /Developer/mysql-5.7.37/logs/vim my.cnf[mysqld]bind-address=0.0.0.0port=3306socket=/Developer/mysql-5.7.37/data/mysql.sockpid-file=/Developer/mysql-5.7.37/logs/mysql.pidbasedir=/Developer/mysql-5.7.37datadir=/Developer/mysql-5.7.37/datamax_connections=20character-set-server=utf8collation-server=utf8_general_cilower_case_table_names=1sudo cp -a my.cnf /etc/my.cnf/usr/lib/systemd/system/mysqld.service#vim mysqld.service[Unit]Description=MySQL ServerAfter=syslog.targetAfter=network.target[Service]Type=simplePermissionsStartOnly=true#ExecStartPre=/bin/mkdir -p /var/run/mysqld#ExecStartPre=/bin/chown mysql:mysql -R /var/run/mysqldExecStart=/Developer/mysql-5.7.37/bin/mysqld_safe --defaults-file=/etc/my.cnfExecStop=/Developer/mysql-5.7.37/bin/mysql.server stopTimeoutSec=300PrivateTmp=trueUser=mysqlGroup=mysqlWorkingDirectory=/Developer/mysql-5.7.37[Install]WantedBy=multi-user.targetsudo cp -a mysqld.service /usr/lib/systemd/system/mysqld.servicecp -a /Developer/mysql-5.7.37/support-files/mysql.server /Developer/mysql-5.7.37/bin/mysql.serversed -i &quot;s;^basedir=.*;basedir=/Developer/mysql-5.7.37;g&quot; /Developer/mysql-5.7.37/bin/mysql.serversed -i &quot;s;^datadir=.*;datadir=/Developer/mysql-5.7.37/data;g&quot; /Developer/mysql-5.7.37/bin/mysql.server#Copying the following password the command&apos;s generated:sudo bin/mysqld --initialize --user=mysql --basedir=/Developer/mysql-5.7.37 --datadir=/Developer/mysql-5.7.37/data#Starting 1sudo systemctl start mysqld#Starting 2#sudo vim support-files/mysql.server basedir=/Developer/mysql-5.7.37datadir=/Developer/mysql-5.7.37/data#Starting MySQL Serversudo support-files/mysql.server startsudo support-files/mysql.server stop#Starting 3#/works/app/mysql/bin/mysqld_safe --datadir=/works/data/mydata --socket=/works/app/mysql/mysql.sock &amp;#mysqladmin -uroot -p -S /works/app/mysql/mysql.sock shutdown/Developer/mysql-5.7.37/bin/mysqld_safe --defaults-file=/etc/my.cnf --socket=/works/app/mysql/mysql.sock &amp;mysqladmin -uroot -p -S /works/app/mysql/mysql.sock shutdown#bin/mysql -uroot -palter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;Aa654321&apos;; FLUSH PRIVILEGES;grant all privileges on *.* to root@&apos;%&apos; identified by &apos;123456&apos; WITH GRANT OPTION;#grant all privileges on *.* to root@&apos;localhost&apos; identified by &apos;Aa654321&apos;;CREATE DATABASE `saas` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; FLUSH PRIVILEGES; docker安装12345678#https://raw.githubusercontent.com/zhaoxunyong/stock-webapp/1.1.x/backend/README.mddocker run -d -p 3306:3306 --restart=always --name mysql \\-e MYSQL_ROOT_PASSWORD=123456 \\# -e MYSQL_DATABASE=test \\-e MYSQL_USER=webase \\-e MYSQL_PASSWORD=123456 \\mysql:5.7.32 \\--character-set-server=utf8 --collation-server=utf8_general_ci --lower_case_table_names=1 Docker123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107systemctl disable iptablessystemctl stop iptablessystemctl disable firewalldsystemctl stop firewalld#Uninstalling old versions:sudo apt-get remove docker docker-engine docker.io containerd runc#Installation from docker offical:#https://docs.docker.com/engine/install/ubuntu/curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -sudo add-apt-repository \\ &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable&quot;sudo apt-get updatesudo apt install docker-ce docker-ce-cli containerd.io#Or Installation From Aliyun#https://www.cnblogs.com/763977251-sg/p/11837130.html#Docker installation#https://aka.ms/vscode-remote/samples/docker-from-dockerapt-get -y install apt-transport-https ca-certificates software-properties-common# step 2: 安装GPG证书curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | apt-key add -# Step 3: 写入软件源信息add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;# Step 4: 更新并安装 Docker-CEapt-get -y update#apt install python3 -y#apt-get -y install docker-ce#sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-pluginsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin#History Version:apt-cache madison docker-ce#Uninstall#sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-compose-pluginsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin#Images, containers, volumes, or customized configuration files on your host are not automatically removed. To #delete all images, containers, and volumes:sudo rm -rf /var/lib/dockersudo rm -rf /var/lib/containerd#Setting china mirror#https://www.cnblogs.com/wushuaishuai/p/9984228.htmlhttps://github.com/zhaoxunyong/vagrant/blob/master/boxes/docker/script.shsudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;&#123; &quot;dns&quot; : [ &quot;8.8.4.4&quot;, &quot;8.8.8.8&quot;, &quot;114.114.114.114&quot; ], &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot;, &quot;https://3laho3y3.mirror.aliyuncs.com&quot;, &quot;http://hub-mirror.c.163.com&quot; ]&#125;EOF#Proxysudo mkdir -p /etc/systemd/system/docker.service.dsudo tee /etc/systemd/system/docker.service.d/http-proxy.conf &lt;&lt;-&apos;EOF&apos;[Service]Environment=&quot;HTTP_PROXY=http://192.168.101.175:1082&quot;Environment=&quot;HTTPS_PROXY=http://192.168.101.175:1082&quot;Environment=&quot;NO_PROXY=127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net,*.aliyun.com,*.163.com,*.docker-cn.com,kubernetes.docker.internal&quot;EOF或者：vim /usr/lib/systemd/system/docker.service # 在dockerd后面加参数ExecStart=/usr/bin/dockerd \\--registry-mirror=https://registry.docker-cn.com \\--registry-mirror=https://3laho3y3.mirror.aliyuncs.com \\--registry-mirror=http://hub-mirror.c.163.com;&quot; \\...以上操作后重启一下 Docker#开启远程API访问端口https://cloud.tencent.com/developer/article/1683689# vim /usr/lib/systemd/system/docker.service[Service]ExecStart=ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.socksudo systemctl daemon-reloadsudo systemctl restart dockerlsof -i:2375COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEdockerd 1259648 root 6u IPv6 2709332 0t0 TCP *:2375 (LISTEN)#Testdocker pull k8s.gcr.io/echoserver:1.4sudo groupadd dockersudo gpasswd -a $&#123;USER&#125; dockersudo chmod 666 /var/run/docker.socksudo systemctl restart dockerdocker info后是否有Registry Mirrors.测速：docker rmi node:latesttime docker pull node:latest History docker installation:123456789101112131415docker version is: 20.10.7:#https://blog.csdn.net/u011519550/article/details/102688892sudo apt-get updatesudo apt-get install apt-transport-https ca-certificates curl software-properties-commoncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable\"apt-cache madison docker-ceapt install docker-ce-cli=5:20.10.7~3-0~ubuntu-xenial docker-ce=5:20.10.7~3-0~ubuntu-xenialUbuntu: Failed to get D-Bus connection: No such file or directory:cat /etc/rc.localsudo mkdir /sys/fs/cgroup/systemdmount -t cgroup -o none,name=systemd cgroup /sys/fs/cgroup/systemd#reboot and docker start your container. Creating centos instance with restrict cpu and memory:1docker run -d --privileged=true --cpus=4 --cpu-shares=4000 -m 2048m --memory-reservation=256m --name mycento registry.zerofinance.net/library/centos:7 /usr/sbin/init","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"elementary os","slug":"elementary-os","date":"2018-12-03T02:06:39.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2018/12/elementary-os.html","link":"","permalink":"http://blog.gcalls.cn/2018/12/elementary-os.html","excerpt":"Elementary OS作为Ubuntu的扩展分支，号称是最美的Linux发行版。系统不仅主题美而且对Ubuntu进行了大幅精简，系统结构显得轻巧不少，运行效率也不错，官方甚至打出了“A fast and open replacement for Windows and macOS”的口号，其野心可见一班。","text":"Elementary OS作为Ubuntu的扩展分支，号称是最美的Linux发行版。系统不仅主题美而且对Ubuntu进行了大幅精简，系统结构显得轻巧不少，运行效率也不错，官方甚至打出了“A fast and open replacement for Windows and macOS”的口号，其野心可见一班。 安装操作系统从官网https://elementary.io/zh_CN/中下载iso文件，下载时输入金额为0即可。用Universal-USB-Installer.exe刻录成U盘进行安装。 docker缩放12345sudo apt updatesudo apt install software-properties-commonsudo add-apt-repository ppa:ricotz/dockysudo apt upgradekillall plank 安装基础包12345sudo apt-get install vimsudo apt-get install unrar#sudo apt install google-chrome-stable#sudo apt install electron-ssrsudo apt install aria2 安装Tweaks123456789101112#sudo add-apt-repository ppa:philip.scott/elementary-tweaks#sudo apt-get update#sudo apt-get install elementary-tweaks#https://github.com/pantheon-tweaks/pantheon-tweakssudo add-apt-repository -y ppa:philip.scott/pantheon-tweakssudo apt-get updatesudo apt install -y pantheon-tweaks#sudo apt-get install dconf-editorsudo apt-get install dconf-tools#sudo apt install nautilus 修改操作系统配置123456cat /proc/sys/fs/inotify/max_user_watches#sudo vim /etc/sysctl.conffs.inotify.max_user_instances=1280fs.inotify.max_user_watches=655360vm.overcommit_memory=1sudo sysctl -p 重启系统，然后再通过应用中心下载：Eddy与GNOME Tweaks，GNOME Tweaks可以设置屏幕缩放。 系统托盘安装stalonetray：1sudo apt-get install stalonetray 配置：vim ~/.stalonetrayrc123456789101112131415#geometry 1x1+1700+1040#transparent true#window_layer top#slot_size 14#icon_size 30#http://stalonetray.sourceforge.net/manpage.htmlgeometry 1x1+1890-0background &quot;#110e0e&quot;transparent truewindow_layer bottomgrow_gravity SEicon_gravity SE slot_size 25icon_size 40 删除多余的网络图标： 1sudo mv /etc/xdg/autostart/nm-applet.desktop ~/ cat /Developer/stalonetray.sh #/bin/sh sleep 1 /usr/bin/stalonetray 配置环境变量vim ~/.bashrc1234567891011121314alias ll=&apos;ls -l&apos;export LANG=zh_CN.UTF-8function proxy_off()&#123; unset http_proxy unset https_proxy echo -e &quot;The proxy has been closed!&quot;&#125;function proxy_on() &#123; export no_proxy=&quot;127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net&quot; export http_proxy=&quot;http://127.0.0.1:1082&quot; export https_proxy=$http_proxy echo -e &quot;The proxy has been opened!&quot;&#125; 使配置生效：1. ~/.bashrc 安装git12345678sudo apt-get install gitgit config --global user.name \"dave.zhao\"git config --global user.email dave.zhao@zerofinance.comgit config --global core.autocrlf falsegit config --global core.safecrlf warngit config --global core.filemode falsegit config --global core.whitespace cr-at-eolgit config --global credential.helper store gui: 123#https://www.gitkraken.com/wget https://release.gitkraken.com/linux/gitkraken-amd64.debsudo dpkg -i gitkraken-amd64.deb 安装输入法以下是五笔的输入法，如果是拼音的话可以直接搜索搜狗拼音并下载安装即可。 12345678910111213#sudo apt-get update#sudo apt-get install im-config fcitx fcitx-config-gtk fcitx-table-wbpy#重启系统后#fcitx-config-gtk3#https://www.beizigen.com/1934.html#http://yongim.ys168.com/wget http://ys-c.ys168.com/244626543/TJRtkVk4K465F3K6KM6/yong-lin-2.5.0-0.7zcp -a yong /opt/sudo /opt/yong/yong-tool.sh --install/opt/yong/yong-tool.sh --select#重启系统后#如果希望五笔拼音一起打的话，修改五笔的配置为：mb/wbpy.ini#快捷键：CTRL_LSHIFT LSHIFT CTRL_SPACE themedocky不用安装。 12#可以用docky替换掉planksudo apt-get install docky 禁止plank自动启动：通过dconf搜索monitored-processes关键字，把其中的plank删除即可。需要把：io.elementary.desktop.cerbere中的plank替换为docky。 皮肤参考https://vinceliuice.github.io/https://github.com/vinceliuice/Canta-theme 推荐几个漂亮的皮肤： eOS-Sierra-GtkeOS-Sierra-Gtk 12git clone https://github.com/btd1337/eOS-Sierra-Gtk ~/.local/share/themes/eOS-Sierra-Gtkgsettings set org.gnome.desktop.interface gtk-theme 'eOS-Sierra-Gtk' 最终效果： vimix-gtk-themesvimix-gtk-themes 1234567git clone https://github.com/vinceliuice/vimix-gtk-themescd vimix-gtk-themes./Installcd ..git clone https://github.com/vinceliuice/vimix-icon-themecd vimix-icon-theme./Installer.sh 最终效果： Sierra-gtk-theme1234#https://github.com/vinceliuice/Sierra-gtk-themegit clone https://github.com/vinceliuice/Sierra-gtk-theme.gitcd Sierra-gtk-theme./install.sh Macos-sierra-CT1git clone https://github.com/zayronxio/Macos-sierra-CT.git ~/.local/share/icons/Macos-sierra-CT la-capitaine-icon-theme123456#https://github.com/btd1337/La-Sierra-Icon-Theme#https://github.com/keeferrourke/la-capitaine-icon-theme.gitmkdir ~/.local/share/iconsgit clone https://github.com/btd1337/La-Sierra-Icon-Theme ~/.local/share/icons/La-Sierra #orgit clone https://github.com/keeferrourke/la-capitaine-icon-theme.git ~/.local/share/icons/la-capitaine-icon-theme 字体1234#wget \"https://dl-sh-ctc-2.pchome.net/25/rm/YosemiteSanFranciscoFont-master.zip\"#mv YosemiteSanFranciscoFont-master SanFranciscoFont#sudo cp -a SanFranciscoFont /usr/share/fonts/sudo git clone https://github.com/AppleDesignResources/SanFranciscoFont /usr/share/fonts/SanFranciscoFont San Francisco Text Medium 添加application菜单12345#https://elementaryos.stackexchange.com/questions/2883/how-can-i-change-the-icon-of-an-application-in-the-elementary-ossudo apt-get install alacarte#Icon=/home/dave/.local/share/icons/hicolor/512x512/apps/appimagekit-balena-etcher-electron.png#rm ~/.config/menus/gnome-applications.menu#打开主菜单就可以进行添加与修改了 wingpanel-indicator-sys-monitor123456#https://www.linuxslaves.com/2018/10/install-wingpanel-system-monitor-indicator-elementary-os-juno.html#https://github.com/PlugaruT/wingpanel-indicator-sys-monitorsudo apt-get install git libglib2.0-dev libgtop2-dev libgranite-dev libgtk-3-dev libwingpanel-2.0-dev meson valacgit clone https://github.com/PlugaruT/wingpanel-indicator-sys-monitor.git &amp;&amp; cd wingpanel-indicator-sys-monitormeson build --prefix=/usr &amp;&amp; cd build/ &amp;&amp; ninjasudo ninja install 桌面图标1234#https://github.com/spheras/desktopfolder#download file from https://github.com/spheras/desktopfolder/releasessudo dpkg -i com.github.spheras.desktopfolder_1.0.10_amd64.deb# logout and login 修改开机启动画面可选。如果不想修改开机启动画面的话，可以不用安装。 1234567#https://tianyijian.github.io/2018/04/05/ubuntu-boot-animation/#https://www.gnome-look.org/browse/cat/109/ord/latest/#https://www.gnome-look.org/p/1237117/unzip Griffin-Grub-Remix.zipcd Griffin-Grub-Remix/sudo ./Install.sh#reboot 在系统设置–&gt;启动应用程序中添加/usr/bin/stalonetray即可。也可以在dconf中添加：io.elementary.desktop.cerbere中添加stalonetray。如果被kill会自动启动。 deepin-wine123456789101112131415161718192021222324#https://github.com/zq1997/deepin-winewget -O- https://deepin-wine.i-m.dev/setup.sh | shsudo apt-fast install deepin.com.qq.im.light#配置，修改显示为160dpiWINEPREFIX=~/.deepinwine/Deepin-QQLight deepin-wine winecfgsudo apt-fast install deepin.com.wechat#配置，修改显示为160dpiWINEPREFIX=~/.deepinwine/Deepin-WeChat deepin-wine winecfgsudo apt-fast install deepin.com.thunderspeed#配置，修改显示为160dpiWINEPREFIX=~/.deepinwine/Deepin-ThunderSpeed deepin-wine winecfgsudo apt-fast install deepin.com.weixin.work#配置，修改显示为160dpiWINEPREFIX=~/.deepinwine/Deepin-WXWork deepin-wine winecfgsudo apt-fast install deepin-screenshotsudo apt-fast install deepin-terminalxdg-open . -&gt; Win+Edeepin-screenshot -&gt; ctrl+alt+Qdeepin-terminal -&gt; ctrl+alt+T 截图可以在应用中心搜索”深度截图”。 常用快捷键1234xdg-open . -&gt; Win+Edeepin-screenshot -&gt; ctrl+alt+Q#deepin-terminal -&gt; ctrl+alt+Tio.elementary.terminal -&gt; ctrl+alt+T Mailspring从https://getmailspring.com/download下载对应的版本安装即可。 WPS字体12345#https://blog.huzhifeng.com/2017/01/15/WPS/#https://www.dropbox.com/s/q6rhaorhsbxbylk/wps_symbol_fonts.zip?dl=0sudo mkdir -p /usr/share/fonts/wps_symbol_fontssudo unzip wps_symbol_fonts.zip -d /usr/share/fonts/wps_symbol_fontssudo chmod 755 /usr/share/fonts/wps_symbol_fonts VPN12345678910sudo apt-get install network-manager-openconnect-gnomesudo mkdir -p /etc/vpncd /etc/vpnsudo wget http://git.infradead.org/users/dwmw2/vpnc-scripts.git/blob_plain/HEAD:/vpnc-scriptsudo chmod +x /etc/vpn/vpnc-script # executesudo openconnect -u aaa --script=/etc/vpn/vpnc-script --no-dtls x.x.x.x# 自动登录, 将密码写入MyScript.txt文件中即可openconnect -u aaa --script=/etc/vpn/vpnc-script --no-dtls x.x.x.x --servercert pin-sha256:+PLuNZB2mIJy8y/Hx3Qwc3QmMhZfulMTOy1S5OakhdY= --passwd-on-stdin &lt; MyScript.txt 也可以通过自动输入密码： 安装spawn： 12#安装spawnsudo apt install expect 以下为对应的脚本： 123456789101112#!/usr/bin/expectset timeout -1#set PWD vagrant#spawn passwdspawn openconnect -u 对应的用户 --script=/etc/vpn/vpnc-script --no-dtls ipexpect \"确定\"send \"确定\\r\"expect \"Password:\"send \"对应的密码\\r\"interact#expect eof javasudo vim /etc/profile.d/java.sh 1234567891011export ANDROID_HOME=/Developer/Android/Sdkexport PATH=$PATH:$ANDROID_HOME/toolsexport PATH=$PATH:$ANDROID_HOME/tools/binexport PATH=$PATH:$ANDROID_HOME/platform-toolsexport PATH=$PATH:$ANDROID_HOME/emulatorexport JAVA_HOME=/Developer/java/jdk1.8.0_152#export JAVA_HOME=$(/usr/libexec/java_home)export M2_HOME=/Developer/apache-maven-3.3.9export GRADLE_USER_HOME=/Developer/.gradleexport PATH=$JAVA_HOME/bin:$M2_HOME/bin:$PATH 使配置生效：1source /etc/profile 将settings.xml放到~/.m2/目录下。注意修改localRepository与password为自己的配置。 其它一些常用工具12345678910#https://www.jianshu.com/p/1e104090ffaasudo apt-get install keepassxsudo apt-get install vlcsudo apt install synapse#synapse也可以用albert代替：https://github.com/albertlauncher/albert#yaheiwget -qO- https://raw.githubusercontent.com/yakumioto/YaHei-Consolas-Hybrid-1.12/master/install.sh | sudo sh#将alt键打造成command键sudo vi /usr/share/X11/xkb/keycodes/evdev#找到LCTL和LALT, 将系统默认的LCTL=37, LALT=64的值互相交换即可。 添加打印机Linux12345678#https://www.kyoceradocumentsolutions.co.za/index/service___support/download_center.false.driver.FS6525MFP._.EN.html#cd \"LinuxPackages/FS-6525MFP series/64bit/Global/English\"sudo ./install.shsudo apt install system-config-printersystem-config-printer#输入URIsocket://192.168.101.2:9100#选择Provide PPD file-&gt;Kyocera FS-6525MFP驱动即可。 Beyond Compare123456wget http://mirrors.aliyun.com/deepin/pool/non-free/b/bcompare/bcompare_4.1.9-21719_amd64.debsudo dpkg -i bcompare_4.1.9-21719_amd64.deb#register#https://gist.github.com/satish-setty/04e1058d3043f4d10e2d52feebe135e8#https://my.oschina.net/sfshine/blog/1829595sudo sed -i \"s/keexjEP3t4Mue23hrnuPtY4TdcsqNiJL-5174TsUdLmJSIXKfG2NGPwBL6vnRPddT7tH29qpkneX63DO9ECSPE9rzY1zhThHERg8lHM9IBFT+rVuiY823aQJuqzxCKIE1bcDqM4wgW01FH6oCBP1G4ub01xmb4BGSUG6ZrjxWHJyNLyIlGvOhoY2HAYzEtzYGwxFZn2JZ66o4RONkXjX0DF9EzsdUef3UAS+JQ+fCYReLawdjEe6tXCv88GKaaPKWxCeaUL9PejICQgRQOLGOZtZQkLgAelrOtehxz5ANOOqCaJgy2mJLQVLM5SJ9Dli909c5ybvEhVmIC0dc9dWH+/N9KmiLVlKMU7RJqnE+WXEEPI1SgglmfmLc1yVH7dqBb9ehOoKG9UE+HAE1YvH1XX2XVGeEqYUY-Tsk7YBTz0WpSpoYyPgx6Iki5KLtQ5G-aKP9eysnkuOAkrvHU8bLbGtZteGwJarev03PhfCioJL4OSqsmQGEvDbHFEbNl1qJtdwEriR+VNZts9vNNLk7UGfeNwIiqpxjk4Mn09nmSd8FhM4ifvcaIbNCRoMPGl6KU12iseSe+w+1kFsLhX+OhQM8WXcWV10cGqBzQE9OqOLUcg9n0krrR3KrohstS9smTwEx9olyLYppvC0p5i7dAx2deWvM1ZxKNs0BvcXGukR+/g\" /usr/lib/beyondcompare/BCompare Then restart BC, click “Enter License”: — BEGIN LICENSE KEY —GXN1eh9FbDiX1ACdd7XKMV7hL7x0ClBJLUJ-zFfKofjaj2yxE53xauIfkqZ8FoLpcZ0Ux6McTyNmODDSvSIHLYhg1QkTxjCeSCk6ARz0ABJcnUmd3dZYJNWFyJun14rmGByRnVPL49QH+Rs0kjRGKCB-cb8IT4Gf0Ue9WMQ1A6t31MO9jmjoYUeoUmbeAQSofvuK8GN1rLRv7WXfUJ0uyvYlGLqzq1ZoJAJDyo0Kdr4ThF-IXcv2cxVyWVW1SaMq8GFosDEGThnY7C-SgNXW30jqAOgiRjKKRX9RuNeDMFqgP2cuf0NMvyMrMScnM1ZyiAaJJtzbxqN5hZOMClUTE+++— END LICENSE KEY —– XMind 81234567891011121314151617wget http://dl2.xmind.cn/xmind-8-update8-linux.zipsudo unzip xmind-8-update8-linux.zip -d /Developer/xmind-8#run /Developer/xmind-8/XMind_amd64/XMind#Put icon into applications, you should create a file:#cd /Developer/xmind-8/XMind_amd64/#echo \"cd /Developer/xmind-8/XMind_amd64 &amp;&amp; ./XMind\" &gt; xmind#chmod +x xmind#If you want to crack, please see: https://blog.csdn.net/weixin_40576010/article/details/89323313vim /Developer/xmind-8/XMind_amd64-javaagent:/Developer/xmind-8/XMind_amd64/XMindCrack.jarcd /Developer/xmind-8/XMind_amd64/./xmind断开网络(一定要这步骤）， 打开Xmind, 在帮助-&gt;序列号邮箱输入：x@iroader.me序列号：XAka34A2rVRYJ4XBIU35UZMUEEF64CMMIYZCK2FZZUQNODEKUHGJLFMSLIQMQUCUBXRENLK6NZL37JXP4PZXQFILMQ2RG5R7G4QNDO3PSOEUBOCDRYSSXZGRARV6MGA33TN2AMUBHEL4FXMWYTTJDEINJXUAV4BAYKBDCZQWVF3LWYXSDCXY546U3NBGOI3ZPAP2SO3CSQFNB7VVIY123456789012345点击确认，然后关闭xmind, 重开，点击帮助-&gt;关于xmind，如果显示激活为，就是成功 SecureCRT12345678910111213wget wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.7_amd64.debsudo dpkg -i libssl1.0.0_1.0.2n-1ubuntu5.7_amd64.debsudo cp -a /snap/gnome-3-34-1804/77/usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0 /usr/local/lib/sudo ldconfig -v#https://blog.csdn.net/duapple/article/details/117757050sudo add-apt-repository ppa:linuxuprising/libpng12 sudo apt updatesudo apt install libpng12-0#installsudo dpkg -i scrt-sfx-8.3.4-1699.ubuntu16-64.x86_64.deb#crackrm /tmp/.securecrt.tmpsudo perl securecrt_linux_crack.pl /usr/bin/SecureCRT virtualbox下载virtualbox与Extension_Pack，直接安装即可。不过安装后虚拟机不能找到usb，是因为没有权限，通过以下命令解决： 123#https://blog.csdn.net/huohongpeng/article/details/60965563cat /etc/group | grep vboxsudo usermod -a -G vboxusers dave 重启系统，再次打开虚拟机，USB设备都已经被识别了。 系统备份与还原备份123#https://blog.csdn.net/sinat_27554409/article/details/78227496#备份sudo tar -cvpzf /media/dave/DATA/elementary.backup.tgz --ignore-failed-read --exclude=/proc --exclude=/lost+found --exclude=/mnt --exclude=/sys --exclude=/media --exclude=/tmp --exclude=/Developer / &gt; /dev/null 还原如果原来的Ubuntu系统已经崩溃，无法进入。则可以使用Ubuntu安装U盘（live USB）进入试用Ubuntu界面。 切换到root用户，找到之前Ubuntu系统的根目录所在磁盘分区（一般电脑上的磁盘分区（假设分区名称为sdaX）均可以在当前Ubuntu系统的根目录下的media目录下（即/media）找到。目录通常为当前根目录下 cd /media/磁盘名称/分区名称）。进入该分区，输入以下指令来删除该根目录下的所有文件： 1sudo rm -rf /media/磁盘名称/分区名称* 将备份文件”elementary.backup.tgz”拷入该分区：1sudo cp -i elementary.backup.tgz /media/磁盘名/分区名sdaX 进入分区并将压缩文件解压缩，参数x是告诉tar程序解压缩备份文件：1sudo tar xvpfz elementary.backup.tgz 重新创建那些在备份时被排除在外的目录：1sudo mkdir proc lost+found mnt sys media tmp Developer","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"Jenkins代码质量扫描与自动化部署","slug":"Jenkins代码质量扫描与自动化部署","date":"2017-11-30T04:03:45.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2017/11/Jenkins代码质量扫描与自动化部署.html","link":"","permalink":"http://blog.gcalls.cn/2017/11/Jenkins代码质量扫描与自动化部署.html","excerpt":"本文记录一下Jenkins代码质量扫描与自动化部署的全过程。","text":"本文记录一下Jenkins代码质量扫描与自动化部署的全过程。 Jenkins安装Jenkins 是一个开源项目，提供了一种易于使用的持续集成系统，使开发者从繁杂的集成中解脱出来，专注于更为重要的业务逻辑实现上。同时 Jenkins 能实施监控集成中存在的错误，提供详细的日志文件和提醒功能，还能用图表的形式形象地展示项目构建的趋势和稳定性。 war安装JENKINS_VERSION：2.73.3 1234wget http://mirrors.jenkins.io/war-stable/latest/jenkins.warmkdir -p /var/jenkinsexport JENKINS_HOME=/var/jenkinsjava -jar jenkins.war --httpPort=9999 初始登录密码在jenkins启动时可以看到。 docker安装12345678910#https://github.com/jenkinsci/docker/blob/master/README.mdmkdir /var/jenkinschown 1000:1000 -R /var/jenkins /works/jenkinsdocker run --name myjenkins -p 9999:8080 -p 50000:50000 \\-e \"JAVA_OPTS=-Xms512m -Xmx512m\" \\-v /works/jenkins/.gradle:/var/jenkins_home/.gradle \\-v /works/jenkins/.m2:/var/jenkins_home/.m2 \\-v /var/jenkins:/var/jenkins_home \\-v /works/jenkins:/works/jenkins \\jenkins/jenkins:lts 也可以自己写Dockerfile编译：123456789101112131415161718192021222324252627282930313233343536# Version: 1.0.0FROM centosMAINTAINER xxx.xxx.com#ENV JENKINS_HOME /usr/local/jenkins#VOLUME [ \"/var/jenkins_home\", \"/usr/local/jenkins\" ]RUN yum install -y unzip gitADD java.sh /etc/profile.d/ADD jenkins_env.tgz /usr/local/ADD settings.xml ~/.m2/RUN ln -s /usr/local/apache-ant-1.10.1 /usr/local/ant \\ &amp;&amp; ln -s /usr/local/groovy-2.4.10 /usr/local/groovy \\ &amp;&amp; ln -s /usr/local/apache-maven-3.3.9 /usr/local/maven \\ &amp;&amp; ln -s /usr/local/gradle-3.4 /usr/local/gradle \\ &amp;&amp; ln -s /usr/local/jdk1.7.0_80 /usr/local/jdk1.7RUN chmod +x /usr/local/gradle/bin/gradle /usr/local/maven/bin/mvn /usr/local/ant/bin/ant /usr/local/groovy/bin/groovy#RUN sed -i 's;SELINUX=.*;SELINUX=disabled;' /etc/selinux/configRUN sed -i 's;LANG=.*;LANG=\"zh_CN.UTF-8\";' /etc/locale.conf#RUN timedatectl set-timezone Asia/ShanghaiRUN echo -e 'search xxx.com \\nnameserver 192.168.100.101 \\nnameserver 114.114.114.114' &gt; /etc/resolv.confRUN . /etc/profile.d/java.shCOPY jenkins.war /usr/local/jenkins/WORKDIR /usr/local/jenkinsENTRYPOINT [ \"/usr/local/jdk1.7/bin/java\", \"-Xmx2048M\", \"-Xms2048M\", \"-XX:PermSize=128M\", \"-XX:MaxPermSize=512m\", \"-jar\", \"-DJENKINS_HOME=/var/jenkins_home\", \"jenkins.war\" ] #CMD [\"-h\"]#EXPOSE 80 java.sh12345678export JAVA_HOME=/usr/local/jdk1.7export MVN_HOME=/usr/local/mavenexport GRADLE_HOME=/usr/local/gradleexport GRADLE_USER_HOME=jenkins_home/.gradleexport ANT_HOME=/usr/local/antexport GROOVY_HOME=/usr/local/groovyexport JENKINS_HOME=/var/jenkins_homeexport PATH=$JAVA_HOME/bin:$MVN_HOME/bin:$GRADLE_HOME/bin:$ANT_HOME/bin:$GROOVY_HOME/bin:$PATH plugin安装需要安装以下插件： Ant Build Pipeline Project statistics Static Analysis Collector Checkstyle PMD FindBugs JaCoCo DRY Git Parameter Extensible Choice Parameter Gradle Publish Over SSH Email Extension Configuration Slicing Environment Injector 环境配置java/maven/git环境配置登录jenkins后，在:系统管理-&gt;Global Tool Configuration中配置。 Jenkins Location登录jenkins后，在:系统管理-&gt;系统设置中配置，用于设置管理员的邮箱地址。 Extended E-mail Notification配置以下信息： Default Subject: 1构建通知:$PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS! Default Content: 123456789101112131415161718192021222324252627&lt;hr/&gt;(本邮件是程序自动下发的，请勿回复！)&lt;br/&gt;&lt;hr/&gt;项目名称：$JOB_NAME&lt;br/&gt;&lt;hr/&gt;项目描述：$JOB_DESCRIPTION&lt;br/&gt;&lt;hr/&gt;构建编号：$BUILD_NUMBER&lt;br/&gt;&lt;hr/&gt;构建状态：$BUILD_STATUS&lt;br/&gt;&lt;hr/&gt;触发原因：$&#123;CAUSE&#125;&lt;br/&gt;&lt;hr/&gt;构建地址：&lt;a href=\"$BUILD_URL\"&gt;$BUILD_URL&lt;/a&gt;&lt;br/&gt;&lt;hr/&gt;构建日志地址：&lt;a href=\"$&#123;BUILD_URL&#125;console\"&gt;$&#123;BUILD_URL&#125;console&lt;/a&gt;&lt;br/&gt;&lt;hr/&gt;git地址：&lt;a href=\"$GIT_URL\"&gt;$&#123;GIT_URL&#125;&lt;/a&gt;&lt;hr/&gt;git版本号：$&#123;GIT_BRANCH&#125;&lt;br/&gt;&lt;hr/&gt;变更集:$&#123;JELLY_SCRIPT,template=\"template.jelly\"&#125;&lt;br/&gt;&lt;hr/&gt;$PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS:Check console output at &lt;a href=\"$BUILD_URL\"&gt;$BUILD_URL&lt;/a&gt; to view the results. 对应的变量请参考https://www.cnblogs.com/weiweifeng/p/8295724.html 邮件模板将template.jelly文件放在$JENKINS_HOME的email-templates目录中。 Default Pre-send Script有时候可能想在手动点击Build时，只发邮件给自己。或者是想临时过滤某些账户，可以通过Default Pre-send Script来实现，在Jenkins—&gt;配置中配置Default Pre-send Script： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// https://wiki.jenkins-ci.org/display/JENKINS/Email-ext+Recipes#Email-extRecipes-AdditionalTemplatesIntheSourceCodedef buildCauses = build.causesdef excludeEmails = []if (build.result.toString().equals(&quot;FAILURE&quot;)) &#123; msg.addHeader(&quot;X-Priority&quot;, &quot;1 (Highest)&quot;); msg.addHeader(&quot;Importance&quot;, &quot;High&quot;); &#125;def getTriggeredEmail(cause) &#123; def email = &quot;&quot; if(cause instanceof hudson.model.Cause.UpstreamCause) &#123; for(def p in cause.upstreamCauses) &#123; if(p instanceof hudson.model.Cause.UserIdCause) &#123; def user = User.get(p.userId) email = user.getProperty(hudson.tasks.Mailer.UserProperty.class).getAddress() if(email!=null &amp;&amp; !email.equals(&quot;&quot;)) &#123; break &#125; &#125; &#125; &#125; else if(cause instanceof hudson.model.Cause.UserIdCause) &#123; def user = User.get(cause.userId) email = user.getProperty(hudson.tasks.Mailer.UserProperty.class).getAddress() &#125; return email&#125;for(def cause in buildCauses) &#123; try &#123; if(cause.shortDescription.indexOf(&quot;Started by upstream&quot;) != -1 || cause.shortDescription.indexOf(&quot;Started by downstream&quot;) != -1) &#123; cancel = true &#125; else if(cause.shortDescription.indexOf(&quot;Started by timer&quot;) != -1 || cause.shortDescription.indexOf(&quot;Started by an SCM change&quot;) != -1) &#123; // Send mail to all recipients def allEmails = msg.getAllRecipients().findAll &#123; addr -&gt; return !excludeEmails.contains(addr.address) &#125; as javax.mail.internet.InternetAddress[] msg.setRecipients(javax.mail.Message.RecipientType.TO, allEmails) &#125; else &#123; String triggeredEmail = getTriggeredEmail(cause) logger.println(&quot;triggeredEmail:&quot;+triggeredEmail) if(triggeredEmail!=null &amp;&amp; !triggeredEmail.equals(&quot;&quot;)) &#123; msg.setRecipient(javax.mail.Message.RecipientType.TO, new javax.mail.internet.InternetAddress(triggeredEmail)) // Send mail to logined user logger.println(&quot;Sending email to triggeredEmail:&quot;+triggeredEmail) &#125; else &#123; logger.println(&quot;Triggered email is empty, send mail canceled!&quot;) cancel = true &#125; &#125; &#125; catch(e) &#123; logger.println(&quot;error:&quot;+e.message) cancel = true &#125;&#125; 静态代码扫描项目配置在pom.xml中添加： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-eclipse-plugin&lt;/artifactId&gt; &lt;version&gt;2.10&lt;/version&gt; &lt;configuration&gt; &lt;wtpversion&gt;2.0&lt;/wtpversion&gt; &lt;additionalProjectnatures&gt; &lt;projectnature&gt;org.eclipse.jdt.core.javanature&lt;/projectnature&gt; &lt;projectnature&gt;net.sf.eclipsecs.core.CheckstyleNature&lt;/projectnature&gt; &lt;projectnature&gt;ch.acanda.eclipse.pmd.builder.PMDNature&lt;/projectnature&gt; &lt;projectnature&gt;edu.umd.cs.findbugs.plugin.eclipse.findbugsNature&lt;/projectnature&gt; &lt;/additionalProjectnatures&gt; &lt;additionalBuildcommands&gt; &lt;buildcommand&gt;org.eclipse.jdt.core.javabuilder&lt;/buildcommand&gt; &lt;buildcommand&gt;net.sf.eclipsecs.core.CheckstyleBuilder&lt;/buildcommand&gt; &lt;buildcommand&gt;ch.acanda.eclipse.pmd.builder.PMDBuilder&lt;/buildcommand&gt; &lt;buildcommand&gt;edu.umd.cs.findbugs.plugin.eclipse.findbugsBuilder&lt;/buildcommand&gt; &lt;/additionalBuildcommands&gt; &lt;/configuration&gt;&lt;/plugin&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-checkstyle-plugin&lt;/artifactId&gt; &lt;version&gt;2.17&lt;/version&gt; &lt;configuration&gt; &lt;configLocation&gt;http://gitlab.aeasycredit.net/dave.zhao/codecheck/raw/master/checkstyle/checkstyle.xml&lt;/configLocation&gt; &lt;!-- &lt;propertiesLocation&gt;/Developer/checkstyle/checkstyle.properties&lt;/propertiesLocation&gt; --&gt; &lt;propertyExpansion&gt;samedir=http://gitlab.aeasycredit.net/dave.zhao/codecheck/raw/master/checkstyle&lt;/propertyExpansion&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;consoleOutput&gt;true&lt;/consoleOutput&gt; &lt;outputFileFormat&gt;xml&lt;/outputFileFormat&gt; &lt;failsOnError&gt;false&lt;/failsOnError&gt; &lt;linkXRef&gt;false&lt;/linkXRef&gt; &lt;/configuration&gt;&lt;/plugin&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-pmd-plugin&lt;/artifactId&gt; &lt;version&gt;3.6&lt;/version&gt; &lt;configuration&gt; &lt;rulesets&gt; &lt;ruleset&gt;http://gitlab.aeasycredit.net/dave.zhao/codecheck/raw/master/pmd/myRuleSet.xml&lt;/ruleset&gt; &lt;/rulesets&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;consoleOutput&gt;true&lt;/consoleOutput&gt; &lt;outputFileFormat&gt;xml&lt;/outputFileFormat&gt; &lt;failsOnError&gt;true&lt;/failsOnError&gt; &lt;linkXRef&gt;false&lt;/linkXRef&gt; &lt;/configuration&gt;&lt;/plugin&gt;&lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;findbugs-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.4&lt;/version&gt; &lt;configuration&gt; &lt;failOnError&gt;true&lt;/failOnError&gt; &lt;threshold&gt;Medium&lt;/threshold&gt; &lt;effort&gt;Default&lt;/effort&gt; &lt;maxRank&gt;15&lt;/maxRank&gt; &lt;outputEncoding&gt;UTF-8&lt;/outputEncoding&gt; &lt;sourceEncoding&gt;UTF-8&lt;/sourceEncoding&gt; &lt;includeFilterFile&gt;http://gitlab.aeasycredit.net/dave.zhao/codecheck/raw/master/findbugs/include_filter.xml&lt;/includeFilterFile&gt; &lt;/configuration&gt; &lt;!-- &lt;executions&gt; &lt;execution&gt; &lt;id&gt;run-findbugs&lt;/id&gt; &lt;phase&gt;install&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;check&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; --&gt;&lt;/plugin&gt; 代码扫描12345678910111213#checkstyle#mvn checkstyle:check 有异常时会中断运行mvn checkstyle:checkstyle#pmdmvn pmd:pmd#重复代码检查mvn pmd:cpd#Findbug#mvn clean install findbugs:check 有异常时会中断运行，并且一定要先编译，因为findbugs是通过class文件来分析的mvn clean install findbugs:findbugs 对应的checkstyle/pmd/findbugs的相关配置请参考：codecheck.zip JaCoCoEclipse插件The update site for EclEmma is http://update.eclemma.org/. EclEmma is also available via the Eclipse Marketplace Client, simply search for “EclEmma”. Maven插件在pom.xml中添加： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475&lt;!-- https://www.cnblogs.com/fnlingnzb-learner/p/10637802.html --&gt;&lt;!-- https://blog.csdn.net/qq_29611427/article/details/88735366 --&gt;&lt;!-- skip: mvn clean install -Djacoco.skip=true --&gt;&lt;plugin&gt; &lt;groupId&gt;org.jacoco&lt;/groupId&gt; &lt;artifactId&gt;jacoco-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.8.7&lt;/version&gt; &lt;configuration&gt; &lt;skip&gt;$&#123;skip_jacoco&#125;&lt;/skip&gt; &lt;includes&gt; &lt;!-- &lt;include&gt;com/**/tenant/mapper/*&lt;/include&gt; --&gt; &lt;/includes&gt; &lt;!-- rules裏面指定覆蓋規則 --&gt; &lt;rules&gt; &lt;rule implementation=\"org.jacoco.maven.RuleConfiguration\"&gt; &lt;element&gt;BUNDLE&lt;/element&gt; &lt;limits&gt; &lt;!-- 指定方法覆蓋到50% --&gt; &lt;limit implementation=\"org.jacoco.report.check.Limit\"&gt; &lt;counter&gt;METHOD&lt;/counter&gt; &lt;value&gt;COVEREDRATIO&lt;/value&gt; &lt;minimum&gt;0.00&lt;/minimum&gt; &lt;/limit&gt; &lt;!-- 指定分支覆蓋到50% --&gt; &lt;limit implementation=\"org.jacoco.report.check.Limit\"&gt; &lt;counter&gt;BRANCH&lt;/counter&gt; &lt;value&gt;COVEREDRATIO&lt;/value&gt; &lt;minimum&gt;0.00&lt;/minimum&gt; &lt;/limit&gt; &lt;!-- 指定類覆蓋到50% --&gt; &lt;limit implementation=\"org.jacoco.report.check.Limit\"&gt; &lt;counter&gt;CLASS&lt;/counter&gt; &lt;value&gt;COVEREDRATIO&lt;/value&gt; &lt;minimum&gt;0.00&lt;/minimum&gt; &lt;!-- 指定類覆蓋到100%，不能遺失任何類 --&gt;&lt;!-- &lt;value&gt;MISSEDCOUNT&lt;/value&gt; --&gt;&lt;!-- &lt;maximum&gt;0&lt;/maximum&gt; --&gt; &lt;/limit&gt; &lt;limit&gt; &lt;counter&gt;COMPLEXITY&lt;/counter&gt; &lt;value&gt;COVEREDRATIO&lt;/value&gt; &lt;!-- 最低覆盖率 --&gt; &lt;minimum&gt;0.00&lt;/minimum&gt; &lt;/limit&gt; &lt;/limits&gt; &lt;/rule&gt; &lt;/rules&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;pre-unit-tests&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;prepare-agent&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;!-- Ensures that the code coverage report for unit tests is created after unit tests have been run --&gt; &lt;execution&gt; &lt;id&gt;post-unit-test&lt;/id&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;report&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;dataFile&gt;target/jacoco.exec&lt;/dataFile&gt; &lt;outputDirectory&gt;target/jacoco-ut&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;!-- &lt;execution&gt; &lt;id&gt;check&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;check&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; --&gt; &lt;/executions&gt;&lt;/plugin&gt; jenkins jacoco插件会有设置rules的地方，如果有使用jenkins的话rules可以不用配置。 代码扫描: 12#mvn clean install jacoco:check 有异常时会中断运行，并且一定要先编译，因为jacoco是通过class文件来分析的mvn clean install jacoco:report jenkins项目配置参数化构建过程 注意：Git parameter只能实现一些简单的过滤条件，如果想实现复杂的过滤的话，可以用Groovy脚本（通过Extensible Choice Plugin实现）： 对应的脚本为： 123456789101112131415// https://gist.github.com/lyuboraykov/8deae849e4812669793adef gitURL = project.scm.key.replace(&quot;git &quot;, &quot;&quot;)def command = &quot;git ls-remote --heads -h $gitURL | grep &apos;.x&apos; | sort -t &apos;/&apos; -k 3 -V&quot;def proc = [&apos;bash&apos;, &apos;-c&apos;, command].execute()proc.waitFor()def branches = []branches = proc.in.text.readLines().findAll &#123; // ==~: match =~: find it =~ /\\d+.*\\.x$/&#125;.collect &#123; //it.replaceAll(/.*/, &apos;&apos;) it = it.replaceAll(/.*\\trefs\\/heads\\//, &apos;&apos;) &#125;.reverse()//.sort().reverse()return branches 或者全部通过shell过滤：1234567// https://gist.github.com/lyuboraykov/8deae849e4812669793adef gitURL = project.scm.key.replace(\"git \", \"\")// refs/heads/1.6.xdef command = \"git ls-remote --heads -h $gitURL | awk '&#123;print \\$2&#125;' | sort -t '/' -k 3 -V -r | egrep '/[0-9]+\\\\.[0-9]+\\\\.x' | sed 's;^refs/heads/;;g'\"def proc = ['bash', '-c', command].execute()proc.waitFor()return proc.in.text.readLines() 另外，由于Jenkins的安全限制，groovy没有权限运行，可以通过http://ip:port/scriptApproval/进行授权。 配置仓库 构建触发器 Poll SCM：定时检查源码变更（根据SCM软件的版本号），如果有更新就checkout最新code下来，然后执行构建动作。我的配置如下： 1*/5 * * * * （每5分钟检查一次源码变化） Build periodically：周期进行项目构建（它不care源码是否发生变化），我的配置如下： 10 2 * * * （每天2:00 必须build一次源码） 构建 注意：checkstyle:checkstyle pmd:pmd pmd:cpd findbugs:findbugs只会警告，错误时不会退出。如果想显示完整的代码扫描结果，又想在错误时退出，可以进行以下配置： 如果有使用jacoco的话不能跳过测试用例： Post Steps：构建完成后的步骤。一般常用于自动化部署。 参考以下的自动化部署脚本： 12345678910111213141516171819202122232425262728293031323334353637#!/bin/bashPATH='/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin'export PATHJOBNAME=$1JARNAME=$2RUSER='root'HOST='192.168.108.183'DEVKEY='/etc/183'PROJECT_PATH=$&#123;JENKINS_HOME&#125;/workspace/$&#123;JOBNAME&#125;if [[ \"$&#123;JOBNAME&#125;\" == \"\" ]]; then echo \"JOBNAME must not be empty!\" exit 1fidos2unix $PROJECT_PATH/pom.xmldos2unix $PROJECT_PATH/Dockerfiledos2unix $PROJECT_PATH/docker.shVERSION=`cat $&#123;PROJECT_PATH&#125;/pom.xml|grep version|sed -n \"1p\"|sed \"s;^\\s&lt;version&gt;;;\"|sed \"s;&lt;/version&gt;;;\"|sed -E 's;\\r\\n;;'`echo \"VERSION=$&#123;VERSION&#125;\" APPNAME=$(echo $JOBNAME|sed 's;\\.dev;;')if [[ \"$&#123;JARNAME&#125;\" == \"\" ]]; then JARNAME=$APPNAME JARFILE=\"$&#123;PROJECT_PATH&#125;/target/$&#123;JARNAME&#125;-$&#123;VERSION&#125;.jar\"else JARFILE=\"$&#123;PROJECT_PATH&#125;/$&#123;JARNAME&#125;/target/$&#123;JARNAME&#125;-$&#123;VERSION&#125;.jar\"fiecho \"JARFILE=$JARFILE\"echo \"scp -i $&#123;DEVKEY&#125; $&#123;JARFILE&#125; $&#123;RUSER&#125;@$&#123;HOST&#125;:/works/app/hkapp/$&#123;APPNAME&#125;\"scp -i $&#123;DEVKEY&#125; $&#123;JARFILE&#125; $&#123;RUSER&#125;@$&#123;HOST&#125;:/works/app/hkapp/$&#123;APPNAME&#125;/scp -i $&#123;DEVKEY&#125; $&#123;PROJECT_PATH&#125;/Dockerfile $&#123;RUSER&#125;@$&#123;HOST&#125;:/works/app/hkapp/$&#123;APPNAME&#125;/scp -i $&#123;DEVKEY&#125; $&#123;PROJECT_PATH&#125;/docker.sh $&#123;RUSER&#125;@$&#123;HOST&#125;:/works/app/hkapp/$&#123;APPNAME&#125;/ssh -i $&#123;DEVKEY&#125; $&#123;RUSER&#125;@$&#123;HOST&#125; \"cd /works/app/hkapp/$&#123;APPNAME&#125; &amp;&amp; sh docker.sh $&#123;VERSION&#125; $&#123;APPNAME&#125;\" 构建设置 此处将代码质量扫描的结果显示出来： 构建后操作 在首页显示代码质量扫描的结果： Editable Email Notification Jenkins视图设置 效果如下： 权限控制Configure Global Security Manage and Assign RolesManage Roles 重点说明一下Project roles：默认dev role是没有Build job的权限，可以在具体的project中配置，比如说允许dev build以.test结尾的项目： 1^.*\\.test$ 或者只允许dev build不以.test开头的项目： 1^(?!hkcash).+$ 或者只允许dev build不以.test结尾的项目： 1^.*\\.(?&lt;!test)$ Assign Roles Configuration Slicing如果项目太多的话，修改每个项目的配置太痛苦，可以通过Slicing批量修改。比如要修改Maven Goals and Options：","categories":[{"name":"jenkins","slug":"jenkins","permalink":"http://blog.gcalls.cn/categories/jenkins/"},{"name":"docker","slug":"jenkins/docker","permalink":"http://blog.gcalls.cn/categories/jenkins/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.gcalls.cn/tags/docker/"},{"name":"jenkins","slug":"jenkins","permalink":"http://blog.gcalls.cn/tags/jenkins/"}]},{"title":"spring cloud+docker+prometheus grafana监控","slug":"spring-cloud-docker-prometheus-grafana监控","date":"2017-11-17T07:28:48.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2017/11/spring-cloud-docker-prometheus-grafana监控.html","link":"","permalink":"http://blog.gcalls.cn/2017/11/spring-cloud-docker-prometheus-grafana监控.html","excerpt":"最近公司上线一套基于docker的spring cloud微服务系统，记录一下相关的监控技术。","text":"最近公司上线一套基于docker的spring cloud微服务系统，记录一下相关的监控技术。 spring boot监控actuatorSpring Boot 包含了一系列的附加特性，来帮助你监控和管理生产环境下运行时的应用程序。你可以通过HTTP endpoints、JMX或者SSH来监控和管理应用——健康状况、系统指标、参数信息、内存状况等等。 添加依赖: 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; admin-dashboardSpring Boot Admin是一个管理和监控Spring Boot应用的项目。我们可以通过Spring Boot Admin的客户端运行，也可以Spring Cloud中注册为一个服务（比如：注册到Eureka中）。Spring Boot Amin仅仅是一个建立在Spring Boot Actuator端点上的AngularJS的应用。 只好不要集成在业务系统中，可以单独建立一个project。只需要进行以下添加： 添加依赖： 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-server&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-server-ui&lt;/artifactId&gt;&lt;/dependency&gt; 启动类添加： 123@EnableDiscoveryClient@EnableAdminServer@SpringBootApplication 配置: 123456789101112131415161718192021222324252627282930313233343536server: port: 8040 management: context-path: /ops security: enabled: false spring: application: name: admin-dashboard boot: admin: monitor: connect-timeout: 10000 read-timeout: 60000eureka: instance: preferIpAddress: true # eureka的ip，如果有多个ip地址时，需要在此处指定# ipAddress: 127.0.0.1 # 续约到期时间（默认90秒） leaseExpirationDurationInSeconds: 30 #续约更新时间间隔（默认30秒），在生产中，最好坚持使用默认值，因为在服务器内部有一些计算，他们对续约做出假设。 leaseRenewalIntervalInSeconds: 10# instance-id: $&#123;spring.application.name&#125;:$&#123;eureka.instance.ipAddress&#125;:$&#123;server.port&#125; statusPageUrlPath: $&#123;management.context-path&#125;/info healthCheckUrlPath: $&#123;management.context-path&#125;/health metadata-map: management.context-path: $&#123;management.context-path&#125; client: registerWithEureka: false serviceUrl: defaultZone: http://localhost:8761/eureka/ 效果： prometheus日志收集Prometheus用于收集actuator的信息，再在grafana中进行显示。 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;io.prometheus&lt;/groupId&gt; &lt;artifactId&gt;simpleclient_spring_boot&lt;/artifactId&gt; &lt;version&gt;0.1.0&lt;/version&gt;&lt;/dependency&gt; 启动类添加： 123@EnablePrometheusEndpoint@EnableSpringBootMetricsCollector@SpringBootApplication 配置: 123456789# prometheus endpoint, enabled default false endpoints: prometheus: enabled: true# prometheus endpoint, enabled default false endpoints: prometheus: enabled: true 也可以对某个接口作数据收集： 123456789101112private final Gauge getErrorTaskListRequests = Gauge.build() .labelNames(\"api\",\"desc\").name(\"boc_getErrorTaskList\") help(\"Boc getErrorTaskList failure.\").register();@Overridepublic List&lt;TaskDto&gt; getErrorTaskList() &#123; getErrorTaskListRequests.clear(); if(taskDtoList!=null &amp;&amp; !taskDtoList.isEmpty()) &#123; getErrorTaskListRequests.labels(\"getErrorTaskList\",\"定时任务监控\").inc(taskDtoList.size()); &#125; return taskDtoList;&#125; 访问：http://ip:port/ops/prometheus prometheus参考：https://segmentfault.com/a/1190000008629939 Prometheus 是使用 Golang 开发的开源监控系统，被人称为下一代监控系统，是为数不多的适合 Docker、Mesos 、Kubernetes 环境的监控系统之一 。 安装12docker run -d --name prometheus -p 9090:9090 -v \\/works/conf/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus 配置prometheus.yml相关配置如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133global: scrape_interval: 10s scrape_timeout: 10s evaluation_interval: 10mscrape_configs: - job_name: config_server #scrape_interval: 5s #scrape_timeout: 5s metrics_path: /ops/prometheus scheme: http #basic_auth: # username: admin # password: 123456 static_configs: - targets: ['192.168.63.21:8100'] labels: instance: 192.168.63.21 - targets: ['192.168.64.21:8100'] labels: instance: 192.168.64.21 - job_name: employee_server metrics_path: /ops/prometheus scheme: http static_configs: - targets: ['192.168.63.31:8078'] labels: instance: 192.168.63.31 - targets: ['192.168.64.31:8078'] labels: instance: 192.168.64.31 - job_name: hkcash_server metrics_path: /ops/prometheus scheme: http static_configs: - targets: ['192.168.63.21:8085'] labels: instance: 192.168.63.21 - targets: ['192.168.64.21:8085'] labels: instance: 192.168.64.21 - job_name: eureka_server metrics_path: /ops/prometheus scheme: http static_configs: - targets: ['192.168.63.31:8761'] labels: instance: 192.168.63.31 - targets: ['192.168.64.31:8761'] labels: instance: 192.168.64.31 - job_name: notify_server metrics_path: /ops/prometheus scheme: http static_configs: - targets: ['192.168.63.31:8086'] labels: instance: 192.168.63.31 - targets: ['192.168.64.31:8086'] labels: instance: 192.168.64.31 - job_name: tu_server metrics_path: /ops/prometheus scheme: http static_configs: - targets: ['192.168.63.11:45678'] labels: instance: 192.168.63.11 - targets: ['192.168.64.11:45678'] labels: instance: 192.168.64.11 - job_name: app_gateway metrics_path: /ops/prometheus scheme: http static_configs: - targets: ['192.168.63.11:8062'] labels: instance: 192.168.63.11 - targets: ['192.168.64.11:8062'] labels: instance: 192.168.64.11 - job_name: lms_webapp metrics_path: /ops/prometheus scheme: http static_configs: - targets: ['192.168.63.101:8079'] labels: instance: 192.168.63.101 - job_name: los_webapp metrics_path: /ops/prometheus scheme: http static_configs: - targets: ['192.168.63.101:8080'] labels: instance: 192.168.63.101 - job_name: cAdvisor static_configs: - targets: ['192.168.63.21:4194'] labels: container_group: 192.168.63.21 - targets: ['192.168.64.21:4194'] labels: container_group: 192.168.64.21 - targets: ['192.168.63.31:4194'] labels: container_group: 192.168.63.31 - targets: ['192.168.64.31:4194'] labels: container_group: 192.168.64.31 - targets: ['192.168.63.11:4194'] labels: container_group: 192.168.63.11 - targets: ['192.168.64.11:4194'] labels: container_group: 192.168.64.11 - targets: ['192.168.63.101:4194'] labels: container_group: 192.168.63.101 - targets: ['192.168.64.178:4194'] labels: container_group: 192.168.64.178 - targets: ['192.168.64.179:4194'] labels: container_group: 192.168.64.179 访问访问：http://ip:9090/graph 效果： grafnaGrafana 是一个开源的图表可视化系统，简言之，其特点在于图表配置比较方便、生成的图表漂亮。Prometheus + Grafana 监控系统的组合中，前者负责采样数据并存储这些数据；后者则侧重于形象生动的展示数据。 安装1docker run -d --name grafana -p 3000:3000 grafana/grafana 访问http://192.168.64.178:3000 默认登录账户密码都为admin 配置添加数据源 添加Templating job: instance: apis: 效果 添加panelmem12mem&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125;mem_free&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125; 效果 Heap123456heap&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125;heap_committed&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125;heap_used&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125;nonheap&#123;job=\"~[[job]]\",instance=\"~[[instance]]\"&#125;nonheap_committed&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125;nonheap_used&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125; Threads123threads&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125;threads_peak&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125;threads_daemon&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125; systemload_average1systemload_average&#123;job=~\"[[job]]\",instance=~\"[[instance]]\"&#125; Gauge_servo_response_api1&#123;job=~\"[[job]]\",instance=~\"[[instance]]\",__name__=~\"gauge_servo_response_api_.*\"&#125; 具体的配置文件参考prometheus.json 整体效果 容器监控通过cAdvisor收集docker日志，再通过prometheus在grafana中显示。 cAdvisor每台容器安装：12345678910docker run --restart=always \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:rw \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/dev/disk/:/dev/disk:ro \\ --publish=4194:8080 \\ --detach=true \\ --name=cadvisor \\ google/cadvisor:latest 访问：http://hostIp:4194/ prometheus配置123456789101112131415161718192021222324252627282930... - job_name: cAdvisor static_configs: - targets: ['192.168.63.21:4194'] labels: container_group: 192.168.63.21 - targets: ['192.168.64.21:4194'] labels: container_group: 192.168.64.21 - targets: ['192.168.63.31:4194'] labels: container_group: 192.168.63.31 - targets: ['192.168.64.31:4194'] labels: container_group: 192.168.64.31 - targets: ['192.168.63.11:4194'] labels: container_group: 192.168.63.11 - targets: ['192.168.64.11:4194'] labels: container_group: 192.168.64.11 - targets: ['192.168.63.101:4194'] labels: container_group: 192.168.63.101 - targets: ['192.168.64.178:4194'] labels: container_group: 192.168.64.178 - targets: ['192.168.64.179:4194'] labels: container_group: 192.168.64.179 grafana dashboard可以从grafana官网导入dashboard：https://grafana.com/dashboards 导入dashboard: 效果 hystrix-dashboardhystrixhystrix旨在通过控制那些访问远程系统、服务和第三方库的节点，从而对延迟和故障提供更强大的容错能力。Hystrix具备拥有回退机制和断路器功能的线程和信号隔离，请求缓存和请求打包（request collapsing，即自动批处理），以及监控和配置等功能。 添加依赖：123456789&lt;!-- /hystrix.stream需要用到spring-boot-starter-actuator --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 启动类中添加：123@EnableEurekaClient@EnableHystrix@SpringBootApplication dashboardHystrix-dashboard是一款针对Hystrix进行实时监控的工具，通过Hystrix Dashboard我们可以在直观地看到各Hystrix Command的请求响应时间, 请求成功率等数据。 添加依赖：1234567891011121314&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt; turbine但是只使用Hystrix Dashboard的话, 你只能看到单个应用内的服务信息, 这明显不够. 我们需要一个工具能让我们汇总系统内多个服务的数据并显示到Hystrix Dashboard上, 这个工具就是Turbine. 添加依赖：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-netflix-turbine&lt;/artifactId&gt;&lt;/dependency&gt; 启动类添加：1234@EnableHystrixDashboard@EnableTurbine@EnableEurekaClient@SpringBootApplication 配置1234567891011121314151617181920212223242526272829303132333435363738394041server: port: 8050management: context-path: /ops security: enabled: false spring: application: name: hystrix-dashboardeureka: instance: preferIpAddress: true # eureka的ip，如果有多个ip地址时，需要在此处指定# ipAddress: 127.0.0.1 # 续约到期时间（默认90秒） leaseExpirationDurationInSeconds: 30 #续约更新时间间隔（默认30秒），在生产中，最好坚持使用默认值，因为在服务器内部有一些计算，他们对续约做出假设。 leaseRenewalIntervalInSeconds: 10 # default: $&#123;spring.cloud.client.hostname&#125;:$&#123;spring.application.name&#125;:$&#123;spring.application.instance_id:$&#123;server.port&#125;&#125;# instance-id: $&#123;spring.application.name&#125;:$&#123;eureka.instance.ipAddress&#125;:$&#123;server.port&#125; statusPageUrlPath: $&#123;management.context-path&#125;/info healthCheckUrlPath: $&#123;management.context-path&#125;/health metadata-map: management.context-path: $&#123;management.context-path&#125; client: registerWithEureka: false serviceUrl: defaultZone: http://localhost:8761/eureka/turbine: appConfig: app-gateway,hkcash-server,tu-server #turbine需要聚合的集群名称，需要在对应的服务的，通过 http://localhost:8050/turbine.stream?cluster=default 访问 #aggregator.clusterConfig: hkcash aggregator.clusterConfig: default instanceUrlSuffix: $&#123;management.context-path&#125;/hystrix.stream #获取集群名表达式，这里表示获取元数据中的cluster数据，在lms的配置文件中配置对应信息 #clusterNameExpression: metadata['cluster'] clusterNameExpression: new String(\"default\") 效果： sleuth zipkinspring cloud sleuth是从google的dapper论文的思想实现的，提供了对spring cloud系列的链路追踪。 目的： 提供链路追踪。通过sleuth可以很清楚的看出一个请求都经过了哪些服务。可以很方便的理清服务间的调用关系。 可视化错误。对于程序未捕捉的异常，可以在zipkin界面上看到。 分析耗时。通过sleuth可以很方便的看出每个采样请求的耗时，分析出哪些服务调用比较耗时。当服务调用的耗时随着请求量的增大而增大时，也可以对服务的扩容提供一定的提醒作用。 优化链路。对于频繁地调用一个服务，或者并行地调用等，可以针对业务做一些优化措施。 应用程序集成sleuth+log添加依赖：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;&lt;/dependency&gt; 这种方式只需要引入jar包即可。如果配置log4j，这样会在打印出如下的日志：122017-04-08 23:56:50.459 INFO [bootstrap,38d6049ff0686023,d1b8b0352d3f6fa9,false] 8764 — [nio-8080-exec-1] demo.JpaSingleDatasourceApplication : Step 2: Handling print2017-04-08 23:56:50.459 INFO [bootstrap,38d6049ff0686023,d1b8b0352d3f6fa9,false] 8764 — [nio-8080-exec-1] demo.JpaSingleDatasourceApplication : Step 1: Handling home 比原先的日志多出了 [bootstrap,38d6049ff0686023,d1b8b0352d3f6fa9,false] 这些内容，[appname,traceId,spanId,exportable]。 appname：服务名称traceId\\spanId：链路追踪的两个术语，后面有介绍exportable:是否是发送给zipkin sleuth+zipkin+httpsleuth收集跟踪信息通过http请求发给zipkin。这种需要启动一个zipkin,zipkin用来存储数据和展示数据。 添加依赖：12345678&lt;!--&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;&lt;/dependency&gt;--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;&lt;/dependency&gt; 配置：1234567spring: sleuth: sampler: percentage: 1.0 zipkin: enabled: true base-url: http://localhost:9411/ sletuh+streaming+zipkin这种方式通过spring cloud streaming将追踪信息发送到zipkin。spring cloud streaming目前只有kafka和rabbitmq的binder。以rabbitmq为例： 添加依赖：12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-stream&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; 配置：12345678910111213spring: sleuth: stream: enabled: true sampler: percentage: 1 #for sleuth rabbitmq: host: 192.168.99.100 port: 5672 username: guest password: guest# virtual-host: cloud_host zipkin-serverZipkin 是 Twitter 的一个开源项目，允许开发者收集Twitter各个服务上的监控数据，并提供查询接口。 安装zipkin-server采用spring方式安装： http方式添加依赖：123456789&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 启动类添加：12@EnableZipkinServer@SpringBootApplication stream方式添加依赖：12345678910111213&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 启动类添加：12@EnableZipkinStreamServer@SpringBootApplication 配置：1234567spring: #@EnableZipkinStreamServer时使用 rabbitmq: host: 192.168.100.88 port: 5672 username: guest password: guest elasticsearch安装如果容器内部通讯没有打通的话，需要采用以下方式部署：1234567891011121314151617tee elasticsearch.yml &lt;&lt; EOFnetwork.host: 192.168.64.179discovery.zen.minimum_master_nodes: 1EOF docker run --privileged=true --net=host -d -h elasticsearch --restart=always --name elasticsearch \\-p 9200:9200 -p 9300:9300 \\-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\-v \"$PWD/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml\" \\elasticsearch:2.4.4如果想将存储指到容器外，可以映射：#-v \"$PWD/config\":/usr/share/elasticsearch/config \\#-v \"$PWD/esdata\":/usr/share/elasticsearch/data \\#indexcurl http://192.168.99.100:9200/_cat/indices?v 如果有打通容器内部通讯或者与zipkin-server部署在同一台机器上，则安装比较简单： 1234docker run -d -h elasticsearch --restart=always --name elasticsearch \\-p 9200:9200 -p 9300:9300 \\-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\elasticsearch:2.4.4 rabbitmq日志采集是通过sleuth-zipkin-stream方式收集的，此处采用rabbitmq。可以采用docker方式安装： 1234567891011docker run -d -h rabbitmq --restart=always --name rabbitmq -p 5672:5672 -p 15672:15672 \\rabbitmq:3.6.6 # login:guest/guest # url:http://ip:15672docker exec -it rabbitmq rabbitmq-plugins enable rabbitmq_management#docker run -d --hostname my-rabbit --name some-rabbit -p 8080:15672 rabbitmq:3-management#docker run -d --hostname my-rabbit --name some-rabbit -e RABBITMQ_DEFAULT_USER=user -e RABBITMQ_DEFAULT_PASS=password rabbitmq:3-management# list_queues#docker exec -it rabbitmq rabbitmqctl list_queues 数据存储Mem内存方式，只适合于测试环境： 配置：123zipkin: storage: type: mem MySQL添加依赖：123456789101112&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-storage-mysql&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt; 配置：12345678910111213141516spring: application: name: zipkin-server datasource: url: jdbc:mysql://192.168.100.88:3308/zipkin?autoReconnect=true username: root password: helloworld driver-class-name: com.mysql.jdbc.Driver# schema: classpath:/mysql.sql# initialize: true# continue-on-error: truezipkin: storage: type: mysql SQL文件 Elasticsearch添加依赖：123456789&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-storage-elasticsearch&lt;/artifactId&gt; &lt;version&gt;1.19.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;&lt;/dependency&gt; 配置：123456789zipkin: storage: type: elasticsearch elasticsearch: cluster: elasticsearch hosts: 192.168.108.183:9300 index: zipkin# index-shards: $&#123;ES_INDEX_SHARDS:5&#125;# index-replicas: $&#123;ES_INDEX_REPLICAS:1&#125; zipkin-dependencies如果为非mem方式部署的zipkin-server，zipkin-dependencies是没有数据的，需要加入zipkin-dependencies模块才能正常显示。以elasticsearch为例： 添加依赖：12345&lt;dependency&gt; &lt;groupId&gt;io.zipkin.dependencies&lt;/groupId&gt; &lt;artifactId&gt;zipkin-dependencies-elasticsearch&lt;/artifactId&gt; &lt;version&gt;1.5.4&lt;/version&gt;&lt;/dependency&gt; 添加代码：123456789101112131415161718192021@Componentpublic class ElasticsearchDependenciesTask &#123; private Logger logger = LoggerFactory.getLogger(this.getClass()); @Value(\"$&#123;spark.es.nodes&#125;\") private String esNodes; @Value(\"$&#123;spark.driver.allowMultipleContexts&#125;\") private String allowMultipleContexts; @Scheduled(cron = \"0 */5 * * * ?\") public void run() throws Exception &#123;// -e ES_HOSTS=\"192.168.108.183:9200\" Map&lt;String, String&gt; envs = Maps.newHashMap(); envs.put(\"spark.driver.allowMultipleContexts\",allowMultipleContexts); envs.put(\"ES_HOSTS\",esNodes); EnvUtils.setEnv(envs); ElasticsearchDependenciesJob.builder().build().run(); &#125; &#125; 配置：123456# dependencies-elasticsearch配置spark: driver: allowMultipleContexts: true es: nodes: 192.168.108.183:9200 内部是通过spark进行数据分析，再生成对应的dependencies数据。 当然也可以通过docker部署：12345docker run --rm --name zipkin-dependencies \\-e STORAGE_TYPE=elasticsearch \\-e ES_HOSTS=192.168.108.183:9200 \\-e \"JAVA_OPTS=-Xms128m -Xmx128m\" \\openzipkin/zipkin-dependencies:1.5.4 这个只会运行一次后退出，如果需要定时执行的话，需要加入到cron中。 效果 可能sleuth收集了很多你不想要的接口请求，可能通过以下配置排除掉：1234567spring: sleuth: web: skip-pattern: /js/.*|/css/.*|/html/.*|/htm/.*|/static/.*|/ops/.*|/api-docs.*|/swagger.*|.*\\.png|.*\\.gif|.*\\.css|.*\\.js|.*\\.html|/favicon.ico|/myhealth scheduled: enabled: false #skip-pattern: .*RedisOperationsSessionRepository jaegerzipkin的效果不太好，可以考虑使用jaeger，由Uber开源。Jaeger兼容OpenTracing的数据模型和instrumentation库，能够为每个服务/端点使用一致的采样方式。 分布式系统调用过程: opentracing 协议opentracing是一套分布式追踪协议，与平台，语言无关，统一接口，方便开发接入不同的分布式追踪系统。 简单理解opentracing: 一个完整的opentracing调用链包含 Trace + span + 无限极分类: Trace：追踪对象，一个Trace代表了一个服务或者流程在系统中的执行过程，如：test.com，redis，mysql等执行过程。一个Trace由多个span组成 span：记录Trace在执行过程中的信息，如：查询的sql，请求的HTTP地址，RPC调用，开始、结束、间隔时间等。无限极分类：服务与服务之间使用无限极分类的方式，通过HTTP头部或者请求地址传输到最低层，从而把整个调用链串起来。 安装可以通过docker-compose安装，请参考docker-compose123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384---version: &apos;2&apos;services: els: image: docker.elastic.co/elasticsearch/elasticsearch:6.0.0 restart: always container_name: els hostname: els networks: - elastic-jaeger environment: #- bootstrap.memory_lock=true - ES_JAVA_OPTS=-Xms512m -Xmx512m ports: - &quot;9200:9200&quot; - &quot;9300:9300&quot; volumes: - ./config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml kibana: image: docker.elastic.co/kibana/kibana:6.0.0 ports: - &quot;5601:5601&quot; environment: ELASTICSEARCH_URL: http://els:9200 depends_on: - els networks: - elastic-jaeger jaeger-collector: environment: - SPAN_STORAGE_TYPE=elasticsearch image: jaegertracing/jaeger-collector:latest ports: - &quot;14267:14267&quot; - &quot;14268:14268&quot; - &quot;9411:9411&quot; depends_on: - els container_name: jaeger-collector hostname: jaeger-collector restart: unless-stopped networks: - elastic-jaeger command: [&quot;/go/bin/collector-linux&quot;, &quot;--span-storage.type=elasticsearch&quot;, &quot;--es.server-urls=http://els:9200&quot;] jaeger-agent: image: jaegertracing/jaeger-agent:latest ports: - &quot;5775:5775/udp&quot; - &quot;5778:5778&quot; - &quot;6831:6831/udp&quot; - &quot;6832:6832/udp&quot; depends_on: - els - jaeger-collector restart: unless-stopped container_name: jaeger-agent hostname: jaeger-agent networks: - elastic-jaeger command: [&quot;/go/bin/agent-linux&quot;, &quot;--collector.host-port=jaeger-collector:14267&quot;] jaeger-query: environment: - SPAN_STORAGE_TYPE=elasticsearch image: jaegertracing/jaeger-query:latest ports: - 16686:16686 depends_on: - els - jaeger-collector restart: unless-stopped container_name: jaeger-query hostname: jaeger-query networks: - elastic-jaeger command: [&quot;/go/bin/query-linux&quot;, &quot;--span-storage.type=elasticsearch&quot;, &quot;--es.server-urls=http://els:9200&quot;, &quot;--es.sniffer=false&quot;, &quot;--query.static-files=/go/jaeger-ui/&quot;, &quot;--log-level=debug&quot;]volumes: esdata1: driver: local eslog: driver: localnetworks: elastic-jaeger: driver: bridge elasticsearch1234567891011121314mkdir -p /works/conf/elasticsearchtee /works/conf/elasticsearch/elasticsearch.yml &lt;&lt; EOFxpack.security.enabled: falsenetwork.host: 0.0.0.0thread_pool.bulk.queue_size: 1000discovery.zen.minimum_master_nodes: 1EOFdocker run -d --name elasticsearch \\-p 9200:9200 -p 9300:9300 \\-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\-v \"/works/conf/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml\" \\docker.elastic.co/elasticsearch/elasticsearch:6.0.0#elasticsearch:5.2.1 jaeger-collector1234567docker run -d --name jaeger-collector \\-p 14267:14267 \\-p 14268:14268 \\-p 9411:9411 \\-e \"SPAN_STORAGE_TYPE=elasticsearch\" \\jaegertracing/jaeger-collector \\/go/bin/collector-linux --es.server-urls=http://192.168.108.1:9200 jaeger-query12345docker run -d --name jaeger-query \\ -p 16686:16686 \\ -e \"SPAN_STORAGE_TYPE=elasticsearch\" \\ jaegertracing/jaeger-query \\ /go/bin/query-linux --es.server-urls=http://192.168.108.1:9200 --query.static-files=/go/jaeger-ui/ kibana1234docker run -d --name jaeger-kibana \\ -p 5601:5601 \\ -e \"ELASTICSEARCH_URL=http://192.168.108.1:9200\" \\ docker.elastic.co/kibana/kibana:6.0.0 jaeger-agent1234567docker run -d --name jaeger-agent \\ -p5775:5775/udp \\ -p6831:6831/udp \\ -p6832:6832/udp \\ -p5778:5778/tcp \\ jaegertracing/jaeger-agent \\ /go/bin/agent-linux --discovery.min-peers=1 --collector.host-port=192.168.108.1:14267 spark-dependencies参考： https://github.com/jaegertracing/spark-dependencies 测试好好久，docker不能分析出对应的依赖关系，用jar就可以。找不到问题所在。只能用jar包: 12345git clone https://github.com/jaegertracing/spark-dependencies.gitcd spark-dependencies./mvnw clean install -Dmaven.test.skip=truecd ./jaeger-spark-dependencies/target/STORAGE=elasticsearch ES_NODES=http://192.168.108.1:9200 java -jar jaeger-spark-dependencies-0.0.1-SNAPSHOT.jar 以下是doker的安装方式： 12345docker run -it --rm --name spark-dependencies \\-e STORAGE=elasticsearch \\-e ES_NODES=http://192.168.108.1:9200 \\-e \"JAVA_OPTS=-Xms1g -Xmx1g\" \\jaegertracing/spark-dependencies 也可以自己写Dockerfile 1234567891011121314FROM java:8-jdkMAINTAINER dave.zhao@aeasycredit.comRUN mkdir /appWORKDIR /appENV APPNAME=jaeger-spark-dependenciesENV VERSION=0.0.1-SNAPSHOTRUN ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo Asia/Shanghai &gt; /etc/timezoneCOPY $&#123;APPNAME&#125;-$&#123;VERSION&#125;.jar /app/ENTRYPOINT [\"sh\", \"-c\", \"STORAGE=$&#123;STORAGE&#125; ES_NODES=$&#123;ES_NODES&#125; java $&#123;JAVA_OPTS&#125; -Djava.security.egd=file:/dev/./urandom -jar /app/$&#123;APPNAME&#125;-$&#123;VERSION&#125;.jar\"] 客户端集成以spring-cloud为例，添加以下依赖： 参考： https://github.com/opentracing-contrib/metahttps://github.com/opentracing-contrib/java-spring-cloudhttps://github.com/opentracing-contrib/java-spring-jaeger 普通spring-web项目使用：https://github.com/opentracing-contrib/java-spring-web 添加依赖： 1234567891011&lt;dependency&gt; &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt; &lt;artifactId&gt;opentracing-spring-cloud-starter&lt;/artifactId&gt; &lt;version&gt;0.1.13&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt; &lt;artifactId&gt;opentracing-spring-jaeger-starter&lt;/artifactId&gt; &lt;version&gt;0.1.1&lt;/version&gt;&lt;/dependency&gt; application.yml: 12345opentracing: jaeger: udp-sender: host: 192.168.108.1 port: 6831 skywalking针对分布式系统的APM（应用性能监控）系统，特别针对微服务、cloud native和容器化(Docker, Kubernetes, Mesos)架构， 其核心是个分布式追踪系统。 安装安装elasticsearch： 12345678910111213mkdir -p /works/conf/elasticsearchtee /works/conf/elasticsearch/elasticsearch.yml &lt;&lt; EOFcluster.name: CollectorDBClusternetwork.host: 0.0.0.0thread_pool.bulk.queue_size: 1000discovery.zen.minimum_master_nodes: 1EOFdocker run -d --name elasticsearch \\-p 9200:9200 -p 9300:9300 \\-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\-v \"/works/conf/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml\" \\elasticsearch:5.3 optional: elasticsearch-head ui： 1docker run -d --name elasticsearch-head -p 9100:9100 mobz/elasticsearch-head:5 optional: kibana ui: 1234docker run -d --name kibana \\ -p 5601:5601 \\ -e \"ELASTICSEARCH_URL=http://192.168.64.178:9200\" \\ kibana:5.3 安装Collector与Web： 123wget http://apache.01link.hk/incubator/skywalking/5.0.0-beta/apache-skywalking-apm-incubating-5.0.0-beta.tar.gztar zxvf apache-skywalking-apm-incubating-5.0.0-beta.tar.gzcd apache-skywalking-apm-incubating 配置config/application.yml:1234567891011naming: jetty: host: 0.0.0.0 port: 10800 contextPath: /...agent_gRPC: gRPC: host: 192.168.108.1 port: 11800... 主要需要修改以上两个配置，不然分开部署的话访问不了。另外机器的时间也需要同步。 启动Collector与Web12cd apache-skywalking-apm-incubating/bin./startup.sh agent复制apache-skywalking-apm-incubating目录下的agent，需要保持目录结构不变。修改config/agent.config： 1234...agent.application_code=app-gateway...collector.servers=192.168.108.1:10800 在启动时加入-javaagent即可： 1java -javaagent:/agent/skywalking-agent.jar -jar xxx.jar 也可以在启动中覆盖agent.config中的agent.application_code或collector.servers参数，注意：一定要以skywalking.开头，详见Setting-override： 1java -javaagent:/agent/skywalking-agent.jar -Dskywalking.agent.application_code=app-gateway -jar xxx.jar 默认情况下会收集除了agent.ignore_suffix参数中以这些后缀结尾的链接，但这个不能满足其他的排除条件，可以通过可选插件apm-trace-ignore-plugin: 1234567#maven must be &gt; 3.1.0git clone https://github.com/apache/incubator-skywalking.gitcd incubator-skywalking/git submodule initgit submodule updatemvn clean package -DskipTestscd apm-sniffer/optional-plugins/trace-ignore-plugin 将apm-sniffer/optional-plugins/trace-ignore-plugin/apm-trace-ignore-plugin.config 复制到agent/config/ 目录下，加上配置： 1trace.ignore_path=/eureka/**,Mysql/JDBI/**,Hystrix/**,/swagger-resources/** 将apm-trace-ignore-plugin-x.jar拷贝到agent/plugins后，重启探针即可生效。 参考 http://tech.lede.com/2017/04/19/rd/server/SpringCloudSleuth/https://segmentfault.com/a/1190000008629939https://mykite.github.io/2017/04/21/zipkin%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E5%8F%8A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%80%EF%BC%89/https://github.com/jukylin/blog/blob/master/Uber%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%BD%E8%B8%AA%E7%B3%BB%E7%BB%9FJaeger%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D%E5%92%8C%E6%A1%88%E4%BE%8B%E3%80%90PHP%20%20%20Hprose%20%20%20Go%E3%80%91.mdhttps://github.com/jaegertracing/jaegerhttps://github.com/opentracing-contrib/java-spring-cloudhttps://github.com/opentracing-contrib/java-spring-jaegerhttps://github.com/opentracing-contrib/java-spring-zipkinhttps://github.com/jaegertracing/spark-dependencieshttps://my.oschina.net/u/2548090/blog/1821372","categories":[{"name":"spring cloud","slug":"spring-cloud","permalink":"http://blog.gcalls.cn/categories/spring-cloud/"},{"name":"docker","slug":"spring-cloud/docker","permalink":"http://blog.gcalls.cn/categories/spring-cloud/docker/"},{"name":"prometheus","slug":"spring-cloud/docker/prometheus","permalink":"http://blog.gcalls.cn/categories/spring-cloud/docker/prometheus/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.gcalls.cn/tags/docker/"},{"name":"spring cloud","slug":"spring-cloud","permalink":"http://blog.gcalls.cn/tags/spring-cloud/"},{"name":"prometheus","slug":"prometheus","permalink":"http://blog.gcalls.cn/tags/prometheus/"}]},{"title":"Kubernetes安装Docker registry","slug":"Kubernetes安装Docker-registry","date":"2017-10-13T08:06:48.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2017/10/Kubernetes安装Docker-registry.html","link":"","permalink":"http://blog.gcalls.cn/2017/10/Kubernetes安装Docker-registry.html","excerpt":"记录一下Kubernetes安装Docker registry的过程。","text":"记录一下Kubernetes安装Docker registry的过程。 生成证书参考http://blog.gcalls.cn/blog/2017/01/Docker学习总结.html#证书安装方式 12345openssl req \\ -subj \"/C=CN/ST=GuangDong/L=ShenZhen/CN=registry.gcalls.cn\" \\ -reqexts SAN -config &lt;(cat /etc/pki/tls/openssl.cnf &lt;(printf \"[SAN]\\nsubjectAltName=DNS:www.abc.com,IP:192.168.10.6\")) \\ -newkey rsa:4096 -nodes -sha256 -keyout domain.key \\ -x509 -days 365 -out domain.crt ConfigMap将domain.crt与domain.key通过configmap方式mount在容器中： registry-configMap.yaml 12345678910111213141516apiVersion: v1kind: ConfigMapmetadata: name: registry-configmap namespace: kube-system labels: app: registry-configmapdata: domain.crt: | -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- domain.key: | -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- PVregistry-pv.yaml 1234567891011121314151617apiVersion: v1kind: PersistentVolumemetadata: name: kube-system-kube-registry-pv labels: kubernetes.io/cluster-service: \"true\"spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: /data #nfs: # path: /data/k8s # server: 192.168.12.171 persistentVolumeReclaimPolicy: Recycle PVCregistry-pvc.yaml 12345678910111213apiVersion: v1kind: PersistentVolumeClaimmetadata: name: kube-registry-pvc namespace: kube-system labels: kubernetes.io/cluster-service: \"true\"spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi registry-dsregistry-ds.yaml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758apiVersion: apps/v1beta1kind: Deploymentmetadata: name: kube-registry-v0 namespace: kube-system labels: k8s-app: kube-registry version: v0 kubernetes.io/cluster-service: \"true\"spec: replicas: 1 selector: matchLabels: k8s-app: kube-registry version: v0 template: metadata: labels: k8s-app: kube-registry version: v0 kubernetes.io/cluster-service: \"true\" spec: containers: - name: registry image: registry:2 resources: # keep request = limit to keep this container in guaranteed class limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi env: - name: REGISTRY_HTTP_ADDR value: :5000 - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY value: /var/lib/registry - name: REGISTRY_HTTP_TLS_CERTIFICATE value: /certs/domain.crt - name: REGISTRY_HTTP_TLS_KEY value: /certs/domain.key volumeMounts: - name: image-store mountPath: /var/lib/registry - name: cert-volume mountPath: /certs ports: - containerPort: 5000 name: registry protocol: TCP volumes: - name: image-store persistentVolumeClaim: claimName: kube-registry-pvc - name: cert-volume configMap: name: registry-configmap registry-svcregistry-svc.yaml 123456789101112131415161718apiVersion: v1kind: Servicemetadata: name: kube-registry namespace: kube-system labels: k8s-app: kube-registry kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"KubeRegistry\"spec: selector: k8s-app: kube-registry type: NodePort ports: - name: registry port: 5000 nodePort: 30009 protocol: TCP 创建12345kubectl create -f registry-configMap.yamlkubectl create -f registry-pv.yamlkubectl create -f registry-pvc.yamlkubectl create -f registry-ds.yamlkubectl create -f registry-svc.yaml 参考registry.zip 客户端如果要用docker pull或者docker push的客户端，都需要执行以下命令：12mkdir -p /etc/docker/certs.d/192.168.10.6:30009cp domain.crt /etc/docker/certs.d/192.168.10.6:30009/ca.crt 否则，会报以下错误：1Error response from daemon: Get https://192.168.10.6:30009/v1/_ping: x509: certificate signed by unknown authority 测试123docker pull hello-worlddocker tag hello-world 192.168.10.6:30009/hello-worlddocker push 192.168.10.6:30009/hello-world 异常如测试出现：Get https://192.168.10.6:30009/v1/_ping: net/http: TLS handshake timeout有可以本地与docker开启了代理，需要关闭docker代理或者将ip添加到NO_PROXY中，文件位于：1/etc/systemd/system/docker.service.d/http-proxy.conf Get https://192.168.10.6:30009/v1/_ping: x509: cannot validate certificate for 192.168.10.6 because it doesn’t contain any IP SANs这个是由于CN为registry.gcalls.cn，但通过ip，需要添加SAN信息：先/etc/pki/tls/openssl.cnf配置，在该文件中找到[ v3_ca ]，在它下面添加如下内容：123[ v3_ca ]# Extensions for a typical CAsubjectAltName = IP:192.168.10.6 也可以直接在创建crt时，传-reqexts SAN参数。 参考 https://github.com/kubernetes/kubernetes/blob/v1.7.5/cluster/addons/registry/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.gcalls.cn/categories/Kubernetes/"},{"name":"docker","slug":"Kubernetes/docker","permalink":"http://blog.gcalls.cn/categories/Kubernetes/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.gcalls.cn/tags/docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.gcalls.cn/tags/Kubernetes/"}]},{"title":"Minikube安装与配置","slug":"Minikube安装与配置","date":"2017-09-25T09:08:32.000Z","updated":"2024-08-02T05:39:00.963Z","comments":true,"path":"/2017/09/Minikube安装与配置.html","link":"","permalink":"http://blog.gcalls.cn/2017/09/Minikube安装与配置.html","excerpt":"在容器编排工具中安装配置最复杂的就是Kubernetes，想要运行一个简单的容器集群环境，对于没有使用过Kubernetes的人来说，需要花费一定的时间去理解各组件的概念和功能，再做大量的安装配置工作才能运行一个kubernetes集群。 从Kubernetes1.3开始提供了一个叫Minikube的强大测试工具，可以在任意主机上运行单节点的小型集群，这个工具默认安装和配置了一个Linux VM，Docker和Kubernetes的相关组件，并且提供Dashboard。目前支持在Linux, OS X及Windows上安装，今天我们介绍的是在Windows上如何安装Minitube。 2020-12-10更新：Windows10下建议直接使用Docker-Desktop On WSL2，不要开启systemd。具体参考：http://blog.gcalls.cn/blog/2020/12/WSL.html","text":"在容器编排工具中安装配置最复杂的就是Kubernetes，想要运行一个简单的容器集群环境，对于没有使用过Kubernetes的人来说，需要花费一定的时间去理解各组件的概念和功能，再做大量的安装配置工作才能运行一个kubernetes集群。 从Kubernetes1.3开始提供了一个叫Minikube的强大测试工具，可以在任意主机上运行单节点的小型集群，这个工具默认安装和配置了一个Linux VM，Docker和Kubernetes的相关组件，并且提供Dashboard。目前支持在Linux, OS X及Windows上安装，今天我们介绍的是在Windows上如何安装Minitube。 2020-12-10更新：Windows10下建议直接使用Docker-Desktop On WSL2，不要开启systemd。具体参考：http://blog.gcalls.cn/blog/2020/12/WSL.html 准备以Windows为例，记录minikube的安装与使用。 Minitube项目地址：https://github.com/kubernetes/minikube Minikube要求在BIOS中开户了VT-x/AMD-v虚拟化。 Kubernetes版本： v1.19.4 安装virtualbox安装版本为VirtualBox 6.1.x 安装choco在Linux世界中，安装一个软件不需要在浏览器中寻找软件的官网，然后将其下载下来，然后双击进行安装。只需要一条简单的命令，就可以完成搜索、安装、更新、卸载等所有操作。其实Windows下,也有这么一个包管理器，功能虽然不及Linux中那些包管理器强大，但是也让Windows下的软件安装方便了不少。这就是Chocolatey。 Chocolatey官网：https://chocolatey.org/ 安装choco： 以管理员模式开启PowerShell：123Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))setx ChocolateyInstall E:\\Chocolatey /M 可以测试是否能正常安装curl： 123456C:\\&gt;choco install curl -y#测试：C:\\&gt;curl baidu.com&lt;html&gt;&lt;meta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"&gt;&lt;/html&gt; 安装minikubeWindows以管理员模式开启CMD：1234C:\\&gt;choco install minikube -yC:\\&gt;minikube versionminikube version: v1.19.4 如果安装速度慢的话，可以开启代理： 12SET HTTP_PROXY=http://192.168.101.175:1082SET HTTPS_PROXY=http://192.168.101.175:1082 MacOS下载对应的文件： 123curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/darwin/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ Linux推荐使用microk8s安装，参考：http://blog.gcalls.cn/blog/2020/12/Kubernetes-Development-Environment.html#microk8s 1234567891011121314151617181920212223242526yum install bash-completion -y ~/.bash_profilealias k=kubectlsource &lt;(kubectl completion bash | sed s/kubectl/k/g)source /usr/share/bash-completion/bash_completioncul -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kubectlchmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/#启动#使用driver=none时，不需要安装VirtualBox，但需要安装dockersudo minikube start --driver=none#Base on VirtualBox:yum localinstall VirtualBox-6.1-6.1.16_140961_el7-1.x86_64.rpmyum localinstall kernel-devel-3.10.0-957.el7.x86_64.rpm rcvboxdrv setupsu - dev#Starting minikube#https://minikube.sigs.k8s.io/docs/handbook/vpn_and_proxy/export http_proxy=\"http://192.168.101.175:1082\"export https_proxy=$http_proxyexport no_proxy=\"127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net,*.aliyun.com,*.163.com,*.docker-cn.com,registry.gcalls.cn\"minikube start --memory=8192 --cpus=4 --kubernetes-version=v1.19.4 WSL参考：http://blog.gcalls.cn/blog/2020/12/WSL.html 安装docker，参考：http://blog.gcalls.cn/blog/2018/12/ubuntu-os.html#Docker minikube安装： 参考：https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/#minikube-kubernetes-from-everywhere 12345678910111213141516171819202122232425# Download the latest version of Minikubecurl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64chmod +x ./minikube &amp;&amp; sudo mv minikube /usr/local/bin/curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.19.4/bin/linux/amd64/kubectlchmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/# Check if the KUBECONFIG is not setecho $KUBECONFIG# Check if the .kube directory is created &gt; if not, no need to create itls $HOME/.kube# Check if the .minikube directory is created &gt; if yes, delete itls $HOME/.minikube# Create the cluster with sudosudo sysctl fs.protected_regular=0export http_proxy=\"http://192.168.101.175:1082\"export https_proxy=$http_proxyexport no_proxy=\"127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net,*.aliyun.com,*.163.com,*.docker-cn.com,registry.gcalls.cn\"sudo minikube start --driver=none#每次执行以上命令均需要执行以下命令才行（root账户下不需要），否则会报以下错误：#Unable to connect to the server: dial tcp 172.21.114.172:8443: connect: no route to hostsudo cp -a /root/.kube /root/.minikube $HOMEsudo chown -R $USER $HOME/.kube $HOME/.minikube#需要修改以上的$HOME/.kube/config中的/root为当前用户目录sed -i 's;/root/;/home/dave/;g' ~/.kube/config#minikube start --driver=docker minikube常用命令启动minikube： 12345678910111213export http_proxy=\"http://192.168.101.175:1082\"export https_proxy=$http_proxyexport no_proxy=\"127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net\"C:\\&gt;minikube start --memory=8192 --cpus=4 --kubernetes-version=v1.19.4Starting local Kubernetes v1.7.5 cluster...Starting VM...Getting VM IP address...Moving files into cluster...Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster. 如果是Hyper-V的话： 最新：新版本的minikube不再需要执行以下操作。以下只作备份记录而也。 注意：需要先创建外部网络，具体请参考https://docs.microsoft.com/en-us/windows-server/virtualization/hyper-v/get-started/create-a-virtual-switch-for-hyper-v-virtual-machines 12minikube.exe start --cpus 2 --memory 2048 --vm-driver=\"hyperv\" --hyperv-virtual-switch=\"OuterNetwork\" --docker-env HTTP_PROXY=http://192.168.10.1:1080 --docker-env HTTPS_PROXY=http://192.168.10.1:1080minikube start --driver=none --docker-env HTTP_PROXY=http://192.168.101.175:1082 --docker-env HTTPS_PROXY=http://192.168.101.175:1082 注意：要加上代理，否则下载不了grc.io中的镜像。 停止minikube： 1C:\\&gt;minikube stop 删除minikube： 1C:\\&gt;minikube delete 测试minikube： 123C:\\&gt;kubectl get noNAME STATUS AGE VERSIONminikube Ready 2h v1.19.4 登录minikube虚拟机： 1C:\\&gt;minikube ssh 查看状态： 1234C:\\&gt;minikube statusminikube: Runningcluster: Runningkubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100 安装一个程序参考https://kubernetes.io/zh/docs/setup/learning-environment/minikube/ DashboardMinikube自带了Kubernetes Dashboard。要浏览这个界面，可以使用内置的minikube dashboard命令：12C:\\&gt; minikube dashboardOpening kubernetes dashboard in default browser... 参考https://github.com/kubernetes/minikubehttps://kubernetes.io/docs/getting-started-guides/minikube/https://kubernetes.io/zh/docs/setup/learning-environment/minikube/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/tags/kubernetes/"}]},{"title":"kubernetes学习总结","slug":"kubernetes学习总结","date":"2017-09-09T11:09:16.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/09/kubernetes学习总结.html","link":"","permalink":"http://blog.gcalls.cn/2017/09/kubernetes学习总结.html","excerpt":"本文记录一下kubernetes的常用功能。","text":"本文记录一下kubernetes的常用功能。 DeploymentKubernetes Deployment提供了官方的用于更新Pod和Replica Set（下一代的Replication Controller）的方法，您可以在Deployment对象中只描述您所期望的理想状态（预期的运行状态），Deployment控制器为您将现在的实际状态转换成您期望的状态，例如，您想将所有的webapp:v1.0.9升级成webapp:v1.1.0，您只需创建一个Deployment，Kubernetes会按照Deployment自动进行升级。现在，您可以通过Deployment来创建新的资源（pod，rs，rc），替换已经存在的资源等。 Deployment集成了上线部署、滚动升级、创建副本、暂停上线任务，恢复上线任务，回滚到以前某一版本（成功/稳定）的Deployment等功能，在某种程度上，Deployment可以帮我们实现无人值守的上线，大大降低我们的上线过程的复杂沟通、操作风险。 Deployment的使用场景 下面是Deployment的典型用例： 使用Deployment来启动（上线/部署）一个Pod或者ReplicaSet检查一个Deployment是否成功执行更新Deployment来重新创建相应的Pods（例如，需要使用一个新的Image）如果现有的Deployment不稳定，那么回滚到一个早期的稳定的Deployment版本暂停或者恢复一个Deployment kind: 定义的对象： Replicationcontroller, ReplicaSet Deployment 区别 Replicationcontroller 的升级版是 ReplicaSet , ReplicaSet支持基于集合的 Label selector, 而RC只支持基于等式的 Lable select Deployment其实就是内部调用 ReplicaSet. DaemonSet 根据标签指定pod 在那个服务器上运行，需要与nodeselect 公用。 server定义的selector 与 Deployment 中的 template 的 lables 对应：123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: tomcat-deploymentspec: replicas: 2 template: metadata: labels: app: tomcat tier: frontend spec: #hostNetwork: true containers: - name: tomcat image: tomcat:8.5.20 ports: - containerPort: 8080 #hostPort: 80 volumeMounts: - name: workdir mountPath: /opt volumes: - name: workdir emptyDir: &#123;&#125; #hostPath: # path: &quot;/data/works/tomcat/logs&quot;---apiVersion: v1kind: Servicemetadata: name: tomcat-serverspec: type: NodePort ports: - port: 11111 # cluster IP 的端口，也就是service的ip，默认为containerPort targetPort: 8080 # container容器的端口 nodePort: 30001 selector: tier: frontend# externalIPs: # - 192.168.10.6# - 192.168.10.7# - 192.168.10.8 外部系统访问service 问题kubernetes 中三种IP 包括 NodeIP node节点的IP地址 PodIP pod的IP地址 clusterIP service的IP地址 nodeIP 是kubernetes集群中每个节点的物理网卡的IP地址， client 访问kubernetes集群使用的IP地址。Pod ip地址 是更具创建的网络类型，网桥分配的IP地址。clusterIP 是一个虚拟的IP， cluster ip 仅作用于kubernetes service 这个对象， 是由kubernetes管理和分配ip地址，源于cluster ip地址池： 123[root@kubernetes nginx]# vim /etc/kubernetes/apiserver# Address range to use for servicesKUBE_SERVICE_ADDRESSES=\"--service-cluster-ip-range=10.254.0.0/16\" cluster IP 无法ping通， 因为没有一个实体网络对象响应； cluster ip 只能结合 service port 组成一个具体的通信接口，单独的cluster IP不具备tcp/ip通信基础； 如果 pod 对外访问，需要在servcie 中 指定 type 为 NodePort； 12345678910111213[root@k8s-master yaml]# kubectl describe service tomcat-serverName: tomcat-serverNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: tier=frontendType: NodePortIP: 10.100.53.62Port: &lt;unset&gt; 11111/TCPNodePort: &lt;unset&gt; 30002/TCPEndpoints: 10.244.36.10:8080,10.244.36.13:8080,10.244.36.7:8080 + 1 more...Session Affinity: NoneEvents: &lt;none&gt; 访问node IP ＋ node port ,可以访问页面。 nodeport 并没有完全解决外部访问service 的问题， 比如负载均衡问题，如果有10 pod 节点， 如果是用谷歌的GCE公有云，那么可以把 service type=NodePort 修改为 LoadBalancer。 另外也可以通过设置pod(daemonset) hostNetwork=true, 将pod中所有容器的端口号直接映射到物理机上， 设置hostNetwork=true的时候需要注意，如果不指定hostport，默认hostport 等于containerport, 如果指定了hostPort, 则hostPort 必须等于containerPort的值。 deployment创建部署命令行方式创建deployment类似于docker run方式：12345678[root@k8s-master ~]# kubectl create deploy --image=nginx nginx-app --port=80deployment.apps/nginx-app created[root@k8s-master ~]# kubectl get deploy nginx-app NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEnginx-app 1 1 1 1 1m[root@k8s-master ~]# kubectl get pod |grep nginx-appnginx-app-2778402574-zv4r6 1/1 Running 0 1m 以上实际上创建的是一个由deployment来管理的Pod。 kubectl run并不是直接创建一个Pod，而是先创建一个Deployment资源（replicas=1），再由与Deployment关联的ReplicaSet来自动创建Pod，这等价于这样一个配置：123456789101112131415161718192021222324252627282930apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: run: nginx-app name: nginx-app namespace: defaultspec: replicas: 1 selector: matchLabels: run: nginx-app strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: run: nginx-app spec: containers: - image: nginx name: nginx-app ports: - containerPort: 80 protocol: TCP dnsPolicy: ClusterFirst restartPolicy: Always 等到容器变成Running后，就可以用kubectl命令来操作它了，比如 kubectl get - 类似于docker ps，查询资源列表 kubectl describe - 类似于docker inspect，获取资源的详细信息 kubectl logs - 类似于docker logs，获取容器的日志 kubectl exec - 类似于docker exec，在容器内执行一个命令 也可以通过api访问：node:1https://192.168.10.6:6443/api/v1/nodes namespaces:1https://192.168.10.6:6443/api/v1/namespaces pods:1https://192.168.10.6:6443/api/v1/pods services:12https://192.168.10.6:6443/api/v1/serviceshttps://192.168.10.6:6443/api/v1/namespaces/default/services/nginx-app endpoint:12https://192.168.10.6:6443/api/v1/endpointshttps://192.168.10.6:6443/api/v1/namespaces/default/endpoints/nginx-app 创建Service前面虽然创建了Pod，但是在kubernetes中，Pod的IP地址会随着Pod的重启而变化，并不建议直接拿Pod的IP来交互。那如何来访问这些Pod提供的服务呢？使用Service。Service为一组Pod（通过labels来选择）提供一个统一的入口，并为它们提供负载均衡和自动服务发现。比如，可以为前面的nginx-app创建一个service：12345678910111213141516171819202122232425262728#kubectl create deployment hello-world --image=datawire/hello-world#kubectl expose deployment hello-world --type=LoadBalancer --port=8000$ kubectl expose deployment nginx-app --port=80 --target-port=80 --type=NodePortNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx-app NodePort 10.102.156.233 &lt;none&gt; 80:32295/TCP 2scurl 127.0.0.1:32295$ kubectl expose deployment nginx-app --port=8000 --target-port=80 --type=LoadBalancerNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx-app LoadBalancer 10.107.61.70 localhost 8000:31876/TCP 8m59s$ curl 127.0.0.1:8000#curl 127.0.0.1:31876 doesn't workservice \"nginx-app\" exposed$ kubectl describe service nginx-appName: nginx-appNamespace: defaultLabels: app=nginx-appAnnotations: &lt;none&gt;Selector: app=nginx-appType: LoadBalancerIP: 10.107.61.70LoadBalancer Ingress: localhostPort: &lt;unset&gt; 8000/TCPTargetPort: 80/TCPNodePort: &lt;unset&gt; 31876/TCPEndpoints: 10.1.0.126:80Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt; 该命令不能设置nodePort，如果需要指定nodePort，需要通过kubectl edit service nginx-app修改：12345678910...spec: clusterIP: 10.97.111.200 ports: - nodePort: 31798 port: 80 protocol: TCP targetPort: 80 nodePort: 32222... 测试：123456789101112[root@k8s-master ~]# kubectl get svcNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx-app 10.97.111.200 &lt;nodes&gt; 80:32222/TCP 10m[root@k8s-master ~]# kubectl run busybox --rm -ti --image=busybox --restart=Never /bin/sh#以上每次退出后会自动删除images中的镜像，每次执行都会重新下载image，所以每次执行都会有些慢。If you don't see a command prompt, try pressing enter./ # ping nginx-appPING nginx-app (10.97.111.200): 56 data bytes^C--- nginx-app ping statistics ---1 packets transmitted, 0 packets received, 100% packet loss 这样，在cluster内部就可以通过http://10.97.111.200和http://node-ip:32222来访问nginx-app。而在cluster外面，则只能通过http://node-ip:32222来访问。 deployment部署文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 2 template: metadata: labels: app: nginx tier: frontend spec: #hostNetwork: true containers: - name: nginx image: nginx:1.12.1 ports: - containerPort: 80 #hostPort: 80 volumeMounts: - name: workdir mountPath: /opt volumes: - name: workdir emptyDir: &#123;&#125; #emptyDir: # medium: Memory #hostPath: # path: \"/data/works/nginx/logs\"---apiVersion: v1kind: Servicemetadata: name: nginx-serverspec: type: NodePort ports: - port: 22222 # cluster IP 的端口 targetPort: 80 # container容器的端口 nodePort: 30002 selector: tier: frontend# externalIPs: # - 192.168.10.6# - 192.168.10.7# - 192.168.10.8 容易混淆的概念：1、NodePort和port 前者是将服务暴露给外部用户使用并在node上、后者则是为内部组件相互通信提供服务的，是在service上的端口。 2、targetPorttargetPort是pod上的端口，用来将pod内的container与外部进行通信的端口 3、port、NodePort、ContainerPort和targetPort在哪儿？ port在service上，负责处理对内的通信，clusterIP:port NodePort在node上，负责对外通信，NodeIP:NodePort ContainerPort在容器上，用于被pod绑定 targetPort在pod上、负责与kube-proxy代理的port和Nodeport数据进行通信 创建1kubectl create -f nginx.yaml 查看状态123[root@k8s-master yaml]# kubectl get deploy nginx -o wide NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE CONTAINER(S) IMAGE(S) SELECTORnginx 1 1 1 1 1d nginx nginx:1.12.1 app=nginx DESIRED 期望的副本数CURRENT 当前副本数UP-TO-DATA 最新副本数AVALLABLE 可用副本数 删除1kubectl delete -f nginx.yaml 如果没有原始的yaml，有两种方式可以删除： 先删除deployment:1234567[root@k8s-master ~]# kubectl get deployNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEnginx 1 1 1 1 30mkubectl delete deploy nginx#或者kubectl delete deployment/nginx 再删除service:12345[root@k8s-master ~]# kubectl get svcNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx 10.100.125.137 &lt;nodes&gt; 80:32222/TCP 10mkubectl delete service nginx 另外一种方式可以先把deployment与service生成yaml，再通过yaml文件生成：12kubectl get deploy,svc nginx-app -o yaml &gt; nginx-app.yamlkubectl delete -f nginx.yaml 使用RS管理PodReplica Set（简称RS）是k8s新一代的Pod controller。与RC相比仅有selector存在差异，RS支持了set-based selector（可以使用in、notin、key存在、key不存在四种方式来选择满足条件的label集合）。Deployment是基于RS实现的，我们可以使用kubectl get rs命令来查看Deployment创建的RS：123[root@k8s-master ~]# kubectl get rs NAME DESIRED CURRENT READY AGEnginx-app-2778402574 1 1 1 34m 由Deployment创建的RS的命名规则为”&lt;Deployment名称&gt;-“。 更新部署（镜像升级）：把image镜像从 nginx:1.12.1 升级到 nginx:1.13kubectl set image deployment/tomcat-deployment nginx=nginx:1.13 直接使用edit修改1kubectl edit deployment/nginx-deployment 扩展副本数1kubectl scale deployment nginx-deployment --replicas=3 kubernetes volume（存储卷）: emptyDirEmptyDir类型的volume创建于pod被调度到某个宿主机上的时候，而同一个pod内的容器都能读写EmptyDir中的同一个文件。一旦这个pod离开了这个宿主机，EmptyDirr中的数据就会被永久删除。所以目前EmptyDir类型的volume主要用作临时空间，比如Web服务器写日志或者tmp文件需要的临时目录。默认的，emptyDir 磁盘会存储在主机所使用的媒介上，可能是SSD，或者网络硬盘，这主要取决于你的环境。当然，我们也可以将emptyDir.medium的值设置为Memory来告诉Kubernetes 来挂在一个基于内存的目录tmpfs，因为tmpfs速度会比硬盘块度了，但是，当主机重启的时候所有的数据都会丢失。12345678910111213141516apiVersion: v1kind: Podmetadata: name: test-pdspec: containers: - image: gcr.io/google_containers/test-webserver name: test-container volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume #emptyDir: &#123;&#125; emptyDir: medium: Memory hostPath为在pod上挂载宿主机上的文件或者目录123456789101112131415spec: #hostNetwork: true containers: - name: tomcat image: tomcat:8.5.20 ports: - containerPort: 8080 #hostPort: 80 volumeMounts: - name: workdir mountPath: /opt volumes: - name: workdir hostPath: path: \"/data/works/tomcat/logs\" nfs使用nfs网络文件服务器提供的共享目录存储数据时，需要部署一个nfs server，定义nfs类型volume 如：12345volumeMounts:- name: workdir nfs: server: nfs-server path: \"/\" 持久卷静态供应需要管理员手动创建 PV，然后创建 PVC 绑定 PV，最后创建 Pod 声明使用 PVC 创建 PV 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748cat PersistentVolume.yaml apiVersion: v1kind: PersistentVolumemetadata: name: task-pv-volume labels: type: localspec: storageClassName: manual #静态供应，名字可以任意取 capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/works/loki/data\" #在创建pod的节点上会新建该目录cat PersistentVolumeClaim.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: task-pv-claimspec: storageClassName: manual #storageClassName要和PV中的一致 accessModes: - ReadWriteOnce #accessMode要和PV中的一致 resources: requests: storage: 3Gi #申请3G容量，申请就近原则，如果有一个10G的和一个20G的PV满足要求，那么使用10G的PV#Testcat Nginx.yaml apiVersion: v1kind: Podmetadata: name: task-pv-podspec: volumes: - name: task-pv-storage persistentVolumeClaim: claimName: task-pv-claim #使用的PVC的名字 containers: - name: task-pv-container1 image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" #容器中挂载的目录 创建后： 12cd /works/loki/dataecho \"volume nginx\" &gt; index.html curl一下kubectl get pod -o wide中的IP就可以显示了。注意： 由于指定的是本地目录，如果飘逸到另外一台，目录会自动创建好。（可以考虑mount某个目录。多机器不建议。） 删除要按照 Pod–&gt;PVC–&gt;PV 的顺序删除，如果先删除了 PVC 会等 Pod 删除掉，才会删除 PVC ，如果先删除了 PV，会等 pod 和 PVC 删除了才会删除 PV。 清理： 123kubectl delete po task-pv-podkubectl delete pvc task-pv-claimkubectl delete pv task-pv-volume 还有NFS等模式，参考以下： https://cloud.tencent.com/developer/article/1825527 https://blog.csdn.net/zhaikaiyun/article/details/104481456 https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/ Namespace命名空间Namespace 在很多情况下用于多租户的资源隔离，Namespace通过将集群内部的资源对象“分配”到不通的Namespace中， 形成逻辑上的分组的不同项目，小组或者 用户组，便于不同的分组在共享使用这个集群的资源的同时还能被分别管理。如果不特别指明namespace，则用户创建的 pod rc service 都将被系统创建到defalut中 kubernetes集群在启动后，会创建一个 default 的 namespace:12345678[root@k8s-master ~]# kubectl get namespaceNAME STATUS AGEdefault Active 6dkube-public Active 6dkube-system Active 6dmonitoring Active 2dsock-shop Active 5dxxx Active 5d 如果不特别指明namespace，则用户创建的 pod rc service 都将被系统创建到defalut中。创建namespace：1234567891011121314#创建fengjian20170221 的命名空间apiVersion: v1kind: Namespacemetadata: name: monitoring---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: tomcat-deployment namespace: monitoring... ConfigMapconfigmap 供容器使用的典型方案如下： 生成为容器内的环境变量 设置容器启动命令的启动参数 以volume的形式挂载为容器内部的文件或者目录4: 注意必须先创建 configMap, 然后pod 才能创建，如果已经创建的pod，升级，环境变量无法找到，一定要做好提前规划。 生成为容器内的环境变量12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455apiVersion: v1kind: ConfigMapmetadata: name: testenv# namespace: testdata: mysql_server: 192.168.10.1 redis_server: 192.168.20.1 mongo_server: 192.168.30.1---apiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: ports: - port: 80 selector: app: nginx---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx# namespace: testspec: replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.12.1 ports: - containerPort: 80 env: - name: mysql_server valueFrom: configMapKeyRef: name: testenv key: mysql_server - name: redis_server valueFrom: configMapKeyRef: name: testenv key: redis_server - name: mongo_server valueFrom: configMapKeyRef: name: testenv key: mongo_server mount的方式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: v1kind: ConfigMapmetadata: name: db-hostsdata: hosts: | 192.168.10.6 k8s-master 192.168.10.7 k8s-node1---apiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: ports: - port: 80 selector: app: nginx---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx# namespace: testspec: replicas: 1 template: metadata: labels: app: nginx spec: volumes: - name: hosts-volume configMap: name: db-hosts containers: - name: nginx image: nginx:1.12.1 ports: - containerPort: 80 volumeMounts: - name: hosts-volume mountPath: /mnt/hosts.append restartPolicy: Always 查询mount的内容：123[root@k8s-master yaml]# kubectl exec nginx-1688079652-917zh cat /mnt/hosts.append/hosts 192.168.10.6 k8s-master192.168.10.7 k8s-node1 查询所有的configmap：1234[root@k8s-master yaml]# kubectl get configmapNAME DATA AGEdb-hosts 1 1dspecial-config 2 3d Nginx IngressKubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress。 部署默认后端我们知道 前端的 Nginx 最终要负载到后端 service 上，那么如果访问不存在的域名咋整？官方给出的建议是部署一个 默认后端，对于未知请求全部负载到这个默认后端上；这个后端啥也不干，就是返回 404，部署如下：12wget https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/default-backend.yamlkubectl create -f default-backend.yaml 部署Ingress Controller部署完后端就得把最重要的组件Nginx Ingres Controller部署：12wget https://raw.githubusercontent.com/kubernetes/ingress/master/examples/daemonset/nginx/nginx-ingress-daemonset.yamlkubectl create -f nginx-ingress-daemonset.yaml 注意：如果需要nginx controller监听80端口的话，需要添加hostNetwork: true的参数：1234spec: terminationGracePeriodSeconds: 60 hostNetwork: true ... 也可以采用deployment方式，参考https://github.com/kubernetes/ingress/tree/master/examples/deployment/nginx。 部署Ingress希望通过frontend.zhaoxy.com访问frontend:80的服务，nginx.zhaoxy.com访问nginx-app:80的服务：12345678910111213141516171819apiVersion: extensions/v1beta1kind: Ingressmetadata: name: web-ingress namespace: defaultspec: rules: - host: frontend.zhaoxy.com http: paths: - backend: serviceName: frontend servicePort: 80 - host: nginx.zhaoxy.com http: paths: - backend: serviceName: nginx-app servicePort: 80 查看ingress状态123[root@k8s-master yaml]# kubectl get ing -o wideNAME HOSTS ADDRESS PORTS AGEweb-ingress www.zhaoxy.com,frontend.zhaoxy.com,nginx.zhaoxy.com 192.168.10.6,192.168.10.7 80 21m 完成后我们可以通过keppalived对nginx做集群即可。 部署Traefik参考http://huxos.me/kubernetes/2017/09/19/kubernetes-cluster-07-ingress.htmlIngress的引入主要解决创建入口站点规则的问题，主要作用于7层入口(http)。 可以通过K8s的Ingress对象定义类似于nginx中的vhost、localtion、upstream等。 Nginx官方也有Ingress的实现nginxinc/kubernetes-ingress来对接k8s。 考虑到Traefik部署较为方便，使用traefik提供Ingress服务。 定义traefik需要的RBAC规则traefik-rbac.yaml: 12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: v1kind: ServiceAccountmetadata: name: traefik-ingress-controller namespace: kube-system---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: traefik-ingress-controllerrules: - apiGroups: - \"\" resources: - pods - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: traefik-ingress-controllerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controllersubjects:- kind: ServiceAccount name: traefik-ingress-controller namespace: kube-system 定义ingress编排的daemonset模版traefik-daemonset.yaml: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647apiVersion: extensions/v1beta1kind: DaemonSetmetadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-controller namespace: kube-systemspec: selector: matchLabels: k8s-app: traefik-ingress-lb template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: containers: - args: - --web - --web.address=:8580 - --kubernetes - --web.metrics - --web.metrics.prometheus image: traefik imagePullPolicy: IfNotPresent name: traefik-ingress-lb ports: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 8580 hostPort: 8580 protocol: TCP resources: requests: #cpu: \"2\" memory: 512M dnsPolicy: ClusterFirst hostNetwork: true nodeSelector: role: ingress restartPolicy: Always schedulerName: default-scheduler serviceAccount: traefik-ingress-controller serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 创建ingress规则traefik-ingress.yaml: 12345678910111213apiVersion: extensions/v1beta1kind: Ingressmetadata: name: metadata namespace: defaultspec: rules: - host: a.zhaoxy.com http: paths: - backend: serviceName: nginx-app servicePort: 80 创建参考traefik.zip1kubectl create -f ./ 访问这样其实相当于定义了一个http的站点，域名a.zhaoxy.com指向了default的metadata-server这个服务。访问相关节点的8580端口就能看到a.zhaoxy.com站点对应的信息了。 可以通过http://a.zhaoxy.com访问nginx-app的80服务。可以通过http://a.zhaoxy.com:8580/dashboard/访问监控服务。 最新命令汇总123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151minikube addons listminikube addons enable dashboardminikube addons enable heapsterminikube addons enable metrics-serverminikube addons enable ingressminikube addons enable ingress-dnskubectl top nodekubectl top pod --all-namespaceskubectl cluster-infominikube service monitoring-grafana -n kube-systemkubectl get pod --watchkubectl delete all --allk get pod --show-labelsk label pod xxx app=foo --overwrite#Creating by commands#http://kubernetes.kansea.com/docs/user-guide/kubectl/kubectl_run/#Not recommend and cannot set the port of NodePort#kubectl run nginx --image=nginx --port=80 --expose#Recommendkubectl create deployment nginx --image=nginx#One way and can set the port of NodePortkubectl create service nodeport nginx --tcp=81:80 --node-port=30000#The other way, but cannot set the port of NodePortkubectl expose deployment nginx --type=NodePort --name nginx --port=80 --target-port=80kubectl create deployment kubia --image=luksa/kubiakubectl expose deployment kubia --type=NodePort --name kubia-http --port=80 --target-port=8080#Don&apos;t use port 80#kubectl expose deployment kubia --type=LoadBalancer --name kubia-http --port=8000 --target-port=8080#kubectl create service nodeport kubia --tcp=8080:80 --node-port=30000minikube service kubia-http --url#Creating by yaml#kubia-deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: kubiaspec: replicas: 2 #指定新创建的pod至少要成功运行多久才视为可用 #让k8s在pod就绪之后继续等待10秒后，才继续执行滚动升级 minReadySeconds: 10 revisionHistoryLimit: 8 progressDeadlineSeconds: 10 strategy: rollingUpdate: maxSurge: 1 #0确保升级过程中pod被挨个替换 maxUnavailable: 0 type: RollingUpdate template: metadata: name: kubia labels: app: kubia spec: containers: - name: nodejs image: luksa/kubia:v2 readinessProbe: periodSeconds: 1 httpGet: path: / port: 8080 selector: matchLabels: app: kubia#--record会记录历史版本号kubectl create -f kubia-deployment.yaml --record#kubia-svc.yamlapiVersion: v1kind: Servicemetadata: name: kubia-httpspec:# sessionAffinity: ClientIP# type: NodePort# type: LoadBalancer ports: - port: 80 targetPort: 8080# nodePort: 30123 selector: app: kubia sessionAffinity: ClientIP#kubia-ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: kubiaspec: rules: - host: kubia.example.com http: paths: - path: /kubia # 将 /kubia 子路径请求转发到 kubia-nodeport 服务的 80 端口 backend: serviceName: kubia-http servicePort: 80#Rolloutkubectl create -f kubia-deployment.yaml --record#kubectl scale deployment kubia --replicas=3kubectl set image deployment kubia nodejs=luksa/kubia:v3kubectl rollout pause deployment kubiakubectl rollout resume deployment kubiakubectl rollout status deployment kubiakubectl rollout history deployment kubiakubectl rollout undo deployment kubia --to-revision=1kubectl patch deployment kubia -p &apos;&#123;&quot;spec&quot;: &#123;&quot;revisionHistoryLimit&quot;: 5&#125;&#125;&apos;#指定新创建的pod至少要成功运行多久才视为可用,让k8s在pod就绪之后继续等待10秒后，才继续执行滚动升级kubectl patch deployment kubia -p &apos;&#123;&quot;spec&quot;: &#123;&quot;minReadySeconds&quot;: 10&#125;&#125;&apos;#滚动失败的超时时间kubectl patch deployment kubia -p &apos;&#123;&quot;spec&quot;: &#123;&quot;progressDeadlineSeconds&quot;: 15&#125;&#125;&apos;#将本地网络端口转发到pod中的端口kubectl port-forward kubia-7d46fb6687-86th4 8888:8080kubectl port-forward service/hello-minikube 7080:80#获取docker内部IP:docker inspect 998f4f7b87c5|grep -i ip#Iptablesiptables -t nat -nvL --line-number#删除：iptables -t nat -D PREROUTING 1iptables -t nat -D POSTROUTING 1docker run --name nginx-test -p 8080:80 -d nginxHelm:#https://helm.sh/docs/intro/install/#curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bashsudo snap install helm --classicIngress:#helm delete &lt;release-name&gt;helm repo add nginx-stable https://helm.nginx.com/stablehelm repo updatehelm install nginx-ing nginx-stable/nginx-ingresshelm install nginx-ingress --namespace kube-system \\--set &quot;rbac.create=true,controller.service.externalIPs[0]=172.21.114.172,controller.service.externalIPs[1]=192.168.102.121&quot; nginx-stable/nginx-ingressTraefk:#https://doc.traefik.io/traefik/getting-started/install-traefik/#use-the-helm-charthelm repo add traefik https://helm.traefik.io/traefikhelm repo updatehelm install traefik traefik/traefikkubectl port-forward $(kubectl get pods --selector &quot;app.kubernetes.io/name=traefik&quot; --output=name) 9000:9000Accessible with the url: http://127.0.0.1:9000/dashboard/ Example12345678910111213141516171819202122#Example:#https://learnk8s.io/spring-boot-kubernetes-guidedocker network create knotedocker run \\ --name=mongo \\ --rm \\ --network=knote \\ mongodocker run \\ --name=knote-java \\ --rm \\ --network=knote \\ -p 8080:8080 \\ -e MONGO_URL=mongodb://mongo:27017/dev \\ learnk8s/knote-java:1.0.0#https://spring.io/guides/gs/spring-boot-kubernetes/$ kubectl create deployment demo --image=springguides/demo --dry-run -o=yaml &gt; deployment.yaml$ echo --- &gt;&gt; deployment.yaml$ kubectl create service clusterip demo --tcp=8080:8080 --dry-run -o=yaml &gt;&gt; deployment.yaml#https://spring.io/guides/topicals/spring-on-kubernetes/ Context1234567#https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/#kubectl context configkubectl config get-contexts#current context configkubectl config current-context#switch config as alik8s-0kubectl config use-context alik8s-0 参考 https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/http://blog.csdn.net/felix_yujing/article/details/51622132docker与kubectl命令对比http://www.cnblogs.com/fengjian2016/p/6423455.htmlhttps://github.com/feiskyer/kubernetes-handbook/blob/master/introduction/101.mdhttp://blog.csdn.net/xts_huangxin/article/details/51891709https://www.stratoscale.com/blog/kubernetes/kubernetes-exposing-pods-service/http://blog.csdn.net/u012804178/article/category/6861460http://feisky.xyz/2016/09/11/Kubernetes%E4%B8%AD%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/http://dockone.io/article/2247https://www.kubernetes.org.cn/%E6%96%87%E6%A1%A3%E4%B8%8B%E8%BD%BDhttp://kubernetes.kansea.com/docs/https://www.kubernetes.org.cn/1885.htmlhttps://mritd.me/2017/03/04/how-to-use-nginx-ingresshttp://huxos.me/kubernetes/2017/09/19/kubernetes-cluster-07-ingress.html","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/tags/kubernetes/"}]},{"title":"Kubeadm集群搭建","slug":"Kubeadm集群搭建","date":"2017-09-01T09:08:32.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/09/Kubeadm集群搭建.html","link":"","permalink":"http://blog.gcalls.cn/2017/09/Kubeadm集群搭建.html","excerpt":"传统的集群安装方式还是比如麻烦，比如说添加新的node节点，需要安装kubelet/proxy，还要配置。kubeadm旨在简化这些繁琐的操作。","text":"传统的集群安装方式还是比如麻烦，比如说添加新的node节点，需要安装kubelet/proxy，还要配置。kubeadm旨在简化这些繁琐的操作。 环境准备docker版本为：1.12.6kubeadm版本为：v1.7.5 主机IP 主机名称 内存 192.168.10.6 k8s-master 1024m 192.168.10.7 k8s-node1 1024m 192.168.10.8 k8s-node2 1024m 系统优化12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061sed -i 's;SELINUX=.*;SELINUX=disabled;' /etc/selinux/configsetenforce 0getenforce#LANG=\"en_US.UTF-8\"sed -i 's;LANG=.*;LANG=\"zh_CN.UTF-8\";' /etc/locale.confcat /etc/NetworkManager/NetworkManager.conf|grep \"dns=none\" &gt; /dev/nullif [[ $? != 0 ]]; then echo \"dns=none\" &gt;&gt; /etc/NetworkManager/NetworkManager.conf systemctl restart NetworkManager.servicefisystemctl disable iptablessystemctl stop iptablessystemctl disable firewalldsystemctl stop firewalld#ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimetimedatectl set-timezone Asia/Shanghai#logined limitcat /etc/security/limits.conf|grep 100000 &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF* - nofile 100000* - nproc 100000EOFfised -i 's;4096;100000;g' /etc/security/limits.d/20-nproc.conf#systemd service limitcat /etc/systemd/system.conf|egrep '^DefaultLimitCORE' &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/systemd/system.conf &lt;&lt; EOFDefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000EOFficat /etc/sysctl.conf|grep \"net.ipv4.ip_local_port_range\" &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 300net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.ip_local_port_range = 1024 65535net.ipv4.ip_forward = 1EOFsysctl -pfisu - root -c \"ulimit -a\"# 同步时间yum -y install ntpsystemctl start ntpdsystemctl enable ntpd 修改主机名1234567891011121314151617181920212223#192.168.10.6hostnamectl --static set-hostname k8s-mastersysctl kernel.hostname=k8s-masterecho '192.168.10.6 k8s-master192.168.10.7 k8s-node1192.168.10.8 k8s-node2' &gt;&gt; /etc/hosts#192.168.10.7hostnamectl --static set-hostname k8s-node1sysctl kernel.hostname=k8s-node1echo '192.168.10.6 k8s-master192.168.10.7 k8s-node1192.168.10.8 k8s-node2' &gt;&gt; /etc/hosts#192.168.10.8hostnamectl --static set-hostname k8s-node2sysctl kernel.hostname=k8s-node2echo '192.168.10.6 k8s-master192.168.10.7 k8s-node1192.168.10.8 k8s-node2' &gt;&gt; /etc/hosts 修改系统参数1234567cat &gt;&gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF#k8snet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl -p /etc/sysctl.d/k8s.conf 安装yum安装每台添加yum源：123456789101112131415161718# docker repotee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[docker-repo]name=Docker Repositorybaseurl=https://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-engine/yum/gpgEOF# k8s repotee /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes-repo]name=Kubernetes Repositorybaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0EOF yum安装：1234yum install -y docker-engine-1.12.6-1.el7.centos.x86_64#yum install -y kubelet kubectl kubernetes-cni kubeadmyum install -y kubernetes-cni-0.5.1-0.x86_64 kubelet-1.7.5-0.x86_64 kubectl-1.7.5-0.x86_64 kubeadm-1.7.5-0.x86_64systemctl enable kubelet 二进制安装：1wget https://dl.k8s.io/v1.7.5/kubernetes-server-linux-amd64.tar.gz 下载镜像镜像列表123456789docker pull gcr.io/google_containers/kube-proxy-amd64:v1.7.5docker pull gcr.io/google_containers/kube-apiserver-amd64:v1.7.5docker pull gcr.io/google_containers/kube-controller-manager-amd64:v1.7.5docker pull gcr.io/google_containers/kube-scheduler-amd64:v1.7.5docker pull gcr.io/google_containers/etcd-amd64:3.0.17docker pull gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1docker pull gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1docker pull gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1docker pull gcr.io/google_containers/pause-amd64:3.0 注意：以上镜像在创建时会自动下载，无需手动下载。以上镜像需要先翻墙才行。 master初始化初始化统一docker与kubernetes的driver：可以直接执行以下命令修改：1sed -i 's;systemd;cgroupfs;g' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 保存配置, 重启：12systemctl daemon-reloadsystemctl restart kubelet kubeadmn创建的etcd为单节点，不建议使用。建议使用外部的etcd集群。具体安装请参考etcd集群安装v1.6.x版本后，–external-etcd-endpoints参数已不能使用。所以要使用–config参数外挂配置文件kubeadm-config.yml：12345678910111213apiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationapi: advertiseAddress: 192.168.10.6networking: #dnsDomain: myk8s.com podSubnet: 10.244.0.0/16etcd: endpoints: - http://192.168.10.6:2379 - http://192.168.10.7:2379 - http://192.168.10.8:2379kubernetesVersion: v1.7.5 具体请参考：kubeadm-config.yml注意：最好不要修改dnsDomain，不然会有一些奇怪的问题。 初始化在master上执行：1kubeadm init --config kubeadm-config.yml 另外一种初始化方式12345#export KUBE_COMPONENT_LOGLEVEL='--v=0'kubeadm init --kubernetes-version=v1.7.5 --apiserver-advertise-address=192.168.10.6#如果是使用flannel网络的话，要加上--pod-network-cidr 10.244.0.0/16kubeadm init --kubernetes-version=v1.7.5 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.10.6 异常处理初始化时, 会发现卡死不动, 可以通过系统日志查看错误1234journalctl -f -u kubelet.server# tail -n100 -f /var/log/messagesfailed to create kubelet: misconfiguration: kubelet cgroup driver: \"cgroupfs\" is different from docker cgroup driver: \"systemd\" 这个是K8S v1.6.x的一个变化, 文件驱动与docker使用的文件驱动不一致, 导致镜像无法启动。此处可以修改kubelet的文件驱动，请参考http://www.jianshu.com/p/02dc13d2f651先确认docker的Cgroup Driver：123[root@k8s-node1 ~]# docker info......Cgroup Driver: cgroupfs 需要将kubeadm.conf的systemd修改为cgroupfs。注意: 每台都要修改： 12# 进入kubelet启动配置文件vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 将1Environment=\"KUBELET_CGROUP_ARGS=--cgroup-driver=systemd\" 替换为：1Environment=\"KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs\" 可以直接执行以下命令修改：1sed -i 's;systemd;cgroupfs;g' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 保存配置, 重启：12systemctl daemon-reloadsystemctl restart kubelet 然后再重新初始化在master上执行：123kubeadm reset#kubeadm init --kubernetes-version=v1.7.5 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.10.6kubeadm init --config kubeadm-config.yml kubeadm init时出现123[apiclient] Temporarily unable to list nodes (will retry)[apiclient] Temporarily unable to list nodes (will retry)[apiclient] Temporarily unable to list nodes (will retry) 应该是dns server把localhost解析到其他地址去了。可以通过nslookup 命令验证：[root@master ~]# nslookup localhost修改的/etc/resolv.conf中的search内容后问题解决。 configmaps “cluster-info” already exists需要清理etcd数据：123systemctl stop etcdrm -fr /var/lib/etcd/etcd/*systemctl restart etcd 正常情况下，应该能显示以下的日志：1234567891011121314151617181920212223242526272829303132333435363738394041424344kubeadm init --kubernetes-version=v1.7.5 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.10.6[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.[init] Using Kubernetes version: v1.7.5[init] Using Authorization mode: RBAC[preflight] Running pre-flight checks[preflight] WARNING: kubelet service is not enabled, please run 'systemctl enable kubelet.service'[preflight] Starting the kubelet service[certificates] Generated CA certificate and key.[certificates] Generated API server certificate and key.[certificates] API Server serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.6][certificates] Generated API server kubelet client certificate and key.[certificates] Generated service account token signing key and public key.[certificates] Generated front-proxy CA certificate and key.[certificates] Generated front-proxy client certificate and key.[certificates] Valid certificates and keys now exist in \"/etc/kubernetes/pki\"[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/controller-manager.conf\"[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/scheduler.conf\"[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\"[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\"[apiclient] Created API client, waiting for the control plane to become ready[apiclient] All control plane components are healthy after 126.837475 seconds[apiclient] Waiting for at least one node to register[apiclient] First node has registered after 6.516528 seconds[token] Using token: 67a477.959aa53030fd8444[apiconfig] Created RBAC rules[addons] Created essential addon: kube-proxy[addons] Created essential addon: kube-dnsYour Kubernetes master has initialized successfully!To start using your cluster, you need to run (as a regular user): sudo cp /etc/kubernetes/admin.conf $HOME/ sudo chown $(id -u):$(id -g) $HOME/admin.conf export KUBECONFIG=$HOME/admin.confYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: http://kubernetes.io/docs/admin/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join --token 67a477.959aa53030fd8444 192.168.10.6:6443 之前的版本, 当我们初始化成功之后, 会发现token不会保留, 如果一旦没有记录下来, 其他节点就没法加入了, 这里添加了kubeadm token命令123kubeadm token listTOKEN TTL EXPIRES USAGES DESCRIPTION67a477.959aa53030fd8444 &lt;forever&gt; &lt;never&gt; authentication,signing The default bootstrap token generated by 'kubeadm init'. 默认情况下, master节点是不会调度pod, 也就是说, 只有一台主机的情况下, 我们无法启动pod, 但有的时候我们的确只有一台机器, 这个时候可以执行命令, 允许master调度pod(这个命令和1.5.x版本不一样)1kubectl taint nodes --all node-role.kubernetes.io/master- 查询node情况1kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes kubectl 命令这个命令是我们经常使用的, 几乎所有的k8s相关操作都需要, 但当我们集群安装好后, 发现这个命令会报错。最直接的方法是带上参数 –kubeconfig1kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes 注意：只有master中才有admin.conf文件，另外node上只有kubelet.conf。所以只能在master进行logs/exec等命令。可以将admin.conf拷贝到其他node机器中：12scp /etc/kubernetes/admin.conf root@192.168.10.7:/etc/kubernetes/scp /etc/kubernetes/admin.conf root@192.168.10.8:/etc/kubernetes/ 如果不想每次都带上参数, 可以配置环境变量1234# 添加tee &gt;&gt; ~/.bash_profile &lt;&lt; EOFexport KUBECONFIG=/etc/kubernetes/admin.confEOF 执行1source ~/.bash_profile 这样就可以不用带–kubeconfig参数了1kubectl get nodes node加入加入node到集群中请记得init后的join命令(类似于下面，但token不一样)，其他的node要加入集群的话，必须用下面的命令：12345678910111213141516171819202122kubeadm join --token 67a477.959aa53030fd8444 192.168.10.6:6443[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.[preflight] Running pre-flight checks[preflight] WARNING: kubelet service is not enabled, please run &apos;systemctl enable kubelet.service&apos;[preflight] Starting the kubelet service[discovery] Trying to connect to API Server &quot;192.168.10.6:6443&quot;[discovery] Created cluster-info discovery client, requesting info from &quot;https://192.168.10.6:6443&quot;[discovery] Cluster info signature and contents are valid, will use API Server &quot;https://192.168.10.6:6443&quot;[discovery] Successfully established connection with API Server &quot;192.168.10.6:6443&quot;[bootstrap] Detected server version: v1.7.5[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)[csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request[csr] Received signed certificate from the API server, generating KubeConfig...[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;Node join complete:* Certificate signing request sent to master and response received.* Kubelet informed of new secure connection details.Run &apos;kubectl get nodes&apos; on the master to see this machine join. 查看node信息12345 kubectl get noNAME STATUS AGE VERSIONk8s-master NotReady 39m v1.7.5k8s-node1 NotReady 32s v1.7.5k8s-node2 NotReady 53s v1.7.5 异常处理node加入后，STATUS状态为：NotReady查看kubelet日志：12systemctl status -l kubeletkubeadm network plugin is not ready: cni config uninitialized 如果采用了非cni方式部署flannel网络，需要去掉cni网络参数，参考https://github.com/kubernetes/kubernetes/issues/43815: you need to edit /etc/systemd/system/kubelet.service.d/10-kubeadm.conf1ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_EXTRA_ARGS remove $KUBELET_NETWORK_ARGS, and then restart kubelet after that kubeadm init should work. 重启kubelet服务后正常：12systemctl daemon-reloadsystemctl restart kubelet 查看pod信息： 123456789[root@k8s-master v1.7.5]# kubectl get pod -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODEkube-apiserver-k8s-master 1/1 Running 0 10m 192.168.10.6 k8s-masterkube-controller-manager-k8s-master 1/1 Running 0 10m 192.168.10.6 k8s-masterkube-dns-1783747724-bh3g3 3/3 Running 0 2h 10.244.3.2 k8s-node2kube-proxy-3l1wf 1/1 Running 0 2h 192.168.10.7 k8s-node1kube-proxy-5mwcl 1/1 Running 0 2h 192.168.10.8 k8s-node2kube-proxy-s02wm 1/1 Running 0 2h 192.168.10.6 k8s-masterkube-scheduler-k8s-master 1/1 Running 0 10m 192.168.10.6 k8s-master 不能查看pod日志报以下的错误：12kubectl logs kube-dns-1783747724-bh3g3 -n kube-system Error from server (BadRequest): a container name must be specified for pod kube-dns-1783747724-bh3g3, choose one of: [kubedns dnsmasq sidecar] 这个是因为为kubectl没有admin权限，所以需要修改~/.bash_profile中的1export KUBECONFIG=/etc/kubernetes/admin.conf 执行1source ~/.bash_profile kube-dns是不能正常运行的，STATUS为Pending这是因为没有部署网络，具体请参考部署网络12345678910[root@k8s-master ~]# kubectl get pod -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODEetcd-k8s-master 1/1 Running 0 37m 192.168.10.6 k8s-masterkube-apiserver-k8s-master 1/1 Running 0 37m 192.168.10.6 k8s-masterkube-controller-manager-k8s-master 1/1 Running 0 37m 192.168.10.6 k8s-masterkube-dns-3913472980-gx5zn 0/3 Pending 0 42m &lt;none&gt; kube-proxy-3970g 1/1 Running 0 4m 192.168.10.8 k8s-node2kube-proxy-t8zhh 1/1 Running 0 42m 192.168.10.6 k8s-masterkube-proxy-xvsdk 1/1 Running 0 3m 192.168.10.7 k8s-node1kube-scheduler-k8s-master 1/1 Running 0 37m 192.168.10.6 k8s-master 部署网络cni网络1wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 如果是使用vagrant，需要修改kube-flannel.yml：1command: [ \"/opt/bin/flanneld\", \"--iface=eth1\", \"--ip-masq\", \"--kube-subnet-mgr\" ] 参考修改后的文件：kube-flannel.yml 创建网络: 1kubectl create -f kube-flannel.yml flannel网络也可以手动部署flannel网络，这种为非cni网络。具体请参考使用Flannel搭建docker网络 需要去掉cni网络： 123sed -i 's;$KUBELET_NETWORK_ARGS ;;g' /etc/systemd/system/kubelet.service.d/10-kubeadm.confsystemctl daemon-reloadsystemctl restart kubelet 测试dns#https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/Create a file named busybox.yaml with the following contents:12345678910111213141516tee busybox.yaml &lt;&lt; EOFapiVersion: v1kind: Podmetadata: name: busybox namespace: defaultspec: containers: - image: busybox command: - sleep - \"3600\" imagePullPolicy: IfNotPresent name: busybox restartPolicy: AlwaysEOF Then create a pod using this file:1kubectl create -f busybox.yaml 测试：123456789#kubectl run busybox --rm -ti --image=busybox --restart=Never -- nslookup -type=srv kubernetes#以上每次退出后会自动删除images中的镜像，每次执行都会重新下载image，所以每次执行都会有些慢。[root@k8s-master config]# kubectl exec -ti busybox -- nslookup kubernetes.defaultServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetes.defaultAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local dashboard安装最新方式https://forums.docker.com/t/docker-for-mac-kubernetes-dashboard/44116/6 123kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/alternative/kubernetes-dashboard.yamlkubectl get pod --namespace=kube-system | grep dashboardkubectl port-forward kubernetes-dashboard-57b79cdfb5-5bj6m 9090:9090 --namespace=kube-system Then, open your browser on http://127.0.0.1:9090 12 and the dashboard should work without any authentification! 或者：1kubectl proxy 直接访问： http://localhost:8001/api/v1/namespaces/kube-system/services/http:kubernetes-dashboard:/proxy/#!/overview?namespace=default 如果不能访问看是不是为http:kubernetes-dashboard，如果是https:kubernetes-dashboard的话会报Error: ‘tls: oversized record received with length 20527’的错误。 see: https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard---1.7.X-and-above 其他方式12345678#1.6.3版本已经不能下载了#wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml#1.7.0版本#https://github.com/kubernetes/dashboard/tree/v1.7.1/src/deploy/recommendedwget https://github.com/kubernetes/dashboard/raw/v1.7.1/src/deploy/recommended/kubernetes-dashboard.yamlkubectl create -f kubernetes-dashboard.yaml 1.6请参考：kubernetes-dashboard.yaml1.7请参考：kubernetes-dashboard.yaml 查看123456kubectl get pod,svc -n kube-system -l k8s-app=kubernetes-dashboardNAME READY STATUS RESTARTS AGEpo/kubernetes-dashboard-2315583659-qt0vm 1/1 Running 1 21hNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc/kubernetes-dashboard 10.106.184.36 &lt;none&gt; 80/TCP 21h 访问第1种访问方式 https://192.168.10.6:6443/uihttps://192.168.10.6:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy 注意：1.7版本后https://192.168.10.6:6443/ui已不能访问。否则访问时会出现以下错误：Error: ‘malformed HTTP response “\\x15\\x03\\x01\\x00\\x02\\x02”‘Trying to reach: ‘http://10.244.2.6:8443/&#39;参考http://tonybai.com/2017/09/26/some-notes-about-deploying-kubernetes-dashboard-1-7-0/ 访问该地址后，我们在浏览器中看到如下登录页面： dashboard v1.7默认支持两种身份校验登录方式：kubeconfig和token两种。我们说说token这种方式。点击选择:Token单选框，提示你输入token。可以通过以下方式获取token： 1234567891011121314151617[root@k8s-master ~]# kubectl get secret -n kube-system|grep dashboardkubernetes-dashboard-token-wlzb0 kubernetes.io/service-account-token 3 3m[root@k8s-master ~]# kubectl describe secret/kubernetes-dashboard-token-wlzb0 -n kube-system Name: kubernetes-dashboard-token-wlzb0Namespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name=kubernetes-dashboard kubernetes.io/service-account.uid=07dd79c6-a58d-11e7-985e-080027a8da33Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi13bHpiMCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjA3ZGQ3OWM2LWE1OGQtMTFlNy05ODVlLTA4MDAyN2E4ZGEzMyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.a81oQ8XkR87luB6MeClqq5OTKXgfK_Dn8ku4uQdh4Y03aMHNwLYRdCzHZ67d_1sFndeX5FaKHr-hCxVz5eLYMVyexcQoegHvJtOA5tOyX1RRF55LCotrfsigG_4IGU9caOBmODV1HSPpQGuVcGtky3-9KIUR1r8JlEsxuFl4aaBp9YmJg_TJx7sDwF5Io_S4M21JrXOP_6Wly-hJnOW5_KF0eUyyUSHPN1HCBx-4l2CANbrK9xONAFUl9cgisfLjNZDhbBvQBZi-Ru6Ugxxkxxok1fADWkJMiwILsW9gV724OfWfnTNM8PUDQYZX1tegRGfm8vnjxEdyKXuuERaLRg 登录后出现： 这个是由于1.7后，默认为最小权限。需要创建权限：参考official-release 123456789101112131415161718tee dashboard-admin.yaml &lt;&lt; EOFapiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard labels: k8s-app: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-systemEOFkubectl create -f dashboard-admin.yaml 让Dashboard v1.7.0支持basic auth login方式： 我们要用basic auth方式登录dashboard，需要对kubernetes-dashboard.yaml进行如下修改： 1234args: - --tls-key-file=/certs/dashboard.key - --tls-cert-file=/certs/dashboard.crt - --authentication-mode=basic &lt;---- 添加这一行 重新创建即可。 第2种访问方式 http://192.168.10.6:9090/uihttp://192.168.10.6:8443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy 可以通过kubectl proxy代理访问： 123456[root@k8s-master ~]# kubectl get ep -n kube-system NAME ENDPOINTS AGEkubernetes-dashboard 10.244.1.5:8443 19m#kubectl proxy --address 192.168.10.6 --port=8443 --accept-hosts='^*$'kubectl proxy --address='0.0.0.0' --port=8443 --accept-hosts='^*$' 这样就可以访问了。此访问为非安全方式，如果提示输入密码或者token时，直接SKIP就可以了。 可能会出现pod为error的错误，将所有的dashboard yaml重新创建一下就可以了。注意：如果是1.6版本，访问地址为：http://192.168.10.6:9090/ui 1.6版本还可以通过NodePort方式访问，但1.7版本不行：参考：https://github.com/qianlei90/Blog/issues/28https://github.com/kubernetes/dashboard/issues/692因为使用kubeadm安装的集群是不带认证的，所以无法直接从https:///ui访问，可以添加NodePort方式访问：123456spec: type: NodePort ports: - port: 80 targetPort: 9090 nodePort: 30000 异常处理User “system:anonymous” cannot get at the cluster scope参考http://www.tongtongxue.com/archives/16338.html编辑/etc/kubernetes/manifests/kube-apiserver.yaml，添加- –anonymous-auth=false：12345spec: containers: - command: - kube-apiserver - --anonymous-auth=false kube-apiserver周期性异常重启:12kubectl describe pod kube-apiserver-k8s-master -n kube-system|grep healthLiveness: http-get https://127.0.0.1:6443/healthz delay=15s timeout=15s period=10s #success=1 #failure=8 可以看到liveness check有8次failure！8次是kube-apiserver的failure门槛值，这个值在/etc/kubernetes/manifests/kube-apiserver.yaml中我们可以看到：123456789livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 6443 scheme: HTTPS initialDelaySeconds: 15 timeoutSeconds: 15 这样，一旦failure次数超限，kubelet会尝试Restart kube-apiserver，这就是问题的原因。那么为什么kube-apiserver的 liveness check 会fail呢？这缘于我们关闭了匿名请求的身份验证权。还是来看/etc/kubernetes/manifests/kube-apiserver.yaml中的livenessProbe段，对于kube-apiserver来说，kubelet会通过访问: https://127.0.0.1:6443/healthz的方式去check是否ok？并且kubelet使用的是anonymous requests。由于上面我们已经关闭了对anonymous-requests的身份验证权，kubelet就会一直无法访问kube-apiserver的/healthz端点，导致kubelet认为kube-apiserver已经死亡，并尝试重启它。 调整/healthz检测的端点:12345678910111213141516171819spec: containers: - command: - kube-apiserver - --anonymous-auth=false ... ... - --insecure-bind-address=127.0.0.1 - --insecure-port=8080 livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 8080 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 15... ... 我们不再用anonymous-requests，但我们可以利用–insecure-bind-address和–insecure-port。让kubelet的请求到insecure port，而不是secure port。由于insecure port的流量不会受到身份验证、授权等功能的限制，因此可以成功probe到kube-apiserver的liveness，kubelet不会再重启kube-apiserver了。 重启kubelet服务：1systemctl restart kubelet Unauthorized参考http://tonybai.com/2017/07/20/fix-cannot-access-dashboard-in-k8s-1-6-4/k8s 1.6.x版本与1.5.x版本的一个很大不同在于1.6.x版本启用了RBAC的Authorization mode(授权模型)，但我们依旧通过basic auth方式进行apiserver的Authentication，而不是用客户端数字证书校验等其他方式：1234567spec: containers: - command: - kube-apiserver ... ... - --basic-auth-file=/etc/kubernetes/basic_auth_file ... ... 添加basic_auth_file内容：1echo \"admin,admin,2017\" &gt; /etc/kubernetes/basic_auth_file basic_auth_file格式为：1password,username,uid 参考完整的/etc/kubernetes/manifests/kube-apiserver.yaml内容：主要添加了以下内容：123456789- --anonymous-auth=false- --basic-auth-file=/etc/kubernetes/basic_auth_file- --insecure-bind-address=127.0.0.1- --insecure-port=8080host: 127.0.0.1 path: /healthz port: 8080 scheme: HTTP 完整的内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273apiVersion: v1kind: Podmetadata: creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-systemspec: containers: - command: - kube-apiserver - --anonymous-auth=false - --basic-auth-file=/etc/kubernetes/basic_auth_file - --insecure-bind-address=127.0.0.1 - --insecure-port=8080 - --secure-port=6443 - --requestheader-group-headers=X-Remote-Group - --requestheader-extra-headers-prefix=X-Remote-Extra- - --client-ca-file=/etc/kubernetes/pki/ca.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key - --allow-privileged=true - --storage-backend=etcd3 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds - --requestheader-allowed-names=front-proxy-client - --service-account-key-file=/etc/kubernetes/pki/sa.pub - --service-cluster-ip-range=10.96.0.0/12 - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key - --experimental-bootstrap-token-auth=true - --requestheader-username-headers=X-Remote-User - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt - --authorization-mode=RBAC - --advertise-address=192.168.10.6 - --etcd-servers=http://192.168.10.6:2379,http://192.168.10.7:2379,http://192.168.10.8:2379 image: gcr.io/google_containers/kube-apiserver-amd64:v1.7.5 livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 8080 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 15 name: kube-apiserver resources: requests: cpu: 250m volumeMounts: - mountPath: /etc/kubernetes/ name: k8s readOnly: true - mountPath: /etc/ssl/certs name: certs - mountPath: /etc/pki name: pki hostNetwork: true volumes: - hostPath: path: /etc/kubernetes name: k8s - hostPath: path: /etc/ssl/certs name: certs - hostPath: path: /etc/pki name: pkistatus: &#123;&#125; User “admin” cannot get at the cluster scopeadmin这个user并未得到足够的授权。这里我们要做的就是给admin选择一个合适的clusterrole。但kubectl并不支持查看user的信息，初始的clusterrolebinding又那么多，一一查看十分麻烦。我们知道cluster-admin这个clusterrole是全权限的，我们就来将admin这个user与clusterrole: cluster-admin bind到一起：1kubectl create clusterrolebinding login-on-dashboard-with-cluster-admin --clusterrole=cluster-admin --user=admin 重启kubelet服务后问题解决：1systemctl restart kubelet 访问https://192.168.10.6:6443/ui，用admin/admin就可以正常登录了。 heapster插件部署安装下面安装Heapster为集群添加使用统计和监控功能，为Dashboard添加仪表盘。 使用InfluxDB做为Heapster的后端存储，开始部署：12345678mkdir -p ~/k8s/heapstercd ~/k8s/heapsterwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yamlwget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yamlkubectl create -f ./ 或者 1234567wget https://github.com/kubernetes/heapster/archive/v1.4.2.zipunzip v1.4.2.zipcd heapster-1.4.2/deploy/kube-config/influxdbkubectl create -f ./cd heapster-1.4.2/deploy/kube-config/rbackubectl create -f ./ 具体可参考heapster.zip 如果是通过heapster.zip创建的话，因为有修改一些内容，需要执行：12kubectl create configmap influxdb-config --from-file=config.toml -n kube-systemkubectl create -f ./ 访问地址：cAdvisor:http://192.168.10.6:4194/http://192.168.10.7:4194/http://192.168.10.8:4194/注意：1.7版本cAdvisor已不能访问，可以手动在每台机器上安装cAdvisor：12345678910docker run --restart=always \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:rw \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/dev/disk/:/dev/disk:ro \\ --publish=4194:8080 \\ --detach=true \\ --name=cadvisor \\ google/cadvisor:latest grafana:https://192.168.10.6:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/http://192.168.10.6:30015/ influxdb:https://192.168.10.6:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdbhttp://192.168.10.6:8083/ 最后确认所有的pod都处于running状态，打开Dashboard,集群的使用统计会以仪表盘的形式显示出来。 异常处理ErrImagePull查看pod状态：12345678910111213[root@k8s-master temp]# kubectl get pod -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODEheapster-1528902802-hjjh8 1/1 Running 0 13s 10.244.66.3 k8s-node2kube-apiserver-k8s-master 1/1 Running 3 20h 192.168.10.6 k8s-masterkube-controller-manager-k8s-master 1/1 Running 27 21h 192.168.10.6 k8s-masterkube-dns-1783747724-3jb47 3/3 Running 3 4h 10.244.100.2 k8s-node1kube-proxy-3l1wf 1/1 Running 8 2d 192.168.10.7 k8s-node1kube-proxy-5mwcl 1/1 Running 10 2d 192.168.10.8 k8s-node2kube-proxy-s02wm 1/1 Running 7 2d 192.168.10.6 k8s-masterkube-scheduler-k8s-master 1/1 Running 27 21h 192.168.10.6 k8s-masterkubernetes-dashboard-2315583659-qt0vm 1/1 Running 13 1d 10.244.100.3 k8s-node1monitoring-grafana-973508798-wg055 0/1 ErrImagePull 0 13s 10.244.66.2 k8s-node2monitoring-influxdb-3871661022-jvpt9 0/1 ErrImagePull 0 12s 10.244.100.4 k8s-node1 发现grafana与influxdb状态为ErrImagePull，查看monitoring-grafana-973508798-wg055给你monitoring-influxdb-3871661022-jvpt9日志：12kubectl describe pod monitoring-influxdb-3871661022-jvpt9 -n kube-system 1m 10s 4 kubelet, k8s-node1 Warning FailedSync Error syncing pod, skipping: failed to \"StartContainer\" for \"influxdb\" with ErrImagePull: \"rpc error: code = 2 desc = Error: Status 405 trying to pull repository google_containers/heapster-influxdb-amd64: \\\"v1 Registry API is disabled. If you are not explicitly using the v1 Registry API, it is possible your v2 image could not be found. Verify that your image is available, or retry with `dockerd --disable-legacy-registry`. See https://cloud.google.com/container-registry/docs/support/deprecation-notices\\\"\" 应该是images的版本在registry中找不到。修改grafana.yaml与influxdb.yaml中image的路径：1234567#grafana.yaml#image: gcr.io/google_containers/heapster-grafana-amd64:v4.4.3image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2#influxdb.yaml#image: gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 删除后重新创建正常：12kubectl delete -f ./kubectl create -f ./ cannot list nodes at the cluster scope查看日志：12kubectl logs -f --tail 100 heapster-1528902802-6kzfk -n kube-systemE0903 07:09:35.016005 1 reflector.go:190] k8s.io/heapster/metrics/util/util.go:51: Failed to list *v1.Node: User \"system:serviceaccount:kube-system:heapster\" cannot list nodes at the cluster scope. (get nodes) 这个是由于rbac问题，需要执行：1kubectl create -f heapster-rbac.yaml ServiceUnavailable请等待kubernetes创建完成即可。 grafana: Problem! the server could not find the requested resource当访问https://192.168.10.6:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/时，出现Problem! the server could not find the requested resource。两种方案解决：方案1:修改grafana.yaml:12#value: /value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ 重新创建grafana：12kubectl delete -f grafana.yamlkubectl create -f grafana.yaml 访问https://192.168.10.6:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana，正常。 方案2:修改grafana.yaml:12345678910111213spec: # In a production setup, we recommend accessing Grafana through an external Loadbalancer # or through a public IP. # type: LoadBalancer # You could also use NodePort to expose the service at a randomly-generated port # type: NodePort ports: - port: 3000 targetPort: 3000 selector: k8s-app: grafana #externalIPs: #- 192.168.10.6 访问http://192.168.10.6:3000，正常。 或者：123456789101112spec: # In a production setup, we recommend accessing Grafana through an external Loadbalancer # or through a public IP. # type: LoadBalancer # You could also use NodePort to expose the service at a randomly-generated port type: NodePort ports: - port: 80 targetPort: 3000 nodePort: 30015 selector: k8s-app: grafana 访问http://192.168.10.6:30015，正常。 influxdb 404 page not found访问https://192.168.10.6:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb时出现404 page not found，这是因为influxdb 官方建议使用命令行或 HTTP API 接口来查询数据库，从 v1.1.0 版本开始默认关闭 admin UI，将在后续版本中移除 admin UI 插件。解决方案如下： 下载config.toml:123456789101112wget https://raw.githubusercontent.com/kubernetes/heapster/master/influxdb/config.toml添加：[admin] enabled = true bind-address = \":8083\" https-enabled = false https-certificate = \"/etc/ssl/influxdb.pem\"# 将修改后的配置写入到 ConfigMap 对象中#kubectl delete configmap influxdb-config -n kube-systemkubectl create configmap influxdb-config --from-file=config.toml -n kube-system 修改influxdb.yaml：1234567891011121314151617181920212223242526272829303132333435363738spec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: influxdb spec: containers: - name: influxdb #image: gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3 image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 volumeMounts: - mountPath: /data name: influxdb-storage - mountPath: /etc/ name: influxdb-config volumes: - name: influxdb-storage emptyDir: &#123;&#125; - name: influxdb-config configMap: name: influxdb-config...---...spec: ports: - name: http port: 8083 targetPort: 8083 - name: api port: 8086 targetPort: 8086 selector: k8s-app: influxdb externalIPs: - 192.168.10.6 重新创建influxdb：12kubectl delete -f influxdb.yamlkubectl create -f influxdb.yaml 访问http://192.168.10.6:8083，正常。 或者：1234567891011121314151617181920212223242526272829303132333435363738spec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: influxdb spec: containers: - name: influxdb #image: gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3 image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 volumeMounts: - mountPath: /data name: influxdb-storage - mountPath: /etc/ name: influxdb-config volumes: - name: influxdb-storage emptyDir: &#123;&#125; - name: influxdb-config configMap: name: influxdb-config...---...spec: type: \"NodePort\" ports: - name: http port: 8083 targetPort: 8083 nodePort: 30016 - name: api port: 8086 targetPort: 8086 selector: k8s-app: influxdb 重新创建influxdb：12kubectl delete -f influxdb.yamlkubectl create -f influxdb.yaml 访问http://192.168.10.6:30016，正常。 重新创建influxdb：12kubectl delete -f influxdb.yamlkubectl create -f influxdb.yaml 访问http://192.168.10.6:8083，正常。 efk插件插件部署安装12git clone https://github.com/kubernetes/kubernetes.gitgit checkout v1.7.5 进入源码的kubernetes/cluster/addons/fluentd-elasticsearch目录，需要定义了 elasticsearch 和 fluentd 使用的 Role 和 RoleBinding。添加es-rbac.yaml与fluentd-es-rbac.yaml两个文件：es-rbac.yml：1234567891011121314151617181920apiVersion: v1kind: ServiceAccountmetadata: name: elasticsearch namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1alpha1metadata: name: elasticsearchsubjects: - kind: ServiceAccount name: elasticsearch namespace: kube-systemroleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io fluentd-es-rbac.yaml：1234567891011121314151617181920apiVersion: v1kind: ServiceAccountmetadata: name: fluentd namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1alpha1metadata: name: fluentdsubjects: - kind: ServiceAccount name: fluentd namespace: kube-systemroleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io 修改es-controller.yaml，添加serviceAccountName:1234spec: serviceAccountName: elasticsearch containers: - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 修改fluentd-es-ds.yaml，添加serviceAccountName：12345spec: serviceAccountName: fluentd containers: - name: fluentd-es image: gcr.io/google_containers/fluentd-elasticsearch:1.22 给 Node 设置标签：DaemonSet fluentd-es-v1.22 只会调度到设置了标签 beta.kubernetes.io/fluentd-ds-ready=true 的 Node，需要在期望运行 fluentd 的 Node 上设置该标签；123456$ kubectl get nodesNAME STATUS AGE VERSIONk8s-node1 Ready 1d v1.7.5$ kubectl label nodes k8s-node1 beta.kubernetes.io/fluentd-ds-ready=true$ kubectl label nodes k8s-node2 beta.kubernetes.io/fluentd-ds-ready=true 创建：1kubectl create -f ./ 具体文件可参考EFK.zip 访问Kibana：地址为：https://192.168.10.6:6443/api/v1/proxy/namespaces/kube-system/services/kibana-logging 如果出现Error: ‘dial tcp 10.244.2.5:5601: connection refused的错误：查询kibana-logging日志：1234[root@k8s-master EFK]# kubectl logs kibana-logging-3757371098-bjrlh -n kube-systemELASTICSEARCH_URL=http://elasticsearch-logging:9200server.basePath: /api/v1/proxy/namespaces/kube-system/services/kibana-logging&#123;\"type\":\"log\",\"@timestamp\":\"2017-09-04T09:28:51Z\",\"tags\":[\"info\",\"optimize\"],\"pid\":5,\"message\":\"Optimizing and caching bundles for kibana and statusPage. This may take a few minutes\"&#125; 请耐心等待kibana后台完成即可。 prometheus监控安装参考kubernetes-prometheus12wget https://raw.githubusercontent.com/giantswarm/kubernetes-prometheus/master/manifests-all.yamlkubectl create -f manifests-all.yaml 配置文件请参考：prometheus.zip More Dashboards以下已经自动添加，无需再手动添加。 See grafana.net for some example dashboards and plugins. Configure Prometheus data source for Grafana.Grafana UI / Data Sources / Add data source Name: prometheusType: PrometheusUrl: http://prometheus:9090AddImport Prometheus Stats:Grafana UI / Dashboards / Import Grafana.net Dashboard: https://grafana.net/dashboards/2LoadPrometheus: prometheusSave &amp; OpenImport Kubernetes cluster monitoring:Grafana UI / Dashboards / Import Grafana.net Dashboard: https://grafana.net/dashboards/162LoadPrometheus: prometheusSave &amp; Open 访问地址：1234567891011121314[root@k8s-node1 ~]# kubectl get svc,ep -n monitoringNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc/alertmanager 10.111.106.237 &lt;nodes&gt; 9093:30582/TCP 14hsvc/grafana 10.108.245.18 &lt;nodes&gt; 3000:30718/TCP 14hsvc/kube-state-metrics 10.109.29.182 &lt;none&gt; 8080/TCP 14hsvc/prometheus 10.101.186.72 &lt;nodes&gt; 9090:32617/TCP 14hsvc/prometheus-node-exporter None &lt;none&gt; 9100/TCP 14hNAME ENDPOINTS AGEep/alertmanager 10.244.100.6:9093 14hep/grafana 10.244.100.8:3000 14hep/kube-state-metrics 10.244.100.9:8080,10.244.15.4:8080 14hep/prometheus 10.244.15.5:9090 14hep/prometheus-node-exporter 192.168.10.6:9100,192.168.10.7:9100 14h 可以看到：prometheus: http://192.168.10.6:32617grafana: http://192.168.10.6:30718 状态查询查看集群状态123456789kubectl cluster-infoKubernetes master is running at https://192.168.10.6:6443Heapster is running at https://192.168.10.6:6443/api/v1/proxy/namespaces/kube-system/services/heapsterKubeDNS is running at https://192.168.10.6:6443/api/v1/proxy/namespaces/kube-system/services/kube-dnsmonitoring-grafana is running at https://192.168.10.6:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafanamonitoring-influxdb is running at https://192.168.10.6:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdbTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 查看所有信息：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#kubectl get pod,svc,ep -o wide -n kube-system#kubectl get no,pod,svc,ep,deploy -o wide -n kube-system[root@k8s-master prometheus]# kubectl get pod,svc,ep -o wide --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGE IP NODEkube-system po/elasticsearch-logging-v1-0wlf2 1/1 Running 0 36m 10.244.96.4 k8s-node2kube-system po/elasticsearch-logging-v1-v7sg7 1/1 Running 0 36m 10.244.3.5 k8s-node1kube-system po/fluentd-es-v1.22-h8f5d 1/1 Running 0 36m 10.244.96.5 k8s-node2kube-system po/fluentd-es-v1.22-mrs8k 1/1 Running 0 36m 10.244.3.6 k8s-node1kube-system po/heapster-1528902802-h8grp 1/1 Running 0 44m 10.244.3.3 k8s-node1kube-system po/kibana-logging-3757371098-x5k1r 1/1 Running 0 36m 10.244.3.4 k8s-node1kube-system po/kube-apiserver-k8s-master 1/1 Running 0 43m 192.168.10.6 k8s-masterkube-system po/kube-controller-manager-k8s-master 1/1 Running 1 1h 192.168.10.6 k8s-masterkube-system po/kube-dns-3913472980-sr3sh 3/3 Running 0 1h 10.244.18.2 k8s-masterkube-system po/kube-proxy-dkmd4 1/1 Running 0 1h 192.168.10.7 k8s-node1kube-system po/kube-proxy-lbx6h 1/1 Running 0 1h 192.168.10.6 k8s-masterkube-system po/kube-proxy-lfw5w 1/1 Running 0 1h 192.168.10.8 k8s-node2kube-system po/kube-scheduler-k8s-master 1/1 Running 1 1h 192.168.10.6 k8s-masterkube-system po/kubernetes-dashboard-2315583659-rjt21 1/1 Running 0 1h 10.244.3.2 k8s-node1kube-system po/monitoring-grafana-241275065-n1wkt 1/1 Running 0 44m 10.244.96.2 k8s-node2kube-system po/monitoring-influxdb-2075516717-td73m 1/1 Running 0 44m 10.244.96.3 k8s-node2monitoring po/alertmanager-1970416631-v460d 1/1 Running 0 7m 10.244.96.6 k8s-node2monitoring po/grafana-core-2777256714-lvx7p 1/1 Running 0 7m 10.244.3.7 k8s-node1monitoring po/kube-state-metrics-2949788559-1ks16 1/1 Running 0 7m 10.244.96.8 k8s-node2monitoring po/kube-state-metrics-2949788559-2bx02 1/1 Running 0 7m 10.244.3.9 k8s-node1monitoring po/node-directory-size-metrics-9722j 2/2 Running 0 7m 10.244.3.8 k8s-node1monitoring po/node-directory-size-metrics-z1sqn 2/2 Running 0 7m 10.244.96.7 k8s-node2monitoring po/prometheus-core-466509865-3qt1c 1/1 Running 0 7m 10.244.96.9 k8s-node2monitoring po/prometheus-node-exporter-7l4bf 1/1 Running 0 7m 192.168.10.7 k8s-node1monitoring po/prometheus-node-exporter-b9v2w 1/1 Running 0 7m 192.168.10.8 k8s-node2NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORdefault svc/kubernetes 10.96.0.1 &lt;none&gt; 443/TCP 1h &lt;none&gt;kube-system svc/elasticsearch-logging 10.110.232.165 &lt;none&gt; 9200/TCP 37m k8s-app=elasticsearch-loggingkube-system svc/heapster 10.99.36.89 &lt;none&gt; 80/TCP 44m k8s-app=heapsterkube-system svc/kibana-logging 10.101.178.124 &lt;none&gt; 5601/TCP 36m k8s-app=kibana-loggingkube-system svc/kube-dns 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP 1h k8s-app=kube-dnskube-system svc/kubernetes-dashboard 10.99.64.238 &lt;none&gt; 80/TCP 1h k8s-app=kubernetes-dashboardkube-system svc/monitoring-grafana 10.99.186.236 &lt;nodes&gt; 80:30015/TCP 44m k8s-app=grafanakube-system svc/monitoring-influxdb 10.98.97.215 192.168.10.6 8083/TCP,8086/TCP 44m k8s-app=influxdbmonitoring svc/alertmanager 10.101.35.35 &lt;nodes&gt; 9093:32099/TCP 7m app=alertmanagermonitoring svc/grafana 10.97.63.144 &lt;nodes&gt; 3000:31989/TCP 7m app=grafana,component=coremonitoring svc/kube-state-metrics 10.101.180.83 &lt;none&gt; 8080/TCP 7m app=kube-state-metricsmonitoring svc/prometheus 10.110.91.88 &lt;nodes&gt; 9090:30059/TCP 7m app=prometheus,component=coremonitoring svc/prometheus-node-exporter None &lt;none&gt; 9100/TCP 7m app=prometheus,component=node-exporterNAMESPACE NAME ENDPOINTS AGEdefault ep/kubernetes 192.168.10.6:6443 1hkube-system ep/elasticsearch-logging 10.244.3.5:9200,10.244.96.4:9200 37mkube-system ep/heapster 10.244.3.3:8082 44mkube-system ep/kibana-logging 10.244.3.4:5601 36mkube-system ep/kube-controller-manager &lt;none&gt; 39mkube-system ep/kube-dns 10.244.18.2:53,10.244.18.2:53 1hkube-system ep/kube-scheduler &lt;none&gt; 39mkube-system ep/kubernetes-dashboard 10.244.3.2:9090 1hkube-system ep/monitoring-grafana 10.244.96.2:3000 44mkube-system ep/monitoring-influxdb 10.244.96.3:8086,10.244.96.3:8083 44mmonitoring ep/alertmanager 10.244.96.6:9093 7mmonitoring ep/grafana 10.244.3.7:3000 7mmonitoring ep/kube-state-metrics 10.244.3.9:8080,10.244.96.8:8080 7mmonitoring ep/prometheus 10.244.96.9:9090 7mmonitoring ep/prometheus-node-exporter 192.168.10.7:9100,192.168.10.8:9100 7m 参考 http://zerosre.com/2017/05/11/k8s%E6%96%B0%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85/http://www.tongtongxue.com/archives/16338.htmlhttp://tonybai.com/2017/07/20/fix-cannot-access-dashboard-in-k8s-1-6-4/http://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.htmlhttp://blog.frognew.com/2017/04/kubeadm-install-kubernetes-1.6.htmlhttp://blog.frognew.com/2017/07/kubeadm-install-kubernetes-1.7.htmlhttps://cloudnil.com/2017/07/10/Deploy-kubernetes1.6.7-with-kubeadm/https://www.centos.bz/2017/05/centos-7-kubeadm-install-k8s-kubernetes/http://leonlibraries.github.io/2017/06/15/Kubeadm%E6%90%AD%E5%BB%BAKubernetes%E9%9B%86%E7%BE%A4/http://www.jianshu.com/p/60069089c981https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/10-%E9%83%A8%E7%BD%B2Heapster%E6%8F%92%E4%BB%B6.mdhttp://jimmysong.io/blogs/kubernetes-installation-on-centos/http://jimmysong.io/blogs/kubernetes-ha-master-installation/http://c.isme.pub/2016/11/22/docker-kubernetes/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/tags/kubernetes/"}]},{"title":"Centos ULIMIT资源限制","slug":"Centos-ULIMIT资源限制","date":"2017-02-06T09:55:40.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/02/Centos-ULIMIT资源限制.html","link":"","permalink":"http://blog.gcalls.cn/2017/02/Centos-ULIMIT资源限制.html","excerpt":"Centos下是通过ulimit进行资源限制，本文介绍一下ulimit的详细设置。","text":"Centos下是通过ulimit进行资源限制，本文介绍一下ulimit的详细设置。 默认配置默认情况下，ulimit是最小设置。重点参数为open files与max user processes，open files默认为1024，max user processes默认为3870。 可能通过以下命令查看：1234567891011121314151617181920212223242526#查看所有的参数：[root@zhaoxy ~]# ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 3870max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 3870virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited#查看max user processes参数：[root@zhaoxy ~]# ulimit -u3870#查看open files参数：[root@zhaoxy ~]# ulimit -n1024 尤其是open files参数，过小时会造成程序在高并发时出现Too many open files的错误。 登录用户配置调整在centos 5/6 等版本中，资源限制的配置可以在/etc/security/limits.conf设置，针对root/user等各个用户或者*代表所有用户来设置。 当然，/etc/security/limits.d/ 中可以配置，系统是先加载limits.conf然后按照英文字母顺序加载limits.d目录下的配置文件，后加载配置覆盖之前的配置。 12345678#添加以下参数：[root@zhaoxy ~]# vim /etc/security/limits.conf#* - nofile 65535#* - nproc 65535* soft nofile 65535* hard nofile 65535* soft nproc 65535* hard nproc 65535 其中：*： 代表所有的用户nofile：代表open filesnproc： 代表max user processessoft： 代表一个警告值，超过这个范围，会出现警告，但不会报错hard： 代表一个真正意义的阀值，超过就会报错-: 代表soft与hard 注意：系统是先加载/etc/security/limits.conf中的配置，再加载/etc/security/limits.d/中的配置，后者会覆盖前者。当用户为*时，受/etc/security/limits.d/中的配置限制，当指定具体的用户时，不受/etc/security/limits.d/中的限制。 当/etc/security/limits.conf中指定的用户为*时，需要修改/etc/security/limits.d/20-nproc.conf文件：123[root@zhaoxy ~]# vim /etc/security/limits.d/20-nproc.conf#* soft nproc 65535root soft nproc unlimited SYSTEMD SERVICE配置调整在CentOS 7/RHEL 7的系统中，使用Systemd替代了之前的SysV，因此/etc/security/limits.conf文件的配置作用域缩小了一些。limits.conf这里的配置，只适用于通过PAM认证登录用户的资源限制，它对systemd的service的资源限制不生效。登录用户的限制，通过/etc/security/limits.conf和limits.d来配置即可。 对于systemd service的资源限制，全局的配置，放在文件/etc/systemd/system.conf和/etc/systemd/user.conf。 同时，也会加载两个对应的目录中的所有.conf文件/etc/systemd/system.conf.d/.conf和/etc/systemd/user.conf.d/.conf其中，system.conf是系统实例使用的，user.conf用户实例使用的。一般的sevice，使用system.conf中的配置即可。systemd.conf.d/*.conf中配置会覆盖system.conf。 可以添加默认参数，修改/etc/systemd/system.conf配置：1234[Manager]DefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000 注意：修改了system.conf后，需要重启系统才会生效。 针对单个Service，也可以设置，以nginx为例。虽然我们修改了/etc/security/limits.conf与/etc/security/limits.d/20-nproc.conf文件，但通过systemctl启动的服务还是默认的参数：123456[root@zhaoxy proc]# cat /proc/3347/limits Limit Soft Limit Hard Limit Units ... Max processes 3870 3870 processes Max open files 1024 4096 files ... 我们需要编辑/usr/lib/systemd/system/nginx.service 文件，添加以下参数：1234[Service]LimitCORE=infinityLimitNOFILE=100000LimitNPROC=100000 然后重启：12systemctl daemon-reloadsystemctl restart nginx.service 再看看修改后的配置：123456[root@zhaoxy proc]# cat /proc/3347/limits Limit Soft Limit Hard Limit Units ... Max processes 100000 100000 processes Max open files 100000 100000 files ... 参考 http://www.cnblogs.com/MYSQLZOUQI/p/5054559.htmlhttp://smilejay.com/2016/06/centos-7-systemd-conf-limits","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"用fpm制作RPM包","slug":"用fpm制作RPM包","date":"2017-01-19T07:14:00.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/用fpm制作RPM包.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/用fpm制作RPM包.html","excerpt":"生成自己的RPM包太麻烦了，使用FPM可以很轻松地生成RPM包，在此记录一下centos7的安装过程。","text":"生成自己的RPM包太麻烦了，使用FPM可以很轻松地生成RPM包，在此记录一下centos7的安装过程。 安装12345678910111213#yum install gcc gcc-c++ make autoconf libevent libevent-devel[root@k8s-master etcd]# yum -y install ruby rubygems ruby-devel gcc rpm-build[root@k8s-master etcd]# gem sources --add https://gems.ruby-china.org/ --remove https://rubygems.org/[root@k8s-master etcd]# gem sources -l*** CURRENT SOURCES ***https://gems.ruby-china.org/[root@k8s-master etcd]# gem install fpm[root@k8s-master rpmfiles]# fpm -v1.7.0 基于已存在rpm包制作12345678910111213141516171819202122232425262728293031323334353637[root@k8s-master rpmfiles]# mkdir rpmfiles[root@k8s-master rpmfiles]# cp etcd-3.0.15-1.x86_64.rpm ./rpmfiles/[root@k8s-master rpmfiles]# cd rpmfiles/#查看script:[root@k8s-master rpmfiles]# rpm -qp --scripts etcd-3.0.15-1.x86_64.rpmwarning: etcd-3.0.15-1.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID c33abc74: NOKEYpreinstall scriptlet (using /bin/sh):getent group etcd &gt;/dev/null || groupadd -r etcdgetent passwd etcd &gt;/dev/null || useradd -r -g etcd -d /var/lib/etcd \\ -s /sbin/nologin -c \"etcd user\" etcdpostinstall scriptlet (using /bin/sh):if [ $1 -eq 1 ] ; then # Initial installation systemctl preset etcd.service &gt;/dev/null 2&gt;&amp;1 || :fichown -R etcd.etcd /var/lib/etcdpreuninstall scriptlet (using /bin/sh):if [ $1 -eq 0 ] ; then # Package removal, not upgrade systemctl --no-reload disable etcd.service || : systemctl stop etcd.service || :fipostuninstall scriptlet (using /bin/sh):systemctl daemon-reload &gt;/dev/null 2&gt;&amp;1 || :#将以上内容生成为以下文件：postinstall.shpostuninstall.shpreinstall.shpreuninstall.sh#解压文件:[root@k8s-master rpmfiles]# rpm2cpio etcd-3.0.15-1.x86_64.rpm | cpio -idmv#制作：fpm -s dir -t rpm -n \"etcd\" -v 3.x-1.x86_64 --pre-install preinstall.sh --post-install postinstall.sh --pre-uninstall preuninstall.sh --post-uninstall postuninstall.sh etc usr var 参考 https://mritd.me/2016/08/02/yum-%E7%AC%94%E8%AE%B0/https://mritd.me/2016/09/13/%E5%9F%BA%E4%BA%8E%E5%AE%98%E6%96%B9-rpm-%E5%BF%AB%E9%80%9F%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89-rpm/","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"ssl证书制作","slug":"ssl证书制作","date":"2017-01-19T04:46:33.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/ssl证书制作.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/ssl证书制作.html","excerpt":"之前公司网站转为使用https方式访问，在此记录一下过程。","text":"之前公司网站转为使用https方式访问，在此记录一下过程。 EasyRSA参考https://sskaje.me/2015/09/easy-rsa-3-howto/ 安装12345678910111213141516171819202122232425wget https://github.com/OpenVPN/easy-rsa/releases/download/v3.0.3/EasyRSA-3.0.3.zipunzip EasyRSA-3.0.3.zip &amp;&amp; mv EasyRSA-3.0.3 /usr/local/ cd /usr/local/EasyRSA-3.0.3wget -O vars https://raw.githubusercontent.com/OpenVPN/easy-rsa/master/easyrsa3/vars.examplecat &gt;&gt; /usr/local/EasyRSA-3.0.3/vars &lt;&lt; EOFset_var EASYRSA \"\\$PWD\"set_var EASYRSA_PKI \"\\$EASYRSA/pki\"set_var EASYRSA_DN \"cn_only\"set_var EASYRSA_REQ_COUNTRY \"CN\"set_var EASYRSA_REQ_PROVINCE \"Guangdong\"set_var EASYRSA_REQ_CITY \"Shenzhen\"set_var EASYRSA_REQ_ORG \"PTC\"set_var EASYRSA_REQ_EMAIL \"zhaoxunyong@qq.com\"set_var EASYRSA_REQ_OU \"Devops\"set_var EASYRSA_KEY_SIZE 2048set_var EASYRSA_ALGO rsaset_var EASYRSA_CA_EXPIRE 7500set_var EASYRSA_CERT_EXPIRE 365set_var EASYRSA_NS_SUPPORT \"no\"set_var EASYRSA_NS_COMMENT \"SSKAJE CERTIFICATE AUTHORITY\"set_var EASYRSA_EXT_DIR \"\\$EASYRSA/x509-types\"set_var EASYRSA_SSL_CONF \"\\$EASYRSA/openssl-1.0.cnf\"set_var EASYRSA_DIGEST \"sha256\"EOF 初始化PKI1./easyrsa init-pki 创建CA1./easyrsa build-ca 生成证书生成证书的操作步骤就两步，生成请求文件，根据请求文件签发证书。easy-rsa 3.0签发证书时要求制定type，可选的值参考x509-types目录下的文件名，包括 server：TLS服务端，适用于https服务端和vpn服务端client：TLS客户端，适用于web浏览器和vpn客户端ca：签发子CA证书gen-req, build-client-full, build-server-full 可以使用 nopass 参数生成不加密的私钥。 一步创建：可以使用 build-client-full 和 build-server-full 直接完成 gen-req 和 sign-req 的过程： 123456#./easyrsa build-client-full abc.com nopass#./easyrsa build-client-full abc.com nopass#gen-req and sign-req，对应的文件位于pki/reqs/与pki/private目录下#./easyrsa build-server-full abc.com$HOSTNAME=abc./easyrsa --subject-alt-name=\"DNS:$HOSTNAME,DNS:www.example.net\" build-server-full $HOSTNAME nopass abc.com:为文件名称。nopass:生成不加密的私钥。 也可以分步创建：生成请求: 1./easyrsa gen-req abc.com abc.com为文件名称，最终会生成abc.com.key给你abc.com.req文件。 签发证书: 1./easyrsa sign-req server abc.com abc.com为文件名称。 签发req文件:如果req文件是外部创建的，可以使用 import-req 导入，再用 sign-req 签发: 1./easyrsa import-req &lt;request_file_path&gt; &lt;short_basename&gt; 导出PKCS 7/PKCS 121./easyrsa export-p7 abc.com 查看证书/查看请求文件使用 show-cert 和 show-req 查看请求文件，参数是请求时的名字： 12./easyrsa show-cert abc.com./easyrsa show-req abc.com 更新数据库1./easyrsa update-db 使用2.x版本12345678910111213141516171819202122yum install epel-releaseyum install easy-rsacd /usr/share/easy-rsa/2.0#修改vars 相关变量：export KEY_COUNTRY=\"CN\"export KEY_PROVINCE=\"Guangdong\"export KEY_CITY=\"shenzhen\"export KEY_ORG=\"abc.com\"export KEY_EMAIL=\"admin@abc.com\"export KEY_OU=\"OPS\"#执行以下命令：. vars #清除文件./clean-all#生成服务端CA./build-ca#生成服务端证书./build-key-server server#生成客户端证书./build-key client1 openssl生成https/ssl的证书假设生成根域名证书，域名为：*.abc.com。 颁发证书如果是对外的服务，需要公共CA机构签署，不需要这步。内网环境CA要给别人颁发证书，首先自己得有一个作为根证书：12345678910111213#初始化cd /etc/pki/CA/touch index.txt serialecho '01' &gt; serial#生成根密钥#为了安全起见，修改cakey.pem私钥文件权限为600或400umask 077; openssl genrsa -out private/cakey.pem 2048#生成根证书openssl req \\-subj \"/C=CN/ST=GuangDong/L=ShenZhen/O=PTC/OU=Devops/CN=*.abc.com\" \\-new -x509 -key private/cakey.pem -out cacert.pem 如果没有加-subj参数时，需要输入以下相关信息： 为web服务器生成ssl密钥方法一：创建签名请求，然后通过私有CA签署：1234567891011121314#openssl req -newkey rsa:4096 -nodes -sha256 -keyout abc.com.key -out abc.com.csr#-reqexts SAN -config参数为添加SAN信息，如果不需要SAN的话，可以不用。openssl req \\-subj \"/C=CN/ST=GuangDong/L=ShenZhen/O=PTC/OU=Devops/CN=*.abc.com\" \\-reqexts SAN -config &lt;(cat /etc/pki/tls/openssl.cnf &lt;(printf \"[SAN]\\nsubjectAltName=DNS:www.abc.com,IP:192.168.10.6\")) \\-newkey rsa:2048 -keyout abc.com.key -out abc.com.csr#或者#openssl genrsa -out abc.com.key 2048#openssl req -subj \"/C=CN/ST=GuangDong/L=ShenZhen/O=PTC/OU=Devops/CN=*.abc.com\" -new -key abc.com.key -out abc.com.csr# 自己签署#一般内网环境采用自己签署(如果是对外的服务，需要公共CA机构签署)：openssl ca -in abc.com.csr -out abc.com.crt -cert /etc/pki/CA/cacert.pem -keyfile /etc/pki/CA/private/cakey.pem -outdir ./#openssl x509 -req -in abc.com.csr -CA /etc/pki/CA/cacert.pem -CAkey /etc/pki/CA/private/cakey.pem -CAcreateserial -out abc.com.crt -days 365 -extensions v3_req -extfile openssl.cnf 方法二：创建签名请求，创建一个加密的私钥，然后通过私钥签署：12345678910111213141516# 生成私钥：openssl genrsa -des3 -out abc.key.pem.encrypted 2048# 生成解密后的private key：openssl rsa -in abc.key.pem.encrypted -out abc.com.key# 创建签名请求：#-reqexts SAN -config参数为添加SAN信息，如果不需要SAN的话，可以不用。openssl req -utf8 -config /etc/pki/tls/openssl.cnf \\-subj \"/C=CN/ST=GuangDong/L=ShenZhen/O=PTC/OU=Devops/CN=*.abc.com\" \\-reqexts SAN -config &lt;(cat /etc/pki/tls/openssl.cnf &lt;(printf \"[SAN]\\nsubjectAltName=DNS:www.abc.com,IP:192.168.10.6\")) \\-new -key abc.key.pem.encrypted \\-out abc.req.csr -days 365# 自己签署#一般内网环境采用自己签署(如果是对外的服务，需要公共CA机构签署)：openssl x509 -req -days 365 -signkey abc.key.pem.encrypted -in abc.req.csr -out abc.cert.crt 公共CA机构签署可以购买geotrust服务或者申请startssl免费一年的服务，具体请参考免费申请-StartSSL-证书 12345678910111213141516171819202122232425262728293031323334353637# 生成key與csropenssl req -newkey rsa:2048 -keyout abc.com.key -out abc.com.csr# 将csr内容上传geotrust后，会生成IntermediateCA.crt、ssl_certificate.crt两个文件cat ssl_certificate.crt IntermediateCA.crt &gt;&gt; abc.com.crt### 添加#将abc.com.crt与abc.key.pem加入到nginx中即可，比如：server &#123; listen 80; server_name abc.com; rewrite ^(.*) https://$server_name$1 permanent;&#125;server &#123; listen 443; server_name abc.com; access_log /var/log/nginx/your-domain.log main; ssl on; ssl_certificate /etc/nginx/ssl/abc.com.crt; ssl_certificate_key /etc/nginx/ssl/abc.com.key; location / &#123; log_not_found on; proxy_pass http://127.0.0.1:8080; proxy_read_timeout 300; proxy_connect_timeout 300; proxy_redirect off; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; &#125;&#125; 私有CA签署时，浏览器不信任证书的解决方案以macos为例：12#下载ca文件sz /etc/pki/CA/cacert.pem 然后再导入系统，具体请参考：解决Mac Chrome打开HTTPS证书错误问题 SAN什么是 SAN，SAN（Subject Alternative Name）是 SSL 标准 x509 中定义的一个扩展。使用了 SAN 字段的 SSL 证书，可以扩展此证书支持的域名，使得一个证书可以支持多个不同域名的解析。比如说：Common Name 字段是 *.google.com，这张证书通过SAN就能够被 www.youtube.com 这个域名所使用。原因就是这是一张带有 SAN 扩展的证书。所以，只执行 NO SAN 命令也可以签发证书，不过却不能够添加多个域名。 想要添加多个域名或泛域名，你需要使用到该扩展。那么默认的 OpenSSL 的配置是不能够满足的，我们需要复制或下载一份默认的 openssl.cnf 文件到本地。如 github openssl。 修改匹配策略：默认匹配策略是：国家名，省份，组织名必须相同（match）。我们改为可选（optional），这样避免我们生成证书请求文件时（csr）去参考 CA 证书。 编辑/etc/pki/tls/openssl.cnf文件：12345678# For the CA policy[ policy_match ]countryName = matchstateOrProvinceName = optionalorganizationName = optionalorganizationalUnitName = optionalcommonName = suppliedemailAddress = optional 以上为可选项。 修改默认值：修改默认值，可以让你更快的颁发证书，一直回车就可以了：编辑/etc/pki/tls/openssl.cnf文件：1234567891011121314151617181920212223242526272829[ req_distinguished_name ]countryName = Country Name (2 letter code)countryName_default = CNcountryName_min = 2countryName_max = 2stateOrProvinceName = State or Province Name (full name)stateOrProvinceName_default = GuangdonglocalityName = Locality Name (eg, city)localityName_default = Shenzhen0.organizationName = Organization Name (eg, company)0.organizationName_default = Dev# we can do this but it is not needed normally :-)#1.organizationName = Second Organization Name (eg, company)#1.organizationName_default = World Wide Web Pty LtdorganizationalUnitName = Organizational Unit Name (eg, section)organizationalUnitName_default = DevcommonName = Common Name (eg, your name or your server\\&apos;s hostname)commonName_max = 64emailAddress = zhaoxunyong@qq.comemailAddress_max = 64# SET-ex3 = SET extension number 3 关键步骤：最关键的地方是修改 v3_req。添加成如下： 12345678910111213141516[ v3_req ]# Extensions to add to a certificate requestbasicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[ alt_names ]DNS.1 = abc.comDNS.2 = *.abc.comDNS.3 = localhostIP.1 = 127.0.0.1IP.2 = 192.168.10.6IP.3 = 192.168.10.7IP.4 = 192.168.10.8 环境变量：openssl 通过 $ENV::name 获取环境变量，在配置文件里使用的时候只需将 name 替换为需要用到的环境变量的名称就可以了： 1$ export SNAS=DNS:abc.com,DNS:*.abc.com,DNS:xyz.com,IP:127.0.0.1 123456789101112131415161718# 修改 openssl.cnf[ v3_req ]# Extensions to add to a certificate requestbasicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = $ENV::SANS# 注释掉这段配置#[ alt_names ]#DNS.1 = abc.com#DNS.2 = *.abc.com#DNS.3 = localhost#IP.1 = 127.0.0.1#IP.2 = 192.168.10.6#IP.3 = 192.168.10.7#IP.4 = 192.168.10.8 jks证书123#生成jks：keytool -genkey -alias abc.com -keyalg RSA -keystore abc.jks -keysize 2048 -dname \"CN=*.abc.com,OU=,O=xxxx有限公司,L=深圳市,ST=广东省,C=CN\" -storepass \"Aa123456\" -keypass \"Aa123456\"#注意：CN表示颁发给哪个url，可以用*.abc.com表示所有 转换jks为OpenSSL的PEM格式文件(.key + .crt) http://ju.outofmemory.cn/entry/212469 先导出p12：1keytool -importkeystore -srckeystore abc.jks -destkeystore abc.p12 -srcstoretype jks -deststoretype pkcs12 方式一：生成pem证书(包含了key，server证书和ca证书)：1234567891011# 生成key 加密的pem证书$ openssl pkcs12 -in abc.p12 -out abc.key.pemEnter Import Password:MAC verified OKEnter PEM pass phrase:Verifying - Enter PEM pass phrase: # 生成key 非加密的pem证书$ openssl pkcs12 -nodes -in server.p12 -out abc.key.pemEnter Import Password:MAC verified OK 剩下步骤与为web服务器生成ssl密钥类似。 方式二：单独导出key：123456#生成key加密的pem证书：$ openssl pkcs12 -in server.p12 -out abc.key.pem#生成非加密的key：#(只保留BEGIN与END中的内容，包括BEGIN与END)openssl pkcs12 -in abc.p12 -nocerts -nodes -out abc.key.pem 单独导出server证书:1openssl pkcs12 -in abc.p12 -nokeys -cacerts -out abc.cert.crt 单独导出ca证书:1openssl pkcs12 -in abc.p12 -nokeys -cacerts -out ca.crt 方式三：12#生成csr文件：keytool -certreq -keyalg RSA -alias abc.com -file abc.req.csr -keystore abc.jks -storepass \"Aa123456\" -keypass \"Aa123456\" 生成私钥:windows下运行kestore-export中的工具：1234567JKS2PFX.bat abc.jks \"Aa123456\" \"abc.com\" \"key/abc\" \"D:\\Developer\\java\\jdk1.7.0_51\\jre\\bin\"#上传key/abc.com.key到linux下并执行：openssl rsa -in abc.com.key -des3 -out abc.key.encrypted#自己签署csr文件：#openssl x509 -req -days 365 -in abc.req.csr -signkey abc.key.encrypted -out abc.cert.crt 12#将csr内容上传geotrust后，会生成IntermediateCA.crt、ssl_certificate.crt两个文件cat ssl_certificate.crt IntermediateCA.crt &gt;&gt; abc.com.crt 格式转换 https://support.wosign.com/index.php?/Knowledgebase/Article/View/36/0/ key、crt转换为p121openssl pkcs12 -export -clcerts -in abc.cert.crt -inkey abc.key.pem -out abc.p12 key、crt转换为pfx1openssl pkcs12 -export -inkey abc.key.pem -in abc.com.crt -out abc.pfx pfx转换jks1keytool -importkeystore -srckeystore abc.pfx -destkeystore abc.com.jks -srcstoretype PKCS12 -deststoretype JKS crt转换为jks1keytool -import -v -trustcacerts -storepass \"Aa123456\" -alias abc.com -file abc.cert.crt -keystore abc.jks 参考 https://deepzz.com/post/based-on-openssl-privateCA-issuer-cert.htmlhttp://liaoph.com/openssl-san/https://mritd.me/2016/07/03/Harbor-%E4%BC%81%E4%B8%9A%E7%BA%A7-Docker-Registry-HTTPS%E9%85%8D%E7%BD%AE/https://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/https://mritd.me/2016/06/22/%E5%85%8D%E8%B4%B9%E7%94%B3%E8%AF%B7-StartSSL-%E8%AF%81%E4%B9%A6/http://seanlook.com/2015/01/18/openssl-self-sign-ca/http://seanlook.com/2015/01/15/openssl-certificate-encryption/http://seanlook.com/2015/01/07/tls-ssl/#commentshttp://netsecurity.51cto.com/art/200602/21066.htmhttp://cnzhx.net/blog/self-signed-certificate-as-trusted-root-ca-in-windows/http://www.cnblogs.com/snandy/p/3262661.htmlhttp://www.iloveandroid.net/2015/10/14/jksTopm/","categories":[{"name":"SSL","slug":"SSL","permalink":"http://blog.gcalls.cn/categories/SSL/"}],"tags":[{"name":"SSL","slug":"SSL","permalink":"http://blog.gcalls.cn/tags/SSL/"}]},{"title":"etcd集群安装","slug":"etcd集群安装","date":"2017-01-16T10:23:01.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/etcd集群安装.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/etcd集群安装.html","excerpt":"etcd是一个应用在分布式环境下的 key/value 存储服务。利用etcd的特性，应用程序可以在集群中共享信息、配置或作服务发现，etcd会在集群的各个节点中复制这些数据并保证这些数据始终正确。etcd无论是在 CoreOS还是Kubernetes体系中都是不可或缺的一环。","text":"etcd是一个应用在分布式环境下的 key/value 存储服务。利用etcd的特性，应用程序可以在集群中共享信息、配置或作服务发现，etcd会在集群的各个节点中复制这些数据并保证这些数据始终正确。etcd无论是在 CoreOS还是Kubernetes体系中都是不可或缺的一环。 rpm安装安装：12yum install -y etcd-3.3.11-2.el7.centos.x86_64#yum install etcd-3.1.9-2.el7.x86_64 版本：3.3.11 配置etcd0:1234567891011121314151617tee /etc/etcd/etcd.conf &lt;&lt; EOF#[Member]ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"ETCD_LISTEN_PEER_URLS=\"http://0.0.0.0:2380\"ETCD_LISTEN_CLIENT_URLS=\"http://0.0.0.0:2379,http://0.0.0.0:4001\"ETCD_NAME=\"master\"##[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://192.168.80.94:2380\"ETCD_ADVERTISE_CLIENT_URLS=\"http://192.168.80.94:2379,http://192.168.80.94:4001\"ETCD_INITIAL_CLUSTER=\"master=http://192.168.80.94:2380,node1=http://192.168.80.97:2380,node2=http://192.168.80.99:2380\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_INITIAL_CLUSTER_STATE=\"new\"EOF# 查看是否正确grep -v ^# /etc/etcd/etcd.conf etcd1:1234567891011121314tee /etc/etcd/etcd.conf &lt;&lt; EOF#[Member]ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"ETCD_LISTEN_PEER_URLS=\"http://0.0.0.0:2380\"ETCD_LISTEN_CLIENT_URLS=\"http://0.0.0.0:2379,http://0.0.0.0:4001\"ETCD_NAME=\"node1\"##[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://192.168.80.97:2380\"ETCD_ADVERTISE_CLIENT_URLS=\"http://192.168.80.97:2379,http://192.168.80.97:4001\"ETCD_INITIAL_CLUSTER=\"master=http://192.168.80.94:2380,node1=http://192.168.80.97:2380,node2=http://192.168.80.99:2380\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_INITIAL_CLUSTER_STATE=\"new\"EOF etcd2:1234567891011121314tee /etc/etcd/etcd.conf &lt;&lt; EOF#[Member]ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"ETCD_LISTEN_PEER_URLS=\"http://0.0.0.0:2380\"ETCD_LISTEN_CLIENT_URLS=\"http://0.0.0.0:2379,http://0.0.0.0:4001\"ETCD_NAME=\"node2\"##[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://192.168.80.99:2380\"ETCD_ADVERTISE_CLIENT_URLS=\"http://192.168.80.99:2379,http://192.168.80.99:4001\"ETCD_INITIAL_CLUSTER=\"master=http://192.168.80.94:2380,node1=http://192.168.80.97:2380,node2=http://192.168.80.99:2380\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_INITIAL_CLUSTER_STATE=\"new\"EOF 启动1234systemctl daemon-reloadsystemctl enable etcdsystemctl start etcdsystemctl status etcd 测试12etcdctl cluster-healthetcdctl --endpoints \"http://192.168.80.94:2379,http://192.168.80.97:2379,http://192.168.80.99:2379\" member list 基于已有集群的服务发现获取集群标识 size 代表要创建的集群大小：123curl -w \"\\n\" 'https://discovery.etcd.io/new?size=3'# 返回如下https://discovery.etcd.io/f6a252c5240cc89b91fa00dac95d5732 设置集群标识,删除掉 ETCD_INITIAL_CLUSTER 字段,添加：1ETCD_DISCOVERY=\"https://discovery.etcd.io/f6a252c5240cc89b91fa00dac95d5732\" 也可以通过已有的集群自动发现：首先需要在已经搭建的etcd中创建用于发现的url123curl -X PUT http://192.168.10.16:2379/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size -d value=3#返回：&#123;\"action\":\"set\",\"node\":&#123;\"key\":\"/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size\",\"value\":\"3\",\"modifiedIndex\":170010,\"createdIndex\":170010&#125;&#125; 其中192.168.10.16为另外的etcd集群环境。 如上表示创建一个集群大小为3的etcd发现url，创建成功后按如下配置启动各节点，可以参考手动启动的命令：12345./etcd --name infra0 --initial-advertise-peer-urls http://192.168.10.6:2380 \\ --listen-peer-urls http://192.168.10.6:2380 \\ --listen-client-urls http://192.168.10.6:2379,http://127.0.0.1:2379 \\ --advertise-client-urls http://192.168.10.6:2379 \\ --discovery http://192.168.10.16:2379/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83 参考 https://mritd.me/2016/09/01/Etcd-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/http://blog.csdn.net/u010511236/article/details/52386229","categories":[{"name":"docker","slug":"docker","permalink":"http://blog.gcalls.cn/categories/docker/"},{"name":"kubernetes","slug":"docker/kubernetes","permalink":"http://blog.gcalls.cn/categories/docker/kubernetes/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.gcalls.cn/tags/docker/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/tags/kubernetes/"}]},{"title":"使用Flannel搭建docker网络","slug":"使用Flannel搭建docker网络","date":"2017-01-16T09:18:45.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/使用Flannel搭建docker网络.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/使用Flannel搭建docker网络.html","excerpt":"docker跨宿主机的网络解决方案有几种： 直接路由+quagga calico flannel weave calico与flannel综合性能比还是很不错，建议使用。本文详细介绍flannel的安装与配置。","text":"docker跨宿主机的网络解决方案有几种： 直接路由+quagga calico flannel weave calico与flannel综合性能比还是很不错，建议使用。本文详细介绍flannel的安装与配置。 具体网络模型如图所示： 本文详细介绍一下flannel的安装与配置。 安装安装etcd参考etcd集群安装 rpm安装安装1yum install -y flannel 版本：0.7.1 配置在etcd中设置flannel所使用的ip段:1etcdctl --endpoints \"http://192.168.10.6:2379,http://192.168.10.7:2379,http://192.168.10.8:2379\" set /coreos.com/network/config '&#123;\"NetWork\":\"10.244.0.0/16\"&#125;' 每台执行：123456789$ sed -i 's;^FLANNEL_ETCD_ENDPOINTS=.*;FLANNEL_ETCD_ENDPOINTS=\"http://192.168.10.6:2379,http://192.168.10.7:2379,http://192.168.10.8:2379\";g' \\/etc/sysconfig/flanneld$ sed -i 's;^FLANNEL_ETCD_PREFIX=.*;FLANNEL_ETCD_PREFIX=\"/coreos.com/network\";g' \\/etc/sysconfig/flanneld$ grep -v ^# /etc/sysconfig/flanneldFLANNEL_ETCD_ENDPOINTS=\"http://192.168.10.6:2379,http://192.168.10.7:2379,http://192.168.10.8:2379\"FLANNEL_ETCD_PREFIX=\"/coreos.com/network\" 如果是vagrant启动的虚拟机的话，会多个10.0.2.15的eth0网段，需要添加–iface参数，需要修改/usr/lib/systemd/system/flanneld.service：12345678$ sed -i 's;^ExecStart=.*;ExecStart=/usr/bin/flanneld-start --iface=eth1 -etcd-endpoints=$&#123;FLANNEL_ETCD_ENDPOINTS&#125; -etcd-prefix=$&#123;FLANNEL_ETCD_PREFIX&#125; $FLANNEL_OPTIONS;g' \\/usr/lib/systemd/system/flanneld.service启动服务：systemctl daemon-reloadsystemctl enable flanneldsystemctl start flanneldsystemctl status flanneld 在service脚本中，会自动通过以下命令生成docker bip所需要的环境变量：1234567[root@k8s-master ~]# /usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker[root@k8s-master ~]# cat /run/flannel/dockerDOCKER_OPT_BIP=\"--bip=10.244.38.1/24\"DOCKER_OPT_IPMASQ=\"--ip-masq=true\"DOCKER_OPT_MTU=\"--mtu=1472\"DOCKER_NETWORK_OPTIONS=\" --bip=10.244.38.1/24 --ip-masq=true --mtu=1472\" docker网段修改：a. 修改docker网段：1234567891011sed -i -e '/ExecStart=/iEnvironmentFile=/run/flannel/docker' -e 's;^ExecStart=/usr/bin/dockerd;ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS;g' \\/usr/lib/systemd/system/docker.service#$ vim /usr/lib/systemd/system/docker.service#EnvironmentFile=/run/flannel/docker#ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS#重启docker服务systemctl daemon-reloadsystemctl enable dockersystemctl restart docker b. 手动修改docker网段：不建议。也可以在docker服务启动后，手动修改docker网段，不过每次开机都要执行，很麻烦。建议采用：修改docker网段：12source /run/flannel/subnet.envifconfig docker0 $&#123;FLANNEL_SUBNET&#125; docker方式不建议。安装flannel:12345678910111213etcdctl rm /coreos.com/network/ --recursiveetcdctl mk /coreos.com/network/config '&#123;\"NetWork\":\"10.244.0.0/16\"&#125;'#etcdctl set /coreos.com/network/config '&#123;\"NetWork\":\"10.244.0.0/16\"&#125;'docker run --net=host -d --privileged=true --restart=always \\ --name flannel \\ -v /run:/run \\ -v /etc/kubernetes:/etc/kubernetes \\ quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-amd64 /opt/bin/flanneld --iface=eth1 \\ -etcd-endpoints=http://192.168.10.6:2379,http://192.168.10.7:2379,http://192.168.10.8:2379 -etcd-prefix=/coreos.com/network#查看网络段：etcdctl ls /coreos.com/network/subnets 宿主机执行：12source /run/flannel/subnet.envifconfig docker0 $&#123;FLANNEL_SUBNET&#125; #修改docker启动文件：vim /usr/lib/systemd/system/docker.serviceEnvironmentFile=/run/flannel/subnet.envExecStart=/usr/bin/dockerd –bip=$FLANNEL_SUBNET –ip-masq=$FLANNEL_IPMASQ –mtu=$FLANNEL_MTU 二进制文件安装安装：12wget https://github.com/coreos/flannel/releases/download/v0.6.1/flannel-v0.6.1-linux-amd64.tar.gztar -zxvf flannel-v0.6.1-linux-amd64.tar.gz 解压后的文件有：flanneld、mk-docker-opts.sh，其中flanneld为执行文件，sh脚本用于生成Docker启动参数。 在etcd中设置flannel所使用的ip段:1etcdctl --endpoints \"http://192.168.10.6:2379,http://192.168.10.7:2379,http://192.168.10.8:2379\" set /coreos.com/network/config '&#123;\"NetWork\":\"10.244.0.0/16\"&#125;' 启动：1flanneld --iface=eth1 -etcd-endpoints=http://192.168.10.6:2379,http://192.168.10.7:2379,http://192.168.10.8:2379 -etcd-prefix=/coreos.com/network 手动生成docker变量：123[root@k8s-master ~]# mk-docker-opts.sh -d /run/flannel/docker_opts.env -c[root@k8s-master ~]# cat /run/flannel/docker_opts.envDOCKER_OPTS=\" --bip=10.244.38.1/24 --ip-masq=true --mtu=1472\" 修改docker启动文件：123$ vim /usr/lib/systemd/system/docker.serviceEnvironmentFile=/run/docker_opts.envExecStart=/usr/bin/dockerd $DOCKER_OPTS 重启docker服务:123$ systemctl daemon-reload$ systemctl enable docker$ systemctl restart docker 测试在3台机器上运行：1docker run -it --rm --name centos centos bash 进入bash后，ip addr查看各自ip，互相ping一下对方的ip，如果可以ping通，表示安装正常，否则请检查相关的安装步骤。 参考 https://mritd.me/2016/09/03/Dokcer-%E4%BD%BF%E7%94%A8-Flannel-%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E8%AE%AF/http://qkxue.net/info/108138/docker-kubernetes-flannelhttps://segmentfault.com/a/1190000007585313http://blog.dataman-inc.com/shurenyun-docker-133/http://cmgs.me/life/docker-network-cloudhttp://dockone.io/article/1115http://blog.liuker.cn/index.php/docker/30.html","categories":[{"name":"docker","slug":"docker","permalink":"http://blog.gcalls.cn/categories/docker/"},{"name":"kubernetes","slug":"docker/kubernetes","permalink":"http://blog.gcalls.cn/categories/docker/kubernetes/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.gcalls.cn/tags/docker/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/tags/kubernetes/"}]},{"title":"Kubeadm rpm安装包制作","slug":"Kubeadm rpm安装包制作","date":"2017-01-04T02:22:47.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/Kubeadm rpm安装包制作.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/Kubeadm rpm安装包制作.html","excerpt":"本文记录一下Kubeadm rpm安装包的制作过程。","text":"本文记录一下Kubeadm rpm安装包的制作过程。 生成rpm安装包123git clone https://github.com/kubernetes/release.gitcd release/rpm/sh docker-build.sh 如出现rpm.po的错误，可以不用理会。生成的包在output/x86_64目录下，可以直接安装rpm包，安装包有： 1234kubeadm-1.6.0-0.alpha.0.2074.a092d8e0f95f52.x86_64.rpmkubectl-1.5.1-0.x86_64.rpmkubelet-1.5.1-0.x86_64.rpmkubernetes-cni-0.3.0.1-0.07a8a2.x86_64.rpm 可以直接安装rpm包，也可以通过yum源方式安装，具体参考下一节。直接安装rpm包：12cd output/x86_64yum localinstall *.rpm 添加yum源对rpm签名，请参考: rpm签名12345678tee /etc/yum.repos.d/k8s.repo &lt;&lt;-'EOF'[k8s-repo]name=kubernetes Repositorybaseurl=file:///docker/works/yumenabled=1gpgcheck=1gpgkey=file:///docker/works/yum/gpgEOF 安装：1yum install -y kubelet kubectl kubernetes-cni kubeadm","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/tags/kubernetes/"}]},{"title":"Linux网络","slug":"Linux网络","date":"2017-01-04T02:03:35.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/Linux网络.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/Linux网络.html","excerpt":"Linux网络可以通过命名空间进行网络隔离，本文记录一下相应的创建命令。","text":"Linux网络可以通过命名空间进行网络隔离，本文记录一下相应的创建命令。 网络命名空间创建12345ip netns add netns0ip netns add netns1#不加ip netns exec netns0 会在默认的root空间创建ip netns exec netns0 ip link add veth0 type veth peer name veth1 查看ip1234567[root@k8s1 ~]$ ip netns exec netns0 ip link show1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: veth1@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000 link/ether a6:b1:72:af:2d:a1 brd ff:ff:ff:ff:ff:ff3: veth0@veth1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000 link/ether 42:90:6e:1e:e6:b9 brd ff:ff:ff:ff:ff:ff 将veth1给netns1空间123456789101112[root@k8s1 ~]$ ip netns exec netns0 ip link set veth1 netns netns1[root@k8s1 ~]$ ip netns exec netns0 ip link show 1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:003: veth0@if2: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000 link/ether 42:90:6e:1e:e6:b9 brd ff:ff:ff:ff:ff:ff link-netnsid 0[root@k8s1 ~]$ ip netns exec netns1 ip link show1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: veth1@if3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000 link/ether a6:b1:72:af:2d:a1 brd ff:ff:ff:ff:ff:ff link-netnsid 0 添加ip12345ip netns exec netns0 ip addr add 10.1.1.2/24 dev veth0ip netns exec netns0 ip link set dev veth0 upip netns exec netns1 ip addr add 10.1.1.1/24 dev veth1ip netns exec netns1 ip link set dev veth1 up ping123456789101112131415161718192021222324[root@k8s1 ~]$ ip netns exec netns0 ifconfigveth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.1.1.2 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::4090:6eff:fe1e:e6b9 prefixlen 64 scopeid 0x20&lt;link&gt; ether 42:90:6e:1e:e6:b9 txqueuelen 1000 (Ethernet) RX packets 13 bytes 1026 (1.0 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 13 bytes 1026 (1.0 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@k8s1 ~]$ ip netns exec netns1 ifconfigveth1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.1.1.1 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::a4b1:72ff:feaf:2da1 prefixlen 64 scopeid 0x20&lt;link&gt; ether a6:b1:72:af:2d:a1 txqueuelen 1000 (Ethernet) RX packets 13 bytes 1026 (1.0 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 13 bytes 1026 (1.0 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@k8s1 ~]$ ip netns exec netns1 ping 10.1.1.2PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data.64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.035 ms64 bytes from 10.1.1.2: icmp_seq=2 ttl=64 time=0.055 ms 查看配对12345678[root@k8s1 ~]$ ip netns exec netns1 ethtool -S veth1NIC statistics: peer_ifindex: 3[root@k8s1 ~]$ ip netns exec netns0 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:003: veth0@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether 42:90:6e:1e:e6:b9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 网桥 参考: http://fp-moon.iteye.com/blog/1468650123456789101112131415yum -y install bridge-utilsbrctl addbr br0brctl addif br0 enp0s8 &amp;&amp; ifconfig enp0s8 0.0.0.0 &amp;&amp; ifconfig br0 192.168.10.6注意：brctl addif br0 enp0s8执行后断网，所以后面的命令需要一起执行。[root@k8s1 ~]$ ifconfig#192.168.10.6已经转移到br0中了br0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.10.6 netmask 255.255.255.0 broadcast 192.168.10.255 inet6 fe80::a00:27ff:fec4:969a prefixlen 64 scopeid 0x20&lt;link&gt; ether 08:00:27:c4:96:9a txqueuelen 0 (Ethernet) RX packets 28 bytes 2821 (2.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 23 bytes 2826 (2.7 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"kubernetes常用命令","slug":"kubernetes常用命令","date":"2017-01-04T01:56:26.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/kubernetes常用命令.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/kubernetes常用命令.html","excerpt":"本文记录一下kubernetes的常用命令。","text":"本文记录一下kubernetes的常用命令。 kubectl常用使用查看类命令查看集群信息1kubectl cluster-info 查看各组件信息1kubectl -s http://localhost:8080 get componentstatuses 查看pods所在的运行节点123kubectl get pods(po) -o widekubectl get pod -o wide -n kube-systemkubectl get pods -o wide --all-namespaces 查看pods定义的详细信息1kubectl get pods -o yaml 查看Replication Controller信息1kubectl get rc 查看service的信息1kubectl get service(svc) 查看节点信息1kubectl get nodes(no) 按selector名来查找pod1kubectl get pod --selector name=redis 查看运行的pod的环境变量1kubectl exec pod名 env 查看运行的pod的日志1kubectl logs -f --tail 100 pod名 查看pod的endpoint1[root@k8s-master ~]$ kubectl get endpoints(ep) 查看namespaces1234kubectl get namespacesNAME STATUS AGEdefault Active 5hkube-system Active 5h 操作类命令创建1kubectl create -f 文件名 重建1kubectl replace -f 文件名 [--force] 删除12345kubectl delete -f 文件名kubectl delete pod pod名kubectl delete rc rc名kubectl delete service service名kubectl delete pod --all 删除所有pods比如需要删除所有的curl实例：参考https://www.58jb.com/html/155.html123kubectl get pods --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODEdefault curl-57077659-swdxm 1/1 Running 0 9m 10.244.3.3 k8s-node2 先查看对应的rs：123kubectl get rsNAME DESIRED CURRENT READY AGEcurl-57077659 1 1 1 50m 再查看对应的deployment：123kubectl get deploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEcurl 1 1 1 1 52m 需要把deployment删除才行：1kubectl delete deployment curl 动态调整rc replicas123456[root@k8s-master ~]$ kubectl scale rc redis-slave --replicas=3[root@k8s-master ~]$ kubectl get rcNAME DESIRED CURRENT AGEfrontend 3 3 2hredis-master 1 1 2hredis-slave 3 3 2h node unschedule1234567891011[root@k8s-master x86_64]# vim unschedule_node.yamlapiVersion: v1kind: Nodemetadata: name: k8s-node1 labels: kubernetes.io/hostname: k8s-node1spec: unschedulable: true[root@k8s-master x86_64]# kubectl replace -f unschedule_node.yaml 或者：unschedule:1234567[root@k8s-master x86_64]# kubectl patch node k8s-node1 -p '&#123;\"spec\": &#123;\"unschedulable\": true&#125;&#125;'[root@k8s-master x86_64]# kubectl get noNAME STATUS AGE127.0.0.1 NotReady 8dk8s-node1 Ready,SchedulingDisabled 8dk8s-node2 Ready 8d schedule:1[root@k8s-master x86_64]# kubectl patch node k8s-node1 -p '&#123;\"spec\": &#123;\"unschedulable\": false&#125;&#125;' 动态调用deployment1kubectl scale deployment elasticsearch --replicas=1 -n kube-system 创建namespaces12kubectl create -f namespace-dev.yamlkubectl get pods --namespace=development","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/tags/kubernetes/"}]},{"title":"Centos yum源搭建","slug":"Centos-yum源搭建","date":"2017-01-03T03:45:45.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/Centos-yum源搭建.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/Centos-yum源搭建.html","excerpt":"Centos安装软件一些有两种，一种通过二进制编译安装或者rpm安装，一种通过yum安装。个人觉得最好还是通过yum安装，简单、好升级。但有些yum源不稳定，造成安装非常慢，本文介绍怎样将yum源下载并搭建到自己的yum源中。","text":"Centos安装软件一些有两种，一种通过二进制编译安装或者rpm安装，一种通过yum安装。个人觉得最好还是通过yum安装，简单、好升级。但有些yum源不稳定，造成安装非常慢，本文介绍怎样将yum源下载并搭建到自己的yum源中。 下载rpm及依赖包需要先安装createrepo与yum-utils1yum -y install createrepo yum-utils 下载rpm及依赖包1yum --downloadonly --downloaddir=x86_64 install nginx 或者：1yumdownloader --destdir=x86_64 --resolve nginx 下载某个repo的所有内容1reposync -r [repo name] 创建repo1createrepo . rpm签名需要先安装1yum install rpm-sign rng-tools 先查看gpg列表：1gpg --list-keys 没有的话，要创建：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556$ gpg --gen-keygpg (GnuPG) 2.0.14; Copyright (C) 2009 Free Software Foundation, Inc.This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Please select what kind of key you want: (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (sign only) (4) RSA (sign only)Your selection? #使用RSA&amp;RSA方式RSA keys may be between 1024 and 4096 bits long.What keysize do you want? (2048) #密钥长度2048Requested keysize is 2048 bitsPlease specify how long the key should be valid. 0 = key does not expire &lt;n&gt; = key expires in n days &lt;n&gt;w = key expires in n weeks &lt;n&gt;m = key expires in n months &lt;n&gt;y = key expires in n yearsKey is valid for? (0) #默认永不过期Key does not expire at allIs this correct? (y/N) y GnuPG needs to construct a user ID to identify your key. #输入密钥所有者的联系方式Real name: zhaoxunyongEmail address: zhaoxunyong@qq.comComment: You selected this USER-ID: \"zhaoxunyong &lt;zhaoxunyong@qq.com&gt;\"Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? OYou need a Passphrase to protect your secret key. can't connect to `/root/.gnupg/S.gpg-agent': No such file or directorygpg-agent[9337]: directory `/root/.gnupg/private-keys-v1.d' created Enter passphrase时，直接设为空。We need to generate a lot of random bytes. It is a good idea to performsome other action (type on the keyboard, move the mouse, utilize thedisks) during the prime generation; this gives the random numbergenerator a better chance to gain enough entropy.gpg: key C3FE29C9 marked as ultimately trustedpublic and secret key created and signed.gpg: checking the trustdbgpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust modelgpg: depth: 0 valid: 1 signed: 0 trust: 0-, 0q, 0n, 0m, 0f, 1upub 2048R/C3FE29C9 2017-01-03 Key fingerprint = C02E 0B2A 8402 97DA ACC6 239F DF03 083E C3FE 29C9uid zhaoxunyong &lt;zhaoxunyong@qq.com&gt;Note that this key cannot be used for encryption. You may want to usethe command \"--edit-key\" to generate a subkey for this purpose. 当现在:We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy.可使用：使用以下命令加速urandom伪设备产生随机数的速度：1rngd -r /dev/urandom 查询gpg列表：12345# gpg --list-keys/root/.gnupg/pubring.gpg------------------------pub 2048R/C3FE29C9 2017-01-03uid zhaoxunyong &lt;zhaoxunyong@qq.com&gt; 修改rpm宏,使用我们的密钥对:1echo '%_gpg_name C3FE29C9' &gt;&gt; ~/.rpmmacros 导出公钥:1gpg -o /docker/works/yum/gpg -a --export C3FE29C9 对rpm签名：123#rpm --resign etcd-3.0.15-1.x86_64.rpmcd /docker/works/yum/find ./ -name \"*.rpm\" -type f -exec rpm --resign '&#123;&#125;' \\; 当rpm太多时，每次输入密码很麻烦，可以用expect自动输入先安装expect：1yum install expect 自动输入密码并签名：1234567891011121314151617181920212223#!/bin/bashecho \"Start...\"yum -y install expectYUM_PATH=/docker/works/yumEXPECT_FILE=/tmp/expectfile_$(date +%y%m%d)cd $YUM_PATHrpms=`find ./ -name \"*.rpm\" -type f`for rpm in $rpms doecho \"rpm --resign $YUM_PATH/$rpm\"cat &lt;&lt; EOF &gt; $EXPECT_FILE#!/usr/bin/expectset PASS Aa123456spawn rpm --resign $YUM_PATH/$rpmexpect \"Enter pass phrase:\" &#123; send \"\\$PASS\\r\" &#125;set timeout -1expect eofEOFexpect -f $EXPECT_FILErm -fr $EXPECT_FILEdoneecho \"finished..\" 当出现[Errno 256] No more mirrors to try的解决办法：123yum clean metadatayum clean allyum -y update 注意：执行yum -y update后会升级内核会导致vagrant加载不了外面的目录，具体解决办法请参考：Vagrant环境搭建#异常解决 添加yum源12345678tee /etc/yum.repos.d/myreop.repo &lt;&lt;-'EOF'[my-repo]name=kubernetes Repositorybaseurl=file:///docker/works/yumenabled=1gpgcheck=1gpgkey=file:///docker/works/yum/gpgEOF 测试123456789yum list | grep my-repodocker-engine-debuginfo.x86_64 1.12.5-1.el7.centos my-repo etcd.x86_64 3.0.15-1 my-repo flannel.x86_64 0.6.2-1 my-repo my-repo kubectl.x86_64 1.5.1-0 my-repo kubelet.x86_64 1.5.1-0 my-repo kubernetes-cni.x86_64 0.3.0.1-0.07a8a2 my-repo rkt.x86_64 1.21.0-1 my-repo 显示某个软件的所有版本 1yum --showduplicates list docker-engine 参考 http://debugo.com/gpg/https://mritd.me/2016/08/02/yum-%E7%AC%94%E8%AE%B0/http://blog.sina.cn/dpool/blog/s/blog_6a5aee670101rx0a.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"Docker学习总结","slug":"Docker学习总结","date":"2017-01-03T03:28:07.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/Docker学习总结.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/Docker学习总结.html","excerpt":"docker是个好东西，虽然年轻，但很有前途。一些大的公司(包括谷歌、IBM、惠普、微软等)都有在使用。有些人不太愿意使用新的技术，怕不稳定，但我个人认为好的东西主要勇于使用，大公司都在使用，怕什么呢，新技术不敢于使用，怎么能进步呢^_^docker相关的概念请大家自行谷歌，还是谷歌好…","text":"docker是个好东西，虽然年轻，但很有前途。一些大的公司(包括谷歌、IBM、惠普、微软等)都有在使用。有些人不太愿意使用新的技术，怕不稳定，但我个人认为好的东西主要勇于使用，大公司都在使用，怕什么呢，新技术不敢于使用，怎么能进步呢^_^docker相关的概念请大家自行谷歌，还是谷歌好… 如果是没有docker基础的话，建议买一本&lt;&lt;第一本docker书 修订版&gt;&gt;，入门不错。docker正式环境只能在linux中使用，所以本文以vagrant+centos7为例介绍。具体环境请参考Vagrant环境搭建 操作系统要求Docker只能运行在64位Linux中，并且内核需要3.8以上，建议使用centos 7版本。以下是我本机的环境，请参考：12345678[root@www ~] cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@www ~] uname -aLinux www.mymydocker.com 3.10.0-327.4.5.el7.x86_64 #1 SMP Mon Jan 25 22:07:14 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux[root@www ~] ll /sys/class/misc/device-mapperlrwxrwxrwx 1 root root 0 Dec 13 08:19 /sys/class/misc/device-mapper -&gt; ../../devices/virtual/misc/device-mapper[root@www ~] grep device-mapper /proc/devices 253 device-mapper 安装docker卸载旧的docker版本当前docker最新版本为1.12，请先卸载旧的docker版本：123rpm -e docker-1.10.3-59.el7.centos.x86_64 \\ docker-common-1.10.3-59.el7.centos.x86_64 \\ container-selinux-1.10.3-59.el7.centos.x86_64 修改配置：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#修改dns配置cat /etc/NetworkManager/NetworkManager.conf|grep \"dns=none\" &gt; /dev/nullif [[ $? != 0 ]]; then echo \"dns=none\" &gt;&gt; /etc/NetworkManager/NetworkManager.conf systemctl restart NetworkManager.servicefi#修改时区ln -sf /usr/share/zoneinfo/Asia/Chongqing /etc/localtime#关闭内核安全sed -i 's;SELINUX=.*;SELINUX=disabled;' /etc/selinux/configsetenforce 0getenforce#关闭防火墙systemctl disable iptablessystemctl stop iptablessystemctl disable firewalldsystemctl stop firewalld#优化内核cat /etc/security/limits.conf|grep 65535 &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF* soft nofile 65535* hard nofile 65535* soft nproc 65535* hard nproc 65535EOFfi#打开端口转发#永久修改：/etc/sysctl.conf中的net.ipv4.ip_forward=1，生效：sysctl -p#临时修改：echo 1 &gt; /proc/sys/net/ipv4/ip_forward，重启后失效cat /etc/sysctl.conf|grep \"net.ipv4.ip_forward\" &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 300net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.ip_local_port_range = 1024 65535net.ipv4.ip_forward = 1EOFsysctl -pfi 添加docker用户组注意：rpm安装已经自动创建了该组，无需再创建。12groupadd -g 2016 docker#useradd docker -u 2016 -g 2016 当发现有docker组时，会自动以docker组启动。 添加yum源：12345678tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[docker]name=Docker Repositorybaseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=http://mirrors.aliyun.com/docker-engine/yum/gpgEOF 安装1yum install docker-engine 手动启动rpm安装的通过systemctl start docker启动。如果是二进制文件的话，通过以下方式启动：12345#默认以/var/run/docker.sock文件监听#手动启动：#-D为debugdocker daemon -D -H tcp://0.0.0.0:2375 \\ -H unix://var/run/docker.sock 如出现Devices cgroup isn’t mounted的错误，请执行以下操作：12wget https://github.com/tianon/cgroupfs-mount/raw/master/cgroupfs-mountsh cgroupfs-mount docker加速官网的速度太慢了，可以使用daocloud加速。请参考what-is-daocloud-accelerator具体操作如下： 注册https://www.daocloud.io/账户点击加速器，获取地址修改/usr/lib/systemd/system/docker.service文件：1234#ExecStart=/usr/bin/dockerd --bip=10.1.10.1/24 --insecure-registry=192.168.10.6:5000 --registry-mirror=http://3fecfd09.m.daocloud.iosed -i \"s;^ExecStart=/usr/bin/dockerd$;ExecStart=/usr/bin/dockerd \\--registry-mirror=http://3fecfd09.m.daocloud.io;\" \\ /usr/lib/systemd/system/docker.service 添加代理国内网站无法访问google的一些资源，可以通过docker代理方式访问：12345678910111213mkdir -p /etc/systemd/system/docker.service.dcat &gt;&gt; /etc/systemd/system/docker.service.d/http-proxy.conf &lt;&lt; EOF[Service]Environment=\"HTTP_PROXY=http://xxxx:xxxx\"Environment=\"HTTPS_PROXY=http://xxxx:xxxx\"Environment=\"NO_PROXY=127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net\"EOFsystemctl daemon-reloadsystemctl show --property=Environment dockersystemctl restart dockersystemctl enable docker 如执行docker info后出现：WARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabled按照以下办法解决：修改/etc/sysctl.conf文件，添加：123net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-arptables = 1 执行sysctl -p后生效。 然后 ps aux | grep docker 然后你就会发现带有镜像的启动参数了。 由于我这边的HTTP_PROXY与HTTPS_PROXY是付费购买的，同时只能几个client访问，需要的话请自行搜索或者购买。 Windows安装win10可以通过InstallDocker.msi直接安装，但win10以下的话，只能通过DockerToolbox安装： 12345docker-machine create -d virtualbox \\--engine-registry-mirror http://3fecfd09.m.daocloud.io \\--engine-registry-mirror https://3gbbfq7n.mirror.aliyuncs.com \\--engine-registry-mirror http://zhaoxunyong.m.alauda.cn \\default Docker Registry参考 http://www.cnblogs.com/lienhua34/p/4922130.htmlhttps://docs.docker.com/registry/deploying/ 安装registry:2普通安装方式安装：12docker create -p 5000:5000 --restart=always --name private_registry \\ -v /docker/registry:/var/lib/registry registry:2 映射主机的/docker/registry目录到容器的/var/lib/registry 异常解决：当出现Get https://192.168.10.6:5000/v1/_ping: Connection failed错误时，由于改为http方式，需要修改以下配置：123456vim /usr/lib/systemd/system/docker.service#添加--insecure-registry参数ExecStart=/usr/bin/dockerd --insecure-registry=192.168.10.6:5000#重启systemctl daemon-reloadsystemctl restart docker 证书安装方式 参考: https://www.tianmaying.com/tutorial/docker-registry先/etc/pki/tls/openssl.cnf配置，在该文件中找到[ v3_ca ]，在它下面添加如下内容：123[ v3_ca ]# Extensions for a typical CAsubjectAltName = IP:192.168.10.6 以上也可以不配置，在openssl加上reqexts SAN参数设置。 安装：1234567891011121314151617181920212223openssl req \\ -subj \"/C=CN/ST=GuangDong/L=ShenZhen/CN=registry.gcalls.cn\" \\ -reqexts SAN -config &lt;(cat /etc/pki/tls/openssl.cnf &lt;(printf \"[SAN]\\nsubjectAltName=DNS:www.abc.com,IP:192.168.10.6\")) \\ -newkey rsa:4096 -nodes -sha256 -keyout domain.key \\ -x509 -days 365 -out domain.crt mkdir -p /etc/docker/certs.d/192.168.10.6:5000cp domain.crt /etc/docker/certs.d/192.168.10.6:5000/ca.crtmkdir -p /root/certs/cp domain.crt domain.key /root/certs/docker run \\ -d \\ --name private_registry --restart=always \\ -e SETTINGS_FLAVOUR=dev \\ -e STORAGE_PATH=/registry-storage \\ -v /docker/registry:/var/lib/registry \\ -u root \\ -p 5000:5000 \\ -v /root/certs:/certs \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ registry:2 注意：如果采用证书方式，需要去掉/usr/lib/systemd/system/docker.service中的–insecure-registry参数，然后再重启：12systemctl daemon-reloadsystemctl restart docker 如果要用docker pull或者docker push的客户端，都需要执行以下命令：12mkdir -p /etc/docker/certs.d/192.168.10.6:5000cp domain.crt /etc/docker/certs.d/192.168.10.6:5000/ca.crt 否则，会报以下错误：1Error response from daemon: Get https://192.168.10.6:5000/v1/_ping: x509: certificate signed by unknown authority 测试123docker pull hello-worlddocker tag hello-world 192.168.10.6:5000/hello-worlddocker push 192.168.10.6:5000/hello-world 如测试出现：Get https://192.168.10.6:5000/v1/_ping: net/http: TLS handshake timeout有可以本地与docker开启了代理，需要关闭docker代理或者将ip添加到NO_PROXY中，文件位于：/etc/systemd/system/docker.service.d/http-proxy.conf Get https://192.168.10.6:5000/v1/_ping: x509: cannot validate certificate for 192.168.10.6 because it doesn’t contain any IP SANs这个是由于CN为registry.gcalls.cn，但通过ip，需要添加SAN信息：先/etc/pki/tls/openssl.cnf配置，在该文件中找到[ v3_ca ]，在它下面添加如下内容：123[ v3_ca ]# Extensions for a typical CAsubjectAltName = IP:192.168.10.6 也可以直接在创建crt时，传-reqexts SAN。 删除镜像文件删除private registry中的镜像：123docker exec -it private_registry /bin/sh#删除/var/lib/registry/docker/registry/v2/repositories目录下对应的目录rm /var/lib/registry/docker/registry/v2/repositories/* 启动1docker start private_registry 查看ip1docker exec private_registry ip addr Docker mirror Registry 参考：https://mritd.me/2016/09/24/Docker-mirror-Registry/ 导出registry配置1docker run -it --rm --entrypoint cat registry:2 /etc/docker/registry/config.yml &gt; config.yml 修改配置如果想要使用mirror功能只需在下面增加proxy选项即可：12345678910111213141516171819202122version: 0.1log: fields: service: registrystorage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registryhttp: addr: :5000 headers: X-Content-Type-Options: [nosniff]health: storagedriver: enabled: true interval: 10s threshold: 3proxy: remoteurl: https://registry-1.docker.io username: [username] password: [password] username与password是可选项，当填写username与password以后就可以从hub pull私有镜像了。 启动mirror registry1234docker run -dt --name docker-registry-mirror \\-v /docker/registry:/var/lib/registry \\-v $PWD/config.yml:/etc/docker/registry/config.yml \\-p 5000:5000 registry:2 导入本地的images到私服中push.sh:12345678#!/bin/shimgs=$(docker images|awk '&#123;print $1\":\"$2&#125;')for img in $imgsdo docker tag $img 192.168.10.6:5000/$img docker push 192.168.10.6:5000/$img docker rmi 192.168.10.6:5000/$imgdone 安装shipyardshipyard可以通过web的方式操作本地的镜像，还可以浏览指定的private registry，具体安装方式如下：1234#dockerui:#docker run -d -p 9000:9000 --name dockerui -v /var/run/docker.sock:/var/run/docker.sock uifd/ui-for-docker#docker start dockeruiwget -O shipyard.sh https://shipyard-project.com/deploy 安装：12#curl -s https://shipyard-project.com/deploy | bash -sexport ACTION=deploy;export PORT=9001;sh shipyard.sh 更新：12#curl -s https://shipyard-project.com/deploy | ACTION=upgrade bash -sexport ACTION=upgrade;export PORT=9001;sh shipyard.sh 移除：12#curl -s https://shipyard-project.com/deploy | ACTION=remove bash -sexport ACTION=remove;sh shipyard.sh 更多查看：1sh shipyard.sh -h 访问地址：1http://192.168.10.6:9001/ 注意： 防火墙要放行2375端口，否则看不到container添加registry时，需要指定https的地址，如：https://192.168.10.6:5000login: admin/shipyard 安装registry uiregistry ui有很多，包括： docker-registry-frontend可以使用docker-registry-frontend，具体安装方式如下： 普通安装方式12345678910#docker run -d -p 8080:8080 atcol/docker-registry-ui(不支持V2，不能使用)sudo docker run \\ -d \\ -e ENV_DOCKER_REGISTRY_HOST=192.168.10.6 \\ -e ENV_DOCKER_REGISTRY_PORT=5000 \\ -p 9002:80 \\ konradkleine/docker-registry-frontend:v2#重启：docker stop $(docker ps -a |grep docker-registry-frontend|awk '&#123;print $1&#125;')docker start $(docker ps -a |grep docker-registry-frontend|awk '&#123;print $1&#125;') 访问：1http://192.168.10.6:9002 证书安装方式12345678910sudo docker run \\ -d \\ -e ENV_DOCKER_REGISTRY_HOST=192.168.10.6 \\ -e ENV_DOCKER_REGISTRY_PORT=5000 \\ -e ENV_DOCKER_REGISTRY_USE_SSL=1 \\ -e ENV_USE_SSL=yes \\ -v /root/certs/domain.crt:/etc/apache2/server.crt:ro \\ -v /root/certs/domain.key:/etc/apache2/server.key:ro \\ -p 9002:443 \\ konradkleine/docker-registry-frontend:v2 异常解决：如果出现The proxy server could not handle the request GET /v2/_catalog的错误，需要添加：1-e ENV_DOCKER_REGISTRY_USE_SSL=1 访问：https://192.168.10.6:9002 Nexus 参考：https://mritd.me/2017/01/08/set-up-docker-registry-by-nexus/安装稍后补充。 harbor 参考：https://vmware.github.io/harbor/index_cn.htmlhttps://mritd.me/2016/09/15/Harbor-%E4%BC%81%E4%B8%9A%E7%BA%A7-Docker-Registry-%E7%AC%AC%E4%BA%8C%E5%BC%B9/注意：harbor不需要事先安装docker registry。 安装123wget https://github.com/vmware/harbor/releases/download/0.5.0/harbor-offline-installer-0.5.0.tgztar zxvf harbor-offline-installer-0.5.0.tgzcd harbor 修改配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@zhaoxy harbor]# grep -v ^# harbor.cfg #修改hostnamehostname = registry.gcalls.cnui_url_protocol = https#修改emailemail_identity = email_server = smtp.exmail.qq.comemail_server_port = 25email_username = xxx@xxx.comemail_password = xxxemail_from = xxx@xxx.comemail_ssl = falseharbor_admin_password = Harbor12345auth_mode = db_authldap_url = ldaps://ldap.mydomain.comldap_basedn = ou=people,dc=mydomain,dc=comldap_uid = uid ldap_scope = 3 db_password = root123#开放用户注册self_registration = onuse_compressed_js = onmax_job_workers = 3 token_expiration = 30verify_remote_cert = oncustomize_crt = on#证书信息crt_country = CNcrt_state = guangdongcrt_location = shenzhencrt_organization = Gcalls.cncrt_organizationalunit = Gcalls.cncrt_commonname = registry.gcalls.cncrt_email = zhaoxunyong@qq.comproject_creation_restriction = everyone#nginx的证书路径ssl_cert = /docker/works/harbor/ca/registry.gcalls.cn.crtssl_cert_key = /docker/works/harbor/ca/registry.gcalls.cn.key 证书生成1234567891011121314151617181920212223#创建CAmkdir cacd caopenssl req \\ -subj \"/C=CN/ST=guangdong/L=shenzhen/CN=*.gcalls.cn\" \\ -newkey rsa:4096 -nodes -sha256 -keyout ca.key \\ -x509 -days 365 -out ca.crt#创建签名请求openssl req \\ -subj \"/C=CN/ST=guangdong/L=shenzhen/CN=registry.gcalls.cn\" \\ -newkey rsa:4096 -nodes -sha256 -keyout registry.gcalls.cn.key \\ -out registry.gcalls.cn.csr#初始化，不然下面的操作会报错cd /etc/pki/CA/touch index.txtecho '01' &gt; serialcd -#签署证书#openssl x509 -req -days 365 -in /csr/abc.req.csr -signkey /csr/abc.key.pem.encrypted -out /csr/abc.cert.crtopenssl ca -in registry.gcalls.cn.csr -out registry.gcalls.cn.crt -cert ca.crt -keyfile ca.key -outdir . 安装1234567#先安装docker-composeyum install epel-releaseyum install python2-pippip install -U docker-compose#安装harbor./install.sh 访问地址登录密码默认为：admin/Harbor123451https://registry.gcalls.cn push测试注意：需要push的客户端要先把ca.crt文件复制到/etc/docker/certs.d/registry.gcalls.cn/目录下123456789#initmkdir -p /etc/docker/certs.d/registry.gcalls.cncp ca/ca.crt /etc/docker/certs.d/registry.gcalls.cn/#logindocker login registry.gcalls.cndocker tag nginx:1.11.5 registry.gcalls.cn/harbor/nginx:1.11.5#需要先通过访问https://registry.gcalls.cn登录后创建harbor项目docker push registry.gcalls.cn/harbor/nginx:1.11.5 Harbor镜像仓库注意：镜像仓库不允许 push 操作，只作为官方仓库缓存。12345678vim common/templates/registry/config.yml# 增加以下内容proxy: remoteurl: https://registry-1.docker.io# 然后重新部署即可docker-compose downrm /data/database /data/job_logs /data/registry /data/secretkeydocker up -d 基本命令创建容器交互式容器：123456789docker run --name web -i -t docker.io/centos /bin/bashdocker run -it -e TZ=Asia/Shanghai ubuntu bash--name：容器命名--rm: 创建并运行一次后自动删除-d: 守护式容器，不加-d的话，会直接进入docker命令行，exit后容器也退出了。--restart=on-failure:5 当容器退出代码为非0时，自动尝试重启5次--restart=always 容器退出时，总会自动启动-v $PWD/website:/var/www/html/website:ro 挂载目录到容器，挂载website目录到容器的var/www/htmlwebsite，权限为ro, rw为可读写(默认) ro为只读--volumes-from containername 把containername所有的VOLUME挂载到新容器中。 显示正在运行的容器：1docker ps 显示所有的容器：1docker ps -a 显示所有的容器的id：1docker ps -a -q 显示最近一个运行的容器：1docker ps -l 删除：1docker rm &lt;CONTAINER ID&gt; 删除数据卷：数据卷是被设计用来持久化数据的，它的生命周期独立于容器，Docker不会在容器被删除后自动删除数据卷，并且也不存在垃圾回收这样的机制来处理没有任何容器引用的数据卷。如果需要在删除容器的同时移除数据卷。可以在删除容器的时候使用 docker rm -v这个命令。无主的数据卷可能会占据很多空间，要清理会很麻烦。1docker rm -vf &lt;CONTAINER ID&gt; -f：表示强制删除运行中的容器。 停止所有的docker容器：1docker stop $(docker ps -q -a) 启动与停止：12docker start &lt;NAME&gt;docker stop &lt;NAME&gt; 特权运行：12#true：Docker将拥有访问host所有设备的权限docker run --privileged=true ...... 进入已启动的容器:1docker attach &lt;NAME&gt; 注意：进入容器后，执行exit后，容器会关掉。要：CTRL+P+Q才不会退出容器docker rm后，再创建容器会恢复到原始内容。 查看已运行的容器日志：123456789docker run --name daemon_dave -d docker.io/centos /bin/sh -c \"while true;do echo hello world;sleep 1;done\"docker run --log-driver=\"syslog\" --name dave -d docker.io/centos /bin/sh -c \"while true;do echo hello world;sleep 1;done\"#显示最近100条的记录：docker logs --tail 100 web#捕捉最新的日志：docker logs -f web#将日志输出到/var/log/message中，通过docker logs会禁用--log-driver=\"syslog\" 容器内进程：1docker top|stats &lt;NAME1&gt; &lt;NAME2&gt; 在容器外部执行容器内的命令：123docker exec -d web touch /etc/new_config_filedocker exec -it web /bin/bash#类似于docker attach web，但容器不会自动退出 docker的目录：/var/lib/docker，包括了镜像、配置。 显示本机中的docker镜像：1docker images 虚悬镜像:镜像列表中，还可以看到一个特殊的镜像，这个镜像既没有仓库名，也没有标签，均为 ，这类无标签镜像也被称为 虚悬镜像(dangling image) ，可以用下面的命令专门删除这类镜像：123456#列出虚悬镜像：docker images -f dangling=true#删除虚悬镜像：docker rmi $(docker images -q -f dangling=true)#删除所有在 mongo:3.2 之前的镜像：docker rmi $(docker images -q -f before=mongo:3.2) 搜索：1docker search redis checkout：12docker logindocker pull docker.io/redis commit到hub.docker.com：commit本机：1docker commit -m \"A newcustom image\" -a \"zhaoxunyong\" redis zhaoxunyong/redis:1.0.0-SNAPSHOT commit到hub.docker.com:1docker push zhaoxunyong/redis:1.0.0-SNAPSHOT 指定端口：12docker run -d -p 80 --name nginx zhaoxunyong/mycentos nginx -g \"daemon off;\"docker run -d -p 8080:80 --name nginx zhaoxunyong/mycentos nginx -g \"daemon off;\" -p 8080:80: 表示把容器中的80端口映射到宿主机的8080端口run -d -P –name nginx zhaoxunyong/mycentos nginx -g “daemon off;”-P：表示将Dockerfile中的EXPOSE端口对外分布 查看映射的端口：1docker port nginx 80 导入或导出镜像导出1docker save -o centos.tar centos:latest 导入到images repo1docker load -i centos.tar 容器导出1234#找到容器iddocker ps#导出：docker export 19cb419ceb15 &gt; mycentos.tar 注意：导出的为运行中的容器内容，包括已经在容器中安装的软件都会一并导出。 导入到images repo1docker import mycentos.tar mycentos 注意：import命令是将tar导入到images中，而不是创建一个运行的容器。可以通过docker images查看。 数据卷容器如果你有一些持续更新的数据需要在容器之间共享，最好创建数据卷容器。数据卷容器，其实就是一个正常的容器，专门用来提供数据卷供其它容器挂载的。首先，创建一个名为 dbdata 的数据卷容器：1docker run -d -v /dbdata --name dbdata training/postgres echo Data-only container for postgres 然后，在其他容器中使用 –volumes-from 来挂载 dbdata 容器中的数据卷。12docker run -d --volumes-from dbdata --name db1 training/postgresdocker run -d --volumes-from dbdata --name db2 training/postgres 可以使用超过一个的 –volumes-from 参数来指定从多个容器挂载不同的数据卷。 也可以从其他已经挂载了数据卷的容器来级联挂载数据卷。1docker run -d --name db3 --volumes-from db1 training/postgres 注意：使用 –volumes-from 参数所挂载数据卷的容器自己并不需要保持在运行状态。如果删除了挂载的容器（包括 dbdata、db1 和 db2），数据卷并不会被自动删除。如果要删除一个数据卷，必须在删除最后一个还挂载着它的容器时使用 docker rm -v 命令来指定同时删除关联的容器。 这可以让用户在容器之间升级和移动数据卷。 Dockerfile基本指令RUNexec形式：RUN [“executable”, “param1”, “param2”]shell形式：RUN command param1 param2在Dockerfile构建镜像的过程(Build)中运行，最终被commit的到镜像。 CMDexec形式：CMD [“executable”, “param1”, “param2”]shell形式：CMD command param1 param2注意：CMD只能指定一条。如果docker run有传cmd的话，Dockerfile中的CMD无效 ENTRYPOINTexec形式：ENTRYPOINT [“executable”, “param1”, “param2”]shell形式：ENTRYPOINT command param1 param2不会被外面的参数覆盖，同时CMD或者外面传的参数会作为ENTRYPOINT的参数也可以在外面：docker run –entrypoint覆盖ENTRYPOINT指令ENTRYPOINT和CMD在容器运行(run、start)时运行。 exec与shell的区别：shell形式和exec的形式的本质区别在于shell形式提供了默认的指令/bin/sh -c，所以其指定的command将在shell的环境下运行。因此指定command的pid将不会是1，因为pid为1的是shell，command进程是shell的子进程。由于shell的pid不为1，因此我们无法直接向其发送信号，敲Ctrl+C是没有任何反应的。通过docker stop强制退出，退出状态为137，137=128 + 9，表明最后是被kill -9杀掉的。shell形式还有一个严重的问题：由于其默认使用/bin/sh来运行命令，如果镜像中不包含/bin/sh，容器会无法启动。exec形式则不然，其直接运行指定的指令，由于exec指定的命令不由shell启动，因此也就无法使用shell中的环境变量，如$HOME。如果希望能够使用环境变量，可以指定命令为sh：CMD [ “sh”, “-c”, “echo”, “$HOME” ]。注意：CMD与ENTRYPOINT要使用env变量的话，都要使用”sh”, “-c”，并且命令要在一个引号中，如：12CMD [\"sh\", \"-c\", \"java -Djava.security.egd=file:/dev/./urandom -jar /app/$&#123;APPNAME&#125;-$&#123;VERSION&#125;.jar\"]ENTRYPOINT [\"sh\", \"-c\", \"java -Djava.security.egd=file:/dev/./urandom -jar /app/$&#123;APPNAME&#125;-$&#123;VERSION&#125;.jar --spring.profiles.active=docker\"] WORKDIR脚本运行的工作目录,目录会自动创建。也可以在外面： -w覆盖WORKDIR指令 ENV设置环境变更：ENV WORK_HOME /zxyWORKDIR $WORK_HOME进入容器可以通过env查看也可以通过外面：-e “WORK_HOME=/zxy”指定 USERUSER nginx指定运行的用户，不指定默认root VOLUMEVOLUME [“/data”]创建一个可以从本地主机或其他容器挂载的挂载点 LABELLABEL location=”New York” type=”Data Center” role=”Web Server” COPY/ADD添加文件，ADD与COPY不同之处在于如果文件是压缩文件，ADD到容器中会自动解压。支持http方式。 ARG构建时，传递参数给构建：ARG buildARG webapp_user=user(user为默认值)传递参数：–build-arg build=1234如：docker build –build-arg build=1234 -t zhaoxunyong/mycentos ./ ONBUILD触发器：在另外有构建基于这个构建时触发，只能继承一次如：ONBUILD ADD test.sh /software/ 构建1docker build --no-cache -t=\"zhaoxunyong/mycentos:latest\" . –no-ache表示每次都会从头到尾构建，通过docker images就可以看到已经构建的镜像。还可以查看历史记录：docker history zhaoxunyong/mycentos 网络连接Networking版本&gt;=1.9推荐使用：123docker network create appdocker network inspect appdocker network ls 创建：12docker run -d --net=app --name db docker.io/redisdocker run -it --net=app --name centos docker.io/centos /bin/bash 测试：1234#ping db就可以ping通进入docker exec -it centos /bin/bash#ping centos也可以ping通进入docker exec -it db /bin/bash 加入到app网络：12docker run -d -p 8088:80 --name nginx docker.io/nginxdocker network connect app nginx 进入docker exec -it nginx /bin/bash，ping db或centos就能ping通 断开：1docker network disconnect app nginx Linklink:1.9之前版本使用：–link 原containername:别名只支持在相同的宿主机中。可以多次使用–link：12docker run -d --name db docker.io/redis docker run -d -p 8088:80 --name nginx --link db:redis_db docker.io/nginx 进入docker exec -it nginx /bin/bash，ping redis_db就能ping通, /etc/hosts可以看到对应的记录。可以在容器中通过env查看环境变量–icc=false:关闭所有没有链接的容器间的通信 Docker compose 参考：http://debugo.com/docker-compose/http://www.cnblogs.com/freefei/p/5311294.html 在开发环境、临时服务器、CI中使用Compose是非常合适的。但是，我们目前不建议你在生产环境中使用。 安装pip12yum install epel-releaseyum install python2-pip 安装compose12pip install -U docker-composedocker-compose version Demo编写Dockerfile:123456789101112131415vim redis/Dockerfile# Version: 1.0.0FROM centosMAINTAINER zhaoxunyong@qq.comRUN [\"yum\", \"-y\", \"install\", \"epel-release\"]RUN yum -y install redisVOLUME [ \"/var/lib/redis\", \"/var/log/redis\" ]#ENTRYPOINT [ \"redis-server\", \"--protected-mode\", \"no\", \"--logfile\", \"/var/log/redis/redis-server.log\" ]ENTRYPOINT [ \"redis-server\" ]CMD [\"--protected-mode\", \"no\", \"--logfile\", \"/var/log/redis/redis-server.log\"]EXPOSE 6379 编译：1docker build -t zhaoxunyong/redis ./ 123456789101112vim nginx/Dockerfile# Version: 1.0.0FROM docker.io/centosRUN [\"yum\", \"-y\", \"install\", \"epel-release\"]RUN yum -y install nginxEXPOSE 80#CMD [ \"nginx\", \"-g\", \"daemon off;\"]docker build -t zhaoxunyong/nginx ./ 编译：1docker build -t zhaoxunyong/nginx ./ compose:vim docker-compose.yml123456789101112131415161718192021222324252627282930313233343536373839404142web: #container_name: web #image: zhaoxunyong/nginx build: ./nginx command: nginx -g 'daemon off;' ports: - \"8081:80\" volumes: - $PWD:/pwd - /webapp:/webapp links: - redis:db #net: app #hostname: web #dns: # - 8.8.8.8 # - 9.9.9.9 #dns_search: # - domain1.example.com #mem_limit: 1000000000 #privileged: true #restart: always #volumes_from: # - service_name # - container_name #expose: # - \"3000\" # - \"8000\" #environment: # - DEBUG=false # - SEND_EMAILS=false #env_file: # - ./common.env # - ./apps/web.env # - /opt/secrets.envredis: #container_name: redis #image: zhaoxunyong/redis build: ./redis ports: - \"6379:6379\" 启动：12345678#docker run --name redis --rm -p 6379:6379 --rm zhaoxunyong/redis --protected-mode no --logfile /var/log/redis/redis-server.log#docker run --name nginx -p 8081:80 --rm --link redis:db zhaoxunyong/nginx nginx -g \"daemon off;\"docker-compose updocker-compose up -ddocker-compose psdocker-compose logsdocker-compose logs -fdocker-compose stop 使用-f指定代替的compose文件。使用-p指定代替compose文件所在的目录。 查看已存在images的Dockfile 参考：https://github.com/lukapeschke/dockerfile-from-image123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[root@k8s-node1 ~]# git clone https://github.com/lukapeschke/dockerfile-from-image.gitcd dockerfile-from-imagedocker build -t lukapeschke/dfa .[root@k8s-node1 ~]# docker run --rm -v '/var/run/docker.sock:/var/run/docker.sock' lukapeschke/dfa &lt;IMAGE_ID&gt;[root@k8s-node1 ~]# docker run --rm -v '/var/run/docker.sock:/var/run/docker.sock' lukapeschke/dfa 405a0b586f7eFROM kubeguide/redis-master:latestRUNADD file:62400a49cced0d7521560b501f6c52227c60f5e2fecd0fef20e4d0e1558f7301 in /RUN /bin/sh -c echo '#!/bin/sh' &gt; /usr/sbin/policy-rc.d \\ &amp;&amp; echo 'exit 101' &gt;&gt; /usr/sbin/policy-rc.d \\ &amp;&amp; chmod +x /usr/sbin/policy-rc.d \\ &amp;&amp; dpkg-divert --local --rename --add /sbin/initctl \\ &amp;&amp; cp -a /usr/sbin/policy-rc.d /sbin/initctl \\ &amp;&amp; sed -i 's/^exit.*/exit 0/' /sbin/initctl \\ &amp;&amp; echo 'force-unsafe-io' &gt; /etc/dpkg/dpkg.cfg.d/docker-apt-speedup \\ &amp;&amp; echo 'DPkg::Post-Invoke &#123; \"rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true\"; &#125;;' &gt; /etc/apt/apt.conf.d/docker-clean \\ &amp;&amp; echo 'APT::Update::Post-Invoke &#123; \"rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true\"; &#125;;' &gt;&gt; /etc/apt/apt.conf.d/docker-clean \\ &amp;&amp; echo 'Dir::Cache::pkgcache \"\"; Dir::Cache::srcpkgcache \"\";' &gt;&gt; /etc/apt/apt.conf.d/docker-clean \\ &amp;&amp; echo 'Acquire::Languages \"none\";' &gt; /etc/apt/apt.conf.d/docker-no-languages \\ &amp;&amp; echo 'Acquire::GzipIndexes \"true\"; Acquire::CompressionTypes::Order:: \"gz\";' &gt; /etc/apt/apt.conf.d/docker-gzip-indexesRUN /bin/sh -c sed -i 's/^#\\s*\\(deb.*universe\\)$/\\1/g' /etc/apt/sources.listCMD [/bin/bash]RUN /bin/sh -c sed -i 's/# \\(.*multiverse$\\)/\\1/g' /etc/apt/sources.list \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get -y upgrade \\ &amp;&amp; apt-get install -y build-essential \\ &amp;&amp; apt-get install -y software-properties-common \\ &amp;&amp; apt-get install -y byobu curl git htop man unzip vim wget \\ &amp;&amp; rm -rf /var/lib/apt/lists/*ADD file:1b7d66a2e6558a749e3bb8462c04292d941ca6456e4a0d27575774591c677768 in /root/.bashrcADD file:f21c0663779541419bb4a70154751df046dc27d6bfb93362b1d42ea1e4dceb37 in /root/.gitconfigADD dir:217b39958cc7efb346372d54c10f32da1230945a6041e9ca98ebb4fe18eb3c07 in /root/.scriptsENV HOME=/rootWORKDIR /rootCMD [bash]RUN /bin/sh -c cd /tmp \\ &amp;&amp; wget http://download.redis.io/redis-stable.tar.gz \\ &amp;&amp; tar xvzf redis-stable.tar.gz \\ &amp;&amp; cd redis-stable \\ &amp;&amp; make \\ &amp;&amp; make install \\ &amp;&amp; cp -f src/redis-sentinel /usr/local/bin \\ &amp;&amp; mkdir -p /etc/redis \\ &amp;&amp; cp -f *.conf /etc/redis \\ &amp;&amp; rm -rf /tmp/redis-stable* \\ &amp;&amp; sed -i 's/^\\(bind .*\\)$/# \\1/' /etc/redis/redis.conf \\ &amp;&amp; sed -i 's/^\\(daemonize .*\\)$/# \\1/' /etc/redis/redis.conf \\ &amp;&amp; sed -i 's/^\\(dir .*\\)$/# \\1\\ndir \\/data/' /etc/redis/redis.conf \\ &amp;&amp; sed -i 's/^\\(logfile .*\\)$/# \\1/' /etc/redis/redis.confVOLUME [/data]WORKDIR /dataADD file:eb94b1ca7cdb7ea1553bfd01686f66b9e5dd9893a4749ce291a8071e40113465 in /etc/redis/redis.confEXPOSE map[6379/tcp:&#123;&#125;]ADD file:58911657b64982dea712ca78ee8811c96e522d06585d29d40a79b2d4f6e935c6 in /etc/redis/redis.confCMD [redis-server /etc/redis/redis.conf]EXPOSE map[6379/tcp:&#123;&#125;]MAINTAINER kubeguideADD multi:0a8d9f4785c98c51ae6f30026e81df4b38fcbda87d73fe189656bdedfe016d32 in /data/ 参考 http://c.isme.pub/2016/11/21/learn-docker-install/","categories":[{"name":"docker","slug":"docker","permalink":"http://blog.gcalls.cn/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://blog.gcalls.cn/tags/docker/"}]},{"title":"使用packer制作vagrant box","slug":"使用packer制作vagrant-box","date":"2017-01-03T01:42:40.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/使用packer制作vagrant-box.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/使用packer制作vagrant-box.html","excerpt":"box可以从官网下载，但有时候下载很慢，并且对应的版本可能不是自己想要的。本文以centos7为例介绍一下怎样制作vagrant box。","text":"box可以从官网下载，但有时候下载很慢，并且对应的版本可能不是自己想要的。本文以centos7为例介绍一下怎样制作vagrant box。 环境准备创建目录12mkdir -p /Vagrant/packer/cd /Vagrant/packer/ 下载centos镜像123#wget http://mirrors.aliyun.com/centos/7.2.1511/isos/x86_64/CentOS-7-x86_64-DVD-1511.iso#wget http://mirrors.aliyun.com/centos/7.3.1611/isos/x86_64/CentOS-7-x86_64-DVD-1611.isowget http://mirrors.aliyun.com/centos/7.4.1708/isos/x86_64/CentOS-7-x86_64-DVD-1708.iso 注意：不要用Minimal版本，否则创建后使用时会出现/sbin/mount.vboxsf: mounting failed with the error的错误。 安装packer工具123456#MacOS#wget https://releases.hashicorp.com/packer/0.12.1/packer_0.12.1_darwin_amd64.zip#unzip packer_0.12.1_darwin_amd64.zip wget https://releases.hashicorp.com/packer/1.1.0/packer_1.1.0_darwin_amd64.zipunzip packer_1.1.0_darwin_amd64.zipsudo mv packer /usr/local/bin/ 下载centos.json12#git clone https://github.com/chef/bento.gitgit clone https://github.com/boxcutter/centos.git 开始制作修改centos7.json先进入centos目录，然后修改cento7.json文件：12345678910111213&#123; \"_comment\": \"Build with `packer build -var-file=centos7.json centos.json`\", \"vm_name\": \"centos7\", \"cpus\": \"1\", \"disk_size\": \"102400\", \"http_directory\": \"kickstart/centos7\", \"iso_checksum\": \"ec7500d4b006702af6af023b1f8f1b890b6c7ee54400bb98cef968b883cd6546\", \"iso_checksum_type\": \"sha256\", \"iso_name\": \"CentOS-7-x86_64-DVD-1708.iso\", \"iso_url\": \"/Vagrant/packer/CentOS-7-x86_64-DVD-1708.iso\", \"memory\": \"1024\", \"parallels_guest_os_type\": \"centos7\"&#125; 主要修改iso_checksum、iso_name、iso_url几个参数，其中iso_checksum值可以通过以下命令获取：12$ shasum -a 256 CentOS-7-x86_64-DVD-1708.isoec7500d4b006702af6af023b1f8f1b890b6c7ee54400bb98cef968b883cd6546 CentOS-7-x86_64-DVD-1708.iso 开始生成1234567891011cd centos#默认会生成所有虚拟机环境的文件，包括vmware/virtualbox/parallels，前提是安装了相应的虚拟机。packer build -var-file=centos7.json centos.json(或者bin/box build centos7，只能在unix环境下执行)#也可以指定生成的是哪个虚拟机：# packer build -only=virtualbox-iso -var-file=centos7.json centos.json# 或者bin/box build centos72 virtualbox# packer build -only=parallels-iso -var-file=centos7.json centos.json# 或者bin/box build centos7 parallels 如果使用parallels，vagrant需要安装plugin：1vagrant plugin install vagrant-parallels 使用parallels时，如出现ImportError: No module named prlsdkapi错误，需要安装ParallelsVirtualizationSDK：参考：https://forum.parallels.com/threads/error-while-building-with-packer.339491/123#brew cask install parallels-virtualization-sdk#wget http://download.parallels.com/desktop/v11/11.2.0-32581/ParallelsVirtualizationSDK-11.2.0-32581-mac.dmgwget http://download.parallels.com/desktop/v12/12.1.3-41532/ParallelsVirtualizationSDK-12.1.3-41532-mac.dmg 注意生成期间不要进入虚拟机进行任何操作。 如果想自定义安装一些软件，可以在script/update.sh中定义，比如：12345678910111213#!/bin/bash -euxif [[ $UPDATE =~ true || $UPDATE =~ 1 || $UPDATE =~ yes ]]; then echo \"==&gt; Applying updates\" yum -y update # 安装自定义软件 #yum -y install gcc gcc-c++ make wget autoconf kernel-devel yum -y install gcc kernel-devel # reboot echo \"Rebooting the machine...\" reboot sleep 60fi 如出现以下表示制作成功：12==&gt; Builds finished. The artifacts of successful builds are:--&gt; virtualbox-iso: 'virtualbox' provider box: box/virtualbox/centos7-0.0.99.box 生成的文件很小，只有439M：12$ ls -lh box/virtualbox/centos7-0.0.99.box-rw-r--r-- 1 zxy wheel 439M 1 3 10:19 box/virtualbox/centos7-0.0.99.box 参考 http://www.cnblogs.com/qinqiao/p/packer-vagrant-centos-box.htmlhttps://www.zzxworld.com/post/create-vagrant-box-base-on-centos.htmlhttp://www.tuicool.com/articles/F7ZjQvy","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"Vagrant环境搭建","slug":"Vagrant环境搭建","date":"2017-01-03T01:14:24.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/Vagrant环境搭建.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/Vagrant环境搭建.html","excerpt":"一般开发时，我们基本上都是在windows或mac上开发，测试与正式环境一般都使用centos系统。开发环境搭建起来非常费时，并且经常出现开发时功能是正常的，但测试环境和生产环境不正常的情况。并且有新同事来时，还得从头搭建相关的环境，费力又费时。我们可以通过vagrant很好地解决这个问题。","text":"一般开发时，我们基本上都是在windows或mac上开发，测试与正式环境一般都使用centos系统。开发环境搭建起来非常费时，并且经常出现开发时功能是正常的，但测试环境和生产环境不正常的情况。并且有新同事来时，还得从头搭建相关的环境，费力又费时。我们可以通过vagrant很好地解决这个问题。 介绍Vagrant是一个基于Ruby的工具，用于创建和部署虚拟化开发环境。它使用Oracle的开源VirtualBox虚拟化系统，使用 Chef创建自动化虚拟环境。 安装这边以mac为例介绍一下vagrant的安装与配置过程，安装过程很简单，直接下载以下两个软件安装即可。 vagrant1wget https://releases.hashicorp.com/vagrant/1.9.1/vagrant_1.9.1.dmg VirtualBox1http://download.virtualbox.org/virtualbox/5.1.10/VirtualBox-5.1.10-112026-OSX.dmg 配置以centos 7为例介绍一下vagrant的配置。 环境准备下载vagrant-centos-7.2.box1wget https://github.com/CommanderK5/packer-centos-template/releases/download/0.7.2/vagrant-centos-7.2.box 由于是国外的网站，下载很慢，其实我们可以自己创建box，具体制作方法请参考使用packer制作vagrant-box.md 初始化123456sudo mkdir -p /Vagrant/boxes/centos-7.2sudo cp -a vagrant-centos-7.2.box /Vagrant/boxes/centos-7.2cd /Vagrant/boxes/centos-7.2sudo vagrant box add centos-7.2 vagrant-centos-7.2.boxsudo vagrant box listsudo vagrant init centos-7.2 配置Vagrantfile假设我们需要两台虚拟机 主机IP 主机名称 内存 192.168.10.6 k8s-master 1024m 192.168.10.6 k8s-node1 512m 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# -*- mode: ruby -*-# vi: set ft=ruby :# All Vagrant configuration is done below. The \"2\" in Vagrant.configure# configures the configuration version (we support older styles for# backwards compatibility). Please don't change it unless you know what# you're doing.Vagrant.configure(\"2\") do |config| #config.vm.box = \"centos-7.2\" #config.vm.hostname = \"mydocker\" #config.vm.network \"private_network\", ip: \"192.168.10.9\"# #config.vm.synced_folder \"/data/docker/registry\", \"/docker/registry\" #config.vm.synced_folder \"/data/docker/works\", \"/docker/works\"# #config.vm.provider \"virtualbox\" do |vb| # #vb.gui = true # vb.memory = \"2048\" #end # #config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL # systemctl restart network #SHELL #config.vm.provision \"shell\", path: \"script.sh\" config.vm.define :k8s_master do |k8s_master| k8s_master.vm.box = \"centos-7.2\" k8s_master.vm.hostname = \"k8s-master\" k8s_master.vm.network \"private_network\", ip: \"192.168.10.6\" k8s_master.vm.synced_folder \"/data/docker/registry\", \"/docker/registry\" k8s_master.vm.synced_folder \"/data/docker/works\", \"/docker/works\" k8s_master.vm.synced_folder \"/data/docker/k8s\", \"/docker/k8s\" k8s_master.vm.provider \"virtualbox\" do |vb| #vb.gui = true vb.memory = \"1024\" end k8s_master.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL systemctl restart network SHELL k8s_master.vm.provision \"shell\" do |s| s.path = \"script.sh\" s.args = [\"--bip=10.1.10.1/24\"] end #k8s_master.vm.provision \"shell\", path: \"script.sh\" end config.vm.define :k8s_node1 do |k8s_node1| k8s_node1.vm.box = \"centos-7.2\" k8s_node1.vm.hostname = \"k8s-node1\" k8s_node1.vm.network \"private_network\", ip: \"192.168.10.7\" k8s_node1.vm.synced_folder \"/data/docker/registry\", \"/docker/registry\" k8s_node1.vm.synced_folder \"/data/docker/works\", \"/docker/works\" k8s_node1.vm.synced_folder \"/data/docker/k8s\", \"/docker/k8s\" k8s_node1.vm.provider \"virtualbox\" do |vb| #vb.gui = true vb.memory = \"1024\" end k8s_node1.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL systemctl restart network SHELL k8s_node1.vm.provision \"shell\" do |s| s.path = \"script.sh\" s.args = [\"--bip=10.1.20.1/24\"] end end #config.vm.define :k8s_node2 do |k8s_node2| # k8s_node2.vm.box = \"centos-7.2\" # k8s_node2.vm.hostname = \"k8s-node2\" # k8s_node2.vm.network \"private_network\", ip: \"192.168.10.8\" # k8s_node2.vm.synced_folder \"/data/docker/registry\", \"/docker/registry\" # k8s_node2.vm.synced_folder \"/data/docker/works\", \"/docker/works\" # k8s_node2.vm.synced_folder \"/data/docker/k8s\", \"/docker/k8s\" # k8s_node2.vm.provider \"virtualbox\" do |vb| # #vb.gui = true # vb.memory = \"512\" # end # k8s_node2.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL # systemctl restart network # SHELL # k8s_node2.vm.provision \"shell\" do |s| # s.path = \"script.sh\" # s.args = [\"--bip=10.1.30.1/24\"] # end #end end 定义shell脚本编写script.sh脚本，安装一些基本的软件，具体可以参考：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173#!/bin/sh#http://www.360doc.com/content/14/1125/19/7044580_428024359.shtml#http://blog.csdn.net/54powerman/article/details/50684844#http://c.biancheng.net/cpp/view/2739.htmlecho \"scripting......\"filepath=/vagrantbip=$1hostname=$2if [[ \"$hostname\" != \"\" ]]; then hostnamectl --static set-hostname $hostname sysctl kernel.hostname=$hostnamefi#关闭内核安全(如果是vagrant方式，第一次完成后需要重启vagrant才能生效。)sed -i 's;SELINUX=.*;SELINUX=disabled;' /etc/selinux/configsetenforce 0getenforcecat /etc/NetworkManager/NetworkManager.conf|grep \"dns=none\" &gt; /dev/nullif [[ $? != 0 ]]; then echo \"dns=none\" &gt;&gt; /etc/NetworkManager/NetworkManager.conf systemctl restart NetworkManager.servicefisystemctl disable iptablessystemctl stop iptablessystemctl disable firewalldsystemctl stop firewalldln -sf /usr/share/zoneinfo/Asia/Chongqing /etc/localtime#logined limitcat /etc/security/limits.conf|grep 100000 &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF* soft nofile 100000* hard nofile 100000* soft nproc 100000* hard nproc 100000EOFfised -i 's;4096;100000;g' /etc/security/limits.d/20-nproc.conf#systemd service limitcat /etc/systemd/system.conf|egrep '^DefaultLimitCORE' &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/systemd/system.conf &lt;&lt; EOFDefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000EOFficat /etc/sysctl.conf|grep \"net.ipv4.ip_local_port_range\" &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 300net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.ip_local_port_range = 1024 65535net.ipv4.ip_forward = 1EOFsysctl -pfisu - root -c \"ulimit -a\"#echo '192.168.10.6 k8s-master#192.168.10.7 k8s-node1#192.168.10.8 k8s-node2' &gt;&gt; /etc/hosts##sed -i 's;en_GB;zh_CN;' /etc/sysconfig/i18n#yum -y install gcc kernel-develmv -f /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup#wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repowget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repotee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[docker]name=Docker Repositorybaseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=http://mirrors.aliyun.com/docker-engine/yum/gpgEOF##tee /etc/yum.repos.d/k8s.repo &lt;&lt;-'EOF'#[k8s-repo]#name=kubernetes Repository##baseurl=https://rpm.mritd.me/centos/7/x86_64##baseurl=file:///docker/works/yum#baseurl=http://www.gcalls.cn/yum#enabled=1#gpgcheck=1##gpgkey=https://cdn.mritd.me/keys/rpm.public.key##gpgkey=file:///docker/works/yum/gpg#gpgkey=http://www.gcalls.cn/yum/gpg#EOFyum -y install epel-releaseyum clean allyum makecache#yum -y install createrepo rpm-sign rng-tools yum-utils yum -y install bind-utils bridge-utils ntpdate setuptool iptables system-config-securitylevel-tui system-config-network-tui \\ ntsysv net-tools lrzsz telnet lsof vim dos2unix unix2dos zip unzip#install docker-compose-----------------------------------------------rpm -e docker-1.10.3-59.el7.centos.x86_64 \\ docker-common-1.10.3-59.el7.centos.x86_64 \\ container-selinux-1.10.3-59.el7.centos.x86_64 &gt; /dev/null 2&gt;&amp;1yum install docker-engine -yyum -y install python2-pippip install -U docker-compose##yum install -y etcd kubernetes##sed -i \"s;^ExecStart=/usr/bin/dockerd$;ExecStart=/usr/bin/dockerd --registry-mirror=https://3gbbfq7n.mirror.aliyuncs.com;\" /usr/lib/systemd/system/docker.service##sed -i \"s;^ExecStart=/usr/bin/dockerd$;ExecStart=/usr/bin/dockerd $&#123;bip&#125; --live-restore \\sed -i \"s;^ExecStart=/usr/bin/dockerd$;ExecStart=/usr/bin/dockerd \\--registry-mirror=http://3fecfd09.m.daocloud.io \\--registry-mirror=https://3gbbfq7n.mirror.aliyuncs.com \\--registry-mirror=http://zhaoxunyong.m.alauda.cn;\" \\/usr/lib/systemd/system/docker.service#mkdir -p /etc/systemd/system/docker.service.d#cat &gt;&gt; /etc/systemd/system/docker.service.d/http-proxy.conf &lt;&lt; EOF#[Service]#Environment=\"HTTP_PROXY=http://thenorth.f.ftq.me:52579\"#Environment=\"HTTPS_PROXY=http://thenorth.f.ftq.me:52579\"#Environment=\"NO_PROXY=127.0.0.1,localhost,10.0.0.0/8,172.0.0.0/8,192.168.0.0/16,*.zerofinance.net\"#EOF##systemctl daemon-reload#systemctl show --property=Environment dockersystemctl restart dockersystemctl enable docker#cd /docker/works/images/Vagrant环境搭建/k8s/#./importK8s.sh##docker load -i /docker/works/images/Vagrant环境搭建/others/redis-master.tar #docker load -i /docker/works/images/Vagrant环境搭建/others/guestbook-redis-slave.tar #docker load -i /docker/works/images/Vagrant环境搭建/others/guestbook-php-frontend.tar##docker load -i /docker/works/images/Vagrant环境搭建/k8s/tar/quagga.tar#docker run -itd --name=router --privileged --net=host index.alauda.cn/georce/router#docker start `docker ps -a |grep 'index.alauda.cn/georce/router'|awk '&#123;print $1&#125;'`#install docker-engine end-----------------------------------------------#mkdir /usr/local/java &gt; /dev/null 2&gt;&amp;1 #cd $filepath/files#tar zxf jdk-8u111-linux-x64.tar.gz -C /usr/local/java/#ln -sf /usr/local/java/jdk1.8.0_111 /usr/local/java/jdk##cat /etc/profile|grep \"JAVA_HOME\" &gt; /dev/null#if [[ $? != 0 ]]; then#cat &gt;&gt; /etc/profile &lt;&lt; EOF# export JAVA_HOME=/usr/local/java/jdk# export PATH=\\$JAVA_HOME/bin:\\$PATH#EOF# source /etc/profile#fi# centos6的脚本会有所不同，可以参考12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#!/bin/sh#http://www.360doc.com/content/14/1125/19/7044580_428024359.shtmlecho \"scripting......\"filepath=/vagrantsed -i 's;en_GB;zh_CN;' /etc/sysconfig/i18nyum -y install yum-fastestmirrorif [ ! -f \"/etc/yum.repos.d/CentOS-Base.repo.from.aliyun.backup\" ]; then mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.from.aliyun.backup #wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS6-Base-163.repo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo yum clean all yum makecachefi #wget http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpmrpm -ivh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm#yum install gcc gcc-c++ make bind-untils libevent libevent-devel sysstat autoconf \\# curl curl-devel -y#yum install gcc gcc-c++ kernel-devel make autoconf libevent libevent-devel bind-untilsyum -y install ntpdate net-tools setuptool iptables system-config-securitylevel-tui system-config-network-tui \\ ntsysv net-tools lrzsz telnet lsof dos2unix unix2dos zip unzip vim curl curl-devel #升级内核为3.10rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org#cd /etc/yum.repos.d/rpm -ivh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpmyum --enablerepo=elrepo-kernel install kernel-lt kernel-lt-devel -y#修改grub.conf文件的default=0sed -i 's;^default=.*;default=0;' /etc/grub.confsed -i 's;^default=.*;default=0;' /boot/grub/grub.conf /opt/VBoxGuestAdditions-4.3.30/init/vboxadd setup#reboot uname -r#chkconfig --level 35 memcached on#service iptables stop#chkconfig --level 35 iptables on#关闭内核安全(如果是vagrant方式，第一次完成后需要重启vagrant才能生效。)sed -i 's;SELINUX=.*;SELINUX=disabled;' /etc/selinux/configsetenforce 0getenforceln -sf /usr/share/zoneinfo/Asia/Chongqing /etc/localtimecat /etc/security/limits.conf|grep 65535 &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF * soft nofile 65535 * hard nofile 65535 * soft nproc 65535 * hard nproc 65535EOFficat /etc/sysctl.conf|grep \"net.ipv4.ip_local_port_range\" &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOF net.ipv4.tcp_fin_timeout = 30 net.ipv4.tcp_keepalive_time = 300 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.ip_local_port_range = 1024 65535EOF sysctl -pfi#mkdir /usr/local/java &gt; /dev/null 2&gt;&amp;1 #cd $filepath/files#tar zxf jdk-8u91-linux-x64.tar.gz -C /usr/local/java/#ln -sf /usr/local/java/jdk1.8.0_91 /usr/local/java/jdk##cat /etc/profile|grep \"JAVA_HOME\" &gt; /dev/null#if [[ $? != 0 ]]; then#cat &gt;&gt; /etc/profile &lt;&lt; EOF#export JAVA_HOME=/usr/local/java/jdk#export PATH=\\$JAVA_HOME/bin:\\$PATH#EOF# source /etc/profile#fi##yum -y install libevent libevent-devel zlib zlib-devel pcre pcre-devel openssl openssl-devel 基本命令启动：1sudo vagrant up 第一次时初始化环境，会调用script.sh脚本。如果需要每次都调用script.sh脚本，需要为provision指定run:”always”属性启动时运行，在启动命令加 –provision 参数,适用于 vagrant up 和 vagrant reloadvm启动状态时，执行 vagrant provision 命令。 第一次启动后，运行sestatus命令显示的还是enable状态，需要重启一下vagrant才会显示disabled。 关闭：1sudo vagrant halt 重启：1sudo vagrant reload 销毁：1sudo vagrant destroy 注意：此操作会删除已创建虚拟机中的所有内容，操作请小心！！！ 登录虚拟机：1sudo vagrant ssh 如果有多个虚拟机时，后面需要加上对应的名称1sudo vagrant ssh k8s_master 导出:将配置好的环境打包给其他同事使用1vagrant package --output centos-export.box 卸载123sudo rm -rf /Applications/Vagrantsudo rm -f /usr/local/bin/vagrantsudo pkgutil --forget com.vagrant.vagrant 快照安装1vagrant plugin install vagrant-vbox-snapshot 操作命令参考http://blog.huatai.me/2015/12/03/use-vagrant-snapshot-create-backup/支持的参数如下：12345vagrant snapshot take [vm-name] &lt;SNAPSHOT_NAME&gt; # take snapshot, labeled by NAMEvagrant snapshot list [vm-name] # list snapshotsvagrant snapshot back [vm-name] # restore last taken snapshotvagrant snapshot delete [vm-name] &lt;SNAPSHOT_NAME&gt; # delete specified snapshotvagrant snapshot go [vm-name] &lt;SNAPSHOT_NAME&gt; # restore specified snapshot 创建快照:1vagrant snapshot take k8s_master k8s_master_snapshot 虚构机的名称可以通过以下命令查看：12345678910vagrant statusCurrent machine states:k8s_master running (virtualbox)k8s_node1 running (virtualbox)k8s_node2 running (virtualbox)This environment represents multiple VMs. The VMs are all listedabove with their current state. For more information about a specificVM, run `vagrant status NAME`. 查看快照列表:1vagrant snapshot list 含有多个虚拟机时，需要指定对应的名称：1vagrant snapshot list k8s_master 从指定快照中恢复:1vagrant snapshot go \"k8s_master_snapshot\" 删除一个快照:1vagrant snapshot delete \"k8s_master_snapshot\" 异常解决/sbin/mount.vboxsf: mounting failed with the error: No such device的解决办法：123yum -y install gcc kernel-devel make/opt/VBoxGuestAdditions-5.1.10/init/vboxadd setupsudo vagrant reload Hyper-V如果有在windows 10安装docker的话，只能使用Hyper-V，VirtualBox启动不了，总结一下使用Hyper-V的使用过程：(Hyper-V需要在管理员模式下运行才行) 下载hyperv对应的box文件1234#centos 7.3wget https://vagrantcloud.com/centos/boxes/7/versions/1707.01/providers/hyperv.box#centos 7.4wget https://vagrantcloud.com/centos/boxes/7/versions/1708.01/providers/hyperv.box 编写Vagrantfile1234567891011121314151617181920config.vm.define :k8s_master do |k8s_master| k8s_master.vm.box = \"centos-7.3\" k8s_master.vm.hostname = \"k8s-master\" k8s_master.vm.network \"private_network\", ip: \"192.168.10.6\" #k8s_master.vm.synced_folder \"d:/data/docker\", \"/docker\" k8s_master.vm.provider \"hyperv\" do |vb| #vb.gui = true vb.memory = \"1024\" vb.cpus = 1 end k8s_master.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL systemctl restart network SHELL k8s_master.vm.provision \"shell\" do |s| s.path = \"script.sh\" #s.args = [\"k8s-master\", \"--bip=10.1.10.1/24\"] s.args = [\"k8s-master\"] end #k8s_master.vm.provision \"shell\", path: \"script.sh\"end 配置Hyper-V网络参考https://quotidian-ennui.github.io/blog/2016/08/17/vagrant-windows10-hyperv/如果不配置网络的话，Hyper-V连接不到网络： 也可以手动添加： 好像Hyper-V不能用private_network，只能用public_network… 注意：对应的box没有wget，可能需要在script.sh脚本中加入：1yum -y install wget 生成Hyper-V box还在继续研究中… 参考 http://www.vincentguo.cn/default/26.htmlhttp://topmanopensource.iteye.com/blog/2002302http://blog.csdn.net/54powerman/article/details/50684844https://my.oschina.net/u/148605/blog/385049","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.gcalls.cn/tags/Linux/"}]},{"title":"Kubernetes集群搭建","slug":"Kubernetes集群搭建","date":"2017-01-03T01:11:20.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/Kubernetes集群搭建.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/Kubernetes集群搭建.html","excerpt":"Kubernetes就不介绍了，用了都说好。","text":"Kubernetes就不介绍了，用了都说好。 环境准备3台虚拟机具体参考：Vagrant环境搭建docker版本为：1.12.5kubernetes版本为：v1.5.1 主机IP 主机名称 软件 内存 192.168.10.6 k8s-master docker、kube-dns、kube-apiserver、kube-controller-manager、kube-scheduler 1024m 192.168.10.7 k8s-node1 docker、kube-proxy、kubelet 512m 192.168.10.8 k8s-node2 docker、kube-proxy、kubelet 512m 安装基础软件以下为每台都需要同样的操作 配置yum源：12345678[root@k8s-master ~]$ tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[docker]name=Docker Repositorybaseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=http://mirrors.aliyun.com/docker-engine/yum/gpgEOF 安装：1yum -y install docker-engine 修改配置：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#修改dnscat /etc/NetworkManager/NetworkManager.conf|grep \"dns=none\" &gt; /dev/nullif [[ $? != 0 ]]; then echo \"dns=none\" &gt;&gt; /etc/NetworkManager/NetworkManager.conf systemctl restart NetworkManager.servicefi#修改时区ln -sf /usr/share/zoneinfo/Asia/Chongqing /etc/localtime#关闭内核安全(如果是vagrant方式，第一次完成后需要重启vagrant才能生效。)sed -i 's;SELINUX=.*;SELINUX=disabled;' /etc/selinux/config#setenforce 0#getenforce#reboot[root@k8s-node2 ~]# sestatusSELinux status: disabled#关闭防火墙systemctl disable iptablessystemctl stop iptablessystemctl disable firewalldsystemctl stop firewalld#优化内核cat /etc/security/limits.conf|grep 65535 &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF* soft nofile 65535* hard nofile 65535* soft nproc 65535* hard nproc 65535EOFfi#打开端口转发#永久修改：/etc/sysctl.conf中的net.ipv4.ip_forward=1，生效：sysctl -p#临时修改：echo 1 &gt; /proc/sys/net/ipv4/ip_forward，重启后失效cat /etc/sysctl.conf|grep \"net.ipv4.ip_forward\" &gt; /dev/nullif [[ $? != 0 ]]; thencat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 300net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.ip_local_port_range = 1024 65535net.ipv4.ip_forward = 1EOFsysctl -pfisu - root -c \"ulimit -a\" docker加速：123sed -i \"s;^ExecStart=/usr/bin/dockerd$;ExecStart=/usr/bin/dockerd \\ --registry-mirror=http://3fecfd09.m.daocloud.io;\" \\ /usr/lib/systemd/system/docker.service docker启动:1234systemctl daemon-reloadsystemctl enable dockersystemctl start dockersystemctl status docker 镜像下载由于kubernetes需要访问grc.io，国内无法访问。可以在安装完成docker后，先下载对应的docker镜像：123456images=(pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.0)for imageName in $&#123;images[@]&#125; ; do docker pull mritd/$imageName docker tag mritd/$imageName gcr.io/google_containers/$imageName docker rmi mritd/$imageNamedone 修改配置192.168.10.6执行：1234567891011121314151617181920212223242526272829303132333435363738394041424344hostnamectl --static set-hostname k8s-master#初始化目录mkdir -p /etc/kubernetes/ssl/# 不重启情况下使内核生效sysctl kernel.hostname=k8s-masterecho '192.168.10.6 k8s-master192.168.10.7 k8s-node1192.168.10.8 k8s-node2' &gt;&gt; /etc/hosts#采用直接路由，docker0的网段不能一样，所以需要修改docker的子网地址--bip=10.1.10.1/24#vim /usr/lib/systemd/system/docker.service#在/etc/sysconfig/docker的OPTIONS添加：(1.10版本才生效)sed -i \"s;^ExecStart=/usr/bin/dockerd.*;ExecStart=/usr/bin/dockerd --bip=10.1.10.1/24 \\--registry-mirror=http://3fecfd09.m.daocloud.io;\" \\ /usr/lib/systemd/system/docker.servicesystemctl daemon-reloadsystemctl restart docker#打通router##192.168.10.6:#route add -net 10.1.20.0 netmask 255.255.255.0 gw 192.168.10.7#route add -net 10.1.30.0 netmask 255.255.255.0 gw 192.168.10.8##192.168.10.7:#route add -net 10.1.10.0 netmask 255.255.255.0 gw 192.168.10.6#route add -net 10.1.30.0 netmask 255.255.255.0 gw 192.168.10.8##192.168.10.8:#route add -net 10.1.10.0 netmask 255.255.255.0 gw 192.168.10.6#route add -net 10.1.20.0 netmask 255.255.255.0 gw 192.168.10.7#手动打通路由比较麻烦，建议通过Quagga打通docker pull index.alauda.cn/georce/router#保存至文件，下次可以直接导入，不用再下载#docker save -o /docker/works/images/Kubernetes集群搭建/k8s/tar/quagga.tar#docker load -i /docker/works/images/Kubernetes集群搭建/k8s/tar/quagga.tardocker run -itd --name=router --privileged --net=host index.alauda.cn/georce/router#注意，系统重启时要自动启动quagga，否则会有问题。可以把以下命令加到/etc/rc.local中：#docker start `docker ps -a |grep 'index.alauda.cn/georce/router'|awk '&#123;print $1&#125;'`echo \"docker start \\`docker ps -a |grep 'index.alauda.cn/georce/router'|awk '&#123;print \\$1&#125;'\\`\" &gt;&gt; /etc/rc.local#执行 ip route 查看下路由表，已有别的docker0的网段信息。 192.168.10.7执行：1234567891011121314151617181920212223242526272829303132hostnamectl --static set-hostname k8s-node1#初始化目录/etc/kubernetes/ssl/# 不重启情况下使内核生效sysctl kernel.hostname=k8s-node1echo '192.168.10.6 k8s-master192.168.10.7 k8s-node1192.168.10.8 k8s-node2' &gt;&gt; /etc/hosts#采用直接路由，docker0的网段不能一样，所以需要修改docker的子网地址--bip=10.1.20.1/24#vim /usr/lib/systemd/system/docker.service#在/etc/sysconfig/docker的OPTIONS添加：(1.10版本才生效)sed -i \"s;^ExecStart=/usr/bin/dockerd.*;ExecStart=/usr/bin/dockerd --bip=10.1.20.1/24 \\--registry-mirror=http://3fecfd09.m.daocloud.io;\" \\ /usr/lib/systemd/system/docker.servicesystemctl daemon-reloadsystemctl restart docker#手动打通路由比较麻烦，建议通过Quagga打通docker pull index.alauda.cn/georce/router#保存至文件，下次可以直接导入，不用再下载#docker save -o /docker/works/images/Kubernetes集群搭建/k8s/tar/quagga.tar#docker load -i /docker/works/images/Kubernetes集群搭建/k8s/tar/quagga.tardocker run -itd --name=router --privileged --net=host index.alauda.cn/georce/router#注意，系统重启时要自动启动quagga，否则会有问题。可以把以下命令加到/etc/rc.local中：#docker start `docker ps -a |grep 'index.alauda.cn/georce/router'|awk '&#123;print $1&#125;'`echo \"docker start \\`docker ps -a |grep 'index.alauda.cn/georce/router'|awk '&#123;print $1&#125;'\\`\" &gt;&gt; /etc/rc.local#执行 ip route 查看下路由表，已有别的docker0的网段信息。 192.168.10.8执行：1234567891011121314151617181920212223242526272829303132hostnamectl --static set-hostname k8s-node2#初始化目录/etc/kubernetes/ssl/# 不重启情况下使内核生效sysctl kernel.hostname=k8s-node2echo '192.168.10.6 k8s-master192.168.10.7 k8s-node1192.168.10.8 k8s-node2' &gt;&gt; /etc/hosts#采用直接路由，docker0的网段不能一样，所以需要修改docker的子网地址--bip=10.1.30.1/24#vim /usr/lib/systemd/system/docker.service#在/etc/sysconfig/docker的OPTIONS添加：(1.10版本才生效)sed -i \"s;^ExecStart=/usr/bin/dockerd.*;ExecStart=/usr/bin/dockerd --bip=10.1.30.1/24 \\--registry-mirror=http://3fecfd09.m.daocloud.io;\" \\ /usr/lib/systemd/system/docker.servicesystemctl daemon-reloadsystemctl restart docker#手动打通路由比较麻烦，建议通过Quagga打通docker pull index.alauda.cn/georce/router#保存至文件，下次可以直接导入，不用再下载#docker save -o /docker/works/images/Kubernetes集群搭建/k8s/tar/quagga.tar#docker load -i /docker/works/images/Kubernetes集群搭建/k8s/tar/quagga.tardocker run -itd --name=router --privileged --net=host index.alauda.cn/georce/router#注意，系统重启时要自动启动quagga，否则会有问题。可以把以下命令加到/etc/rc.local中：#docker start `docker ps -a |grep 'index.alauda.cn/georce/router'|awk '&#123;print $1&#125;'`echo \"docker start \\`docker ps -a |grep 'index.alauda.cn/georce/router'|awk '&#123;print $1&#125;'\\`\" &gt;&gt; /etc/rc.local#执行 ip route 查看下路由表，已有别的docker0的网段信息。 安装kubernetesyum安装每台添加yum源：12345678tee /etc/yum.repos.d/k8s.repo &lt;&lt;-'EOF'[k8s-repo]name=kubernetes Repositorybaseurl=https://rpm.mritd.me/centos/7/x86_64enabled=1gpgcheck=1gpgkey=https://cdn.mritd.me/keys/rpm.public.keyEOF 如果这个源不稳定的话，可以下载我创建好的源，直接通过yum localinstall *.rpm方式安装123git clone https://git.coding.net/zhaoxunyong/repo.gitcd repo/yum/kubernetes/x86_64yum -y localinstall kubernetes-1.5.1-git82450d0.el7.centos.x86_64.rpm master(192.168.10.6)执行 1yum install -y etcd kubernetes 其他安装：1yum -y install kubernetes 二进制安装12345wget https://storage.googleapis.com/kubernetes-release/release/v1.5.1/kubernetes.tar.gztar zxvf kubernetes.tar.gz#下载对应的server与client文件cd kubernetessh cluster/get-kube-binaries.sh 下载好的文件分别位于：12server/kubernetes-server-linux-amd64.tar.gzclient/kubernetes-client-linux-amd64.tar.gz 具体安装步骤稍后补充 rpm方式配置kubernetesmaster(192.168.10.6)执行证书制作如果不采用证书方式安装，请略过此节。 自签 CA123456# 创建证书存放目录mkdir cert &amp;&amp; cd cert# 创建 CA 私钥openssl genrsa -out ca-key.pem 2048# 自签 CAopenssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj \"/CN=kube-ca\" 签署 apiserver 证书首先先修改openssl的配置123456789101112131415161718192021# 复制 openssl 配置文件#cp /etc/pki/tls/openssl.cnf .# 编辑 openssl 配置使其支持 IP 认证vim openssl.cnf[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = kubernetesDNS.2 = kubernetes.defaultDNS.3 = kubernetes.default.svcDNS.4 = kubernetes.default.svc.cluster.local# kubernetes server ipIP.1 = 10.254.0.1# master ip(如果都在一台机器上写一个就行)IP.2 = 192.168.10.6 签署apiserver相关的证书123456# 生成apiserver私钥openssl genrsa -out apiserver-key.pem 2048# 生成签署请求openssl req -new -key apiserver-key.pem -out apiserver.csr -subj \"/CN=kube-apiserver\" -config openssl.cnf# 使用自建 CA 签署openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf 签署 node 的证书还需要签署每个节点的证书先修改一下openssl配置12345678910111213141516# copy master 的 openssl 配置#cp openssl.cnf worker-openssl.cnf# 修改 worker-openssl 配置vim worker-openssl.cnf[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]# 此处填写node的内网ip，多个node ip地址以此类推IP.2 = NODE2-IPIP.1 = 192.168.10.7IP.2 = 192.168.10.8 签署node1的证书 123456# 生成 node1 私钥openssl genrsa -out node1-worker-key.pem 2048# 生成 签署请求openssl req -new -key node1-worker-key.pem -out node1-worker.csr -subj \"/CN=node1\" -config worker-openssl.cnf# 使用自建 CA 签署openssl x509 -req -in node1-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out node1-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf 签署node2的证书123456# 生成 node2 私钥openssl genrsa -out node2-worker-key.pem 2048# 生成 签署请求openssl req -new -key node2-worker-key.pem -out node2-worker.csr -subj \"/CN=node2\" -config worker-openssl.cnf# 使用自建 CA 签署openssl x509 -req -in node2-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out node2-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf 生成集群管理证书1234# 签署一个集群管理证书openssl genrsa -out admin-key.pem 2048openssl req -new -key admin-key.pem -out admin.csr -subj \"/CN=kube-admin\"openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365 配置证书生成kubeconfig.yaml文件此文件供需要通过证书方式访问apiserver时使用：12345678910111213141516171819[root@k8s-master ~]$ vim kubeconfig.yamlapiVersion: v1clusters:- cluster: certificate-authority: /etc/kubernetes/ssl/ca.pem server: https://192.168.10.6:6443 name: default-clustercontexts:- context: cluster: default-cluster user: default-admin name: default-systemcurrent-context: default-systemkind: Configusers:- name: default-admin user: client-certificate: /etc/kubernetes/ssl/admin.pem client-key: /etc/kubernetes/ssl/admin-key.pem 生成worker1-kubeconfig.yaml文件此文件供需要通过证书方式访问kubelet时使用：123456789101112131415161718[root@k8s-master ~]$ vim worker1-kubeconfig.yamlapiVersion: v1kind: Configclusters:- name: local cluster: certificate-authority: /etc/kubernetes/ssl/ca.pemusers:- name: kubelet user: client-certificate: /etc/kubernetes/ssl/node1-worker.pem client-key: /etc/kubernetes/ssl/node1-worker-key.pemcontexts:- context: cluster: local user: kubelet name: kubelet-contextcurrent-context: kubelet-context 生成worker2-kubeconfig.yaml文件此文件供需要通过证书方式访问kubelet时使用：123456789101112131415161718[root@k8s-master ~]$ vim worker2-kubeconfig.yamlapiVersion: v1kind: Configclusters:- name: local cluster: certificate-authority: /etc/kubernetes/ssl/ca.pemusers:- name: kubelet user: client-certificate: /etc/kubernetes/ssl/node2-worker.pem client-key: /etc/kubernetes/ssl/node2-worker-key.pemcontexts:- context: cluster: local user: kubelet name: kubelet-contextcurrent-context: kubelet-context copy证书123456789101112131415# 先把证书 copy 到配置目录mkdir -p /etc/kubernetes/ssl/bin/cp -a ca.pem apiserver.pem apiserver-key.pem \\ admin.pem admin-key.pem \\ kubeconfig.yaml \\ /etc/kubernetes/ssl# rpm 安装的 kubernetes 默认使用 kube 用户，需要更改权限chown kube:kube -R /etc/kubernetes/ssl# copy证书到所有node节点：scp kubeconfig.yaml admin.pem admin-key.pem ca.pem \\ node1-worker.pem node1-worker-key.pem worker1-kubeconfig.yaml root@192.168.10.7:/etc/kubernetes/ssl/scp kubeconfig.yaml admin.pem admin-key.pem ca.pem \\ node2-worker.pem node2-worker-key.pem worker2-kubeconfig.yaml root@192.168.10.8:/etc/kubernetes/ssl/ etcd1234567891011121314151617181920sed -i 's;^ETCD_LISTEN_CLIENT_URLS=.*;ETCD_LISTEN_CLIENT_URLS=\"http://localhost:2379,http://192.168.10.6:2379\";' /etc/etcd/etcd.confsed -i 's;^ETCD_ADVERTISE_CLIENT_URLS=.*;ETCD_ADVERTISE_CLIENT_URLS=\"http://localhost:2379,http://192.168.10.6:2379\";' /etc/etcd/etcd.conf[root@k8s-master ~]$ grep -v ^# /etc/etcd/etcd.conf ETCD_NAME=defaultETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"ETCD_LISTEN_CLIENT_URLS=\"http://localhost:2379,http://192.168.10.6:2379\"ETCD_ADVERTISE_CLIENT_URLS=\"http://localhost:2379,http://192.168.10.6:2379\"systemctl restart etcd.servicesystemctl enable etcd.servicesystemctl status etcd.serviceetcdctl cluster-health#member 8e9e05c52164694d is healthy: got healthy result from http://192.168.10.6:2379#cluster is healthyetcdctl member list#8e9e05c52164694d: name=default peerURLs=http://localhost:2380 clientURLs=http://192.168.10.6:2379,http://localhost:2379 isLeader=true config123456789101112tee /etc/kubernetes/config &lt;&lt;-'EOF'KUBE_LOGTOSTDERR=\"--logtostderr=true\"KUBE_LOG_LEVEL=\"--v=0\"KUBE_ALLOW_PRIV=\"--allow-privileged=false\"KUBE_MASTER=\"--master=https://192.168.10.6:6443\"EOF[root@k8s-master ~]$ grep -v ^# /etc/kubernetes/configKUBE_LOGTOSTDERR=\"--logtostderr=true\"KUBE_LOG_LEVEL=\"--v=0\"KUBE_ALLOW_PRIV=\"--allow-privileged=false\"KUBE_MASTER=\"--master=https://192.168.10.6:6443\" apiserver修改：12345678910111213141516tee /etc/kubernetes/apiserver &lt;&lt;-'EOF'KUBE_API_ADDRESS=\"--bind-address=192.168.10.6 --insecure-bind-address=127.0.0.1\"KUBE_API_PORT=\"--secure-port=6443 --insecure-port=8080\"KUBE_ETCD_SERVERS=\"--etcd-servers=http://192.168.10.6:2379\"KUBE_SERVICE_ADDRESSES=\"--service-cluster-ip-range=10.254.0.0/16\"KUBE_ADMISSION_CONTROL=\"--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota\"KUBE_API_ARGS=\"--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\"EOF[root@k8s-master ~]$ grep -v ^# /etc/kubernetes/apiserverKUBE_API_ADDRESS=\"--bind-address=192.168.10.6 --insecure-bind-address=127.0.0.1\"KUBE_API_PORT=\"--secure-port=6443 --insecure-port=8080\"KUBE_ETCD_SERVERS=\"--etcd-servers=http://192.168.10.6:2379\"KUBE_SERVICE_ADDRESSES=\"--service-cluster-ip-range=10.254.0.0/16\"KUBE_ADMISSION_CONTROL=\"--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota\"KUBE_API_ARGS=\"--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\" 如果不使用证书的话：1234567[root@k8s-master ~]$ grep -v ^# /etc/kubernetes/apiserverKUBE_API_ADDRESS=\"--address=0.0.0.0\"KUBE_API_PORT=\"port=8080\"KUBE_ETCD_SERVERS=\"--etcd-servers=http://192.168.10.6:2379\"KUBE_SERVICE_ADDRESSES=\"--service-cluster-ip-range=10.254.0.0/16\"KUBE_ADMISSION_CONTROL=\"--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota\"KUBE_API_ARGS=\"\" 启动：123systemctl enable kube-apiserversystemctl restart kube-apiserversystemctl status kube-apiserver controller-manager修改：123456tee /etc/kubernetes/controller-manager &lt;&lt;-'EOF'KUBE_CONTROLLER_MANAGER_ARGS=\"--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --master=http://127.0.0.1:8080\"EOF[root@k8s-master ~]$ grep -v ^# /etc/kubernetes/controller-managerKUBE_CONTROLLER_MANAGER_ARGS=\"--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --master=http://127.0.0.1:8080\" 如果不使用证书的话：1KUBE_CONTROLLER_MANAGER_ARGS=\"\" 启动：123systemctl enable kube-controller-managersystemctl restart kube-controller-managersystemctl status kube-controller-manager scheduler修改：123456tee /etc/kubernetes/scheduler &lt;&lt;-'EOF'KUBE_SCHEDULER_ARGS=\"--kubeconfig=/docker/k8s/kubernetes/config\"EOF[root@k8s-master ~]$ grep -v ^# /etc/kubernetes/schedulerKUBE_SCHEDULER_ARGS=\"--kubeconfig=/etc/kubernetes/ssl/kubeconfig.yaml\" 如果不使用证书的话：1KUBE_SCHEDULER_ARGS=\"\" 启动：123systemctl enable kube-schedulersystemctl restart kube-schedulersystemctl status kube-scheduler kube-dns参考： http://www.pangxie.space/docker/1055https://my.oschina.net/fufangchun/blog/732762https://seanzhau.com/blog/post/seanzhau/6261234da213 随便说一下kube-dns的作用：kubernetes服务发现有两种： 环境变量方式: 这种方式必须要先创建service再创建pod，否则pod中没有对应的环境变量dns服务方式: 通过dns解析对应的服务，推荐使用。 域名规则：SERVICENAME.NAMESPACENAME.svc.CLUSTERDOMAIN SERVICENAME：每个Service的名字 NAMESPACENAME：Service所属的namespace的名字 svc：固定值 CLUSTERDOMAIN：集群内部的域名 解析特点：从上面可以看出，我们的域名是又臭又长，看起来很不爽。但是在kubernetes集群中，我们在解析的时候不是必须完全输入完才可以解析。在同一个命令空间下如果我们引用的话，只需要引用对应的Service的名字。如果引用了非同一命名空间下的Service，那么我们只需要加上其对应的命名空间的名字即可。例如：a命名空间(namespace)下有个Service:s1 App: a1，b命名空间(namespace)下有个Service:s2 App: a2现在App a1中需要使用a1 和 a2，那么只需要写出 a1 和 a2.b即可。反过来a2也是这样。 kube-dns为1.3新增的功能，不用再手动安装skyDns，使用更方便，但没有包括在rpm包中。我们可以手动从二进制包中copy到/usr/bin目录中:1234567891011121314151617cp kube-dns /usr/bin/#新建kube-dns配置文件tee /etc/kubernetes/kube-dns &lt;&lt;-'EOF'# kubernetes kube-dns configKUBE_DNS_PORT=\"--dns-port=53\"KUBE_DNS_DOMAIN=\"--domain=k8s.zxy.com\"#KUBE_DNS_MASTER=\"--kube-master-url=http://127.0.0.1:8080\"KUBE_DNS_ARGS=\"--kubecfg-file=/etc/kubernetes/ss/kubeconfig.yaml\"EOF[root@k8s-master]# grep -v ^# /etc/kubernetes/kube-dns#### kubernetes kube-dns configKUBE_DNS_PORT=\"--dns-port=53\"KUBE_DNS_DOMAIN=\"--domain=k8s.zxy.com\"KUBE_DNS_ARGS=\"--kubecfg-file=/etc/kubernetes/ssl/kubeconfig.yaml\" 如果不使用证书的话：1234KUBE_DNS_PORT=\"--dns-port=53\"KUBE_DNS_DOMAIN=\"--domain=k8s.zxy.com\"KUBE_DNS_MASTER=\"--kube-master-url=http://127.0.0.1:8080\"KUBE_DNS_ARGS=\"\" 新建kube-dns.service配置文件1234567891011121314151617181920tee /usr/lib/systemd/system/kube-dns.service &lt;&lt;-'EOF'[Unit]Description=Kubernetes Kube-dns ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=kube-apiserver.serviceRequires=kube-apiserver.service [Service]WorkingDirectory=/var/lib/kube-dnsEnvironmentFile=-/etc/kubernetes/kube-dnsExecStart=/usr/bin/kube-dns \\ $KUBE_DNS_PORT \\ $KUBE_DNS_DOMAIN \\ $KUBE_DNS_MASTER \\ $KUBE_DNS_ARGSRestart=on-failure [Install]WantedBy=multi-user.targetEOF 创建工作目录：1mkdir -p /var/lib/kube-dns 启动：123systemctl enable kube-dnssystemctl restart kube-dnssystemctl status kube-dns 注意：kube-dns重启好慢… 修改/etc/resolv.confmaster主机添加域名：123456789tee /etc/resolv.conf &lt;&lt;-'EOF'# k8s.zxy.com为对应的域名，其他保存不变search default.svc.k8s.zxy.com svc.k8s.zxy.com k8s.zxy.com# dns服务的ipnameserver 192.168.10.6 nameserver 8.8.8.8nameserver 114.114.114.114EOF 测试：123456789nslookup -type=srv kubernetesServer: 192.168.10.6Address: 192.168.10.6#53kubernetes.default.svc.k8s.zxy.com service = 10 100 0 3563366661643766.kubernetes.default.svc.k8s.zxy.com.curl http://127.0.0.1:8081/readinessokcurl http://127.0.0.1:8081/cache node1(192.168.10.7)执行修改/etc/resolv.conf添加域名：123456789tee /etc/resolv.conf &lt;&lt;-'EOF'# k8s.zxy.com为对应的域名，其他保存不变search default.svc.k8s.zxy.com svc.k8s.zxy.com k8s.zxy.com# dns服务的ipnameserver 192.168.10.6 nameserver 8.8.8.8nameserver 114.114.114.114EOF config修改：123456789101112tee /etc/kubernetes/config &lt;&lt;-'EOF'KUBE_LOGTOSTDERR=\"--logtostderr=true\"KUBE_LOG_LEVEL=\"--v=0\"KUBE_ALLOW_PRIV=\"--allow-privileged=false\"KUBE_MASTER=\"--master=https://192.168.10.6:6443\"EOF[root@k8s-node1 ~]$ grep -v ^# /etc/kubernetes/configKUBE_LOGTOSTDERR=\"--logtostderr=true\"KUBE_LOG_LEVEL=\"--v=0\"KUBE_ALLOW_PRIV=\"--allow-privileged=false\"KUBE_MASTER=\"--master=https://192.168.10.6:6443\" 如果不使用证书的话：1234KUBE_LOGTOSTDERR=\"--logtostderr=true\"KUBE_LOG_LEVEL=\"--v=0\"KUBE_ALLOW_PRIV=\"--allow-privileged=false\"KUBE_MASTER=\"--master=http://127.0.0.1:8080\" kubelet修改：123456789101112tee /etc/kubernetes/kubelet &lt;&lt;-'EOF'KUBELET_ADDRESS=\"--address=192.168.10.7\"KUBELET_HOSTNAME=\"--hostname-override=k8s-node1\"KUBELET_API_SERVER=\"--api-servers=https://192.168.10.6:6443\"KUBELET_ARGS=\"--tls-cert-file=/etc/kubernetes/ssl/node1-worker.pem --tls-private-key-file=/etc/kubernetes/ssl/node1-worker-key.pem --kubeconfig=/etc/kubernetes/ssl/worker1-kubeconfig.yaml --cluster-domain=k8s.zxy.com --cluster-dns=192.168.10.6\"EOF[root@k8s-node1 ~]$ grep -v ^# /etc/kubernetes/kubelet KUBELET_ADDRESS=\"--address=192.168.10.7\"KUBELET_HOSTNAME=\"--hostname-override=k8s-node1\"KUBELET_API_SERVER=\"--api-servers=https://192.168.10.6:6443\"KUBELET_ARGS=\"--tls-cert-file=/etc/kubernetes/ssl/node1-worker.pem --tls-private-key-file=/etc/kubernetes/ssl/node1-worker-key.pem --kubeconfig=/etc/kubernetes/ssl/worker1-kubeconfig.yaml --cluster-domain=k8s.zxy.com --cluster-dns=192.168.10.6\" 如果不使用证书的话：1234KUBELET_ADDRESS=\"--address=192.168.10.7\"KUBELET_HOSTNAME=\"--hostname-override=k8s-node1\"KUBELET_API_SERVER=\"--api-servers=http://192.168.10.6:8080\"KUBELET_ARGS=\"--cluster-domain=k8s.zxy.com --cluster-dns=192.168.10.6\" 启动：123systemctl enable kubeletsystemctl restart kubeletsystemctl status kubelet kube-proxy修改：123456tee /etc/kubernetes/proxy &lt;&lt;-'EOF'KUBE_PROXY_ARGS=\"--kubeconfig=/etc/kubernetes/ssl/worker1-kubeconfig.yaml\"EOF[root@k8s-node1 ~]$ grep -v ^# /etc/kubernetes/proxyKUBE_PROXY_ARGS=\"--kubeconfig=/etc/kubernetes/ssl/worker1-kubeconfig.yaml\" 如果不使用证书的话：1KUBE_PROXY_ARGS=\"\" 启动：123systemctl enable kube-proxysystemctl restart kube-proxysystemctl status kube-proxy 查看日志：1234567891011[root@k8s-node1 ~]$ tail -n10 -f /var/log/messages Jan 4 06:40:49 localhost systemd: Configuration file /usr/lib/systemd/system/wpa_supplicant.service is marked executable. Please remove executable permission bits. Proceeding anyway.Jan 4 06:40:49 localhost systemd: Started Kubernetes Kube-Proxy Server.Jan 4 06:40:49 localhost systemd: Starting Kubernetes Kube-Proxy Server...Jan 4 06:40:50 localhost kube-proxy: I0104 14:40:50.171482 12524 server.go:215] Using iptables Proxier.Jan 4 06:40:50 localhost kube-proxy: W0104 14:40:50.174673 12524 proxier.go:254] clusterCIDR not specified, unable to distinguish between internal and external trafficJan 4 06:40:50 localhost kube-proxy: I0104 14:40:50.174701 12524 server.go:227] Tearing down userspace rules.Jan 4 06:40:50 localhost kube-proxy: I0104 14:40:50.188726 12524 conntrack.go:81] Set sysctl 'net/netfilter/nf_conntrack_max' to 131072Jan 4 06:40:50 localhost kube-proxy: I0104 14:40:50.189159 12524 conntrack.go:66] Setting conntrack hashsize to 32768Jan 4 06:40:50 localhost kube-proxy: I0104 14:40:50.189401 12524 conntrack.go:81] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400Jan 4 06:40:50 localhost kube-proxy: I0104 14:40:50.189418 12524 conntrack.go:81] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600 node监控(ctAdvisor)http://192.168.10.7:4194/ node2(192.168.10.8)执行同node1一样，以下内容会有所不一样：1234567#kubeletKUBELET_ADDRESS=\"--address=192.168.10.8\"KUBELET_HOSTNAME=\"--hostname-override=k8s-node2\"KUBELET_ARGS=\"--tls-cert-file=/etc/kubernetes/ssl/node2-worker.pem --tls-private-key-file=/etc/kubernetes/ssl/node2-worker-key.pem --kubeconfig=/etc/kubernetes/ssl/worker2-kubeconfig.yaml --cluster-domain=k8s.zxy.com --cluster-dns=192.168.10.6\"#proxyKUBE_PROXY_ARGS=\"--master=https://192.168.10.6:6443 --kubeconfig=/etc/kubernetes/ssl/worker1-kubeconfig.yaml\" 启动服务12345for SERVICES in kubelet kube-proxy; dosystemctl enable $SERVICESsystemctl start $SERVICESsystemctl status $SERVICESdone 测试重启master所有服务：1234for SERVICES in etcd kube-dns kube-apiserver kube-controller-manager kube-scheduler; dosystemctl restart $SERVICESsystemctl status $SERVICESdone 重启node所有服务：1234for SERVICES in kubelet kube-proxy; dosystemctl restart $SERVICESsystemctl status $SERVICESdone 在master中可以查看node:123456789101112131415[root@k8s-master ~]$ kubectl get nodeNAME STATUS AGEk8s-node1 Ready 12mk8s-node2 Ready 7m#非master需要：[root@k8s-master ~]$ kubectl --kubeconfig=/etc/kubernetes/ssl/kubeconfig.yaml get nodeNAME STATUS AGEk8s-node1 Ready 12mk8s-node2 Ready 7m[root@k8s-master ~]$ curl https://192.168.10.6:6443/api/v1/nodes \\ --cert /etc/kubernetes/ssl/apiserver.pem --key /etc/kubernetes/ssl/apiserver-key.pem --cacert /etc/kubernetes/ssl/ca.pem#node中开启代理[root@k8s-node1 ~]$ kubectl --kubeconfig=/etc/kubernetes/ssl/kubeconfig.yaml proxy 二进制方式配置kubernetes二进制方式与rpm方式差不多，此处只简单列出非加密方式的启动命令: masterkube-apiserver:123456/usr/bin/kube-apiserver --logtostderr=true --v=0 \\ --etcd-servers=http://192.168.10.6:2379 \\ --address=0.0.0.0 port=8080 \\ --allow-privileged=false \\ --service-cluster-ip-range=10.254.0.0/16 \\ --admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota kube-controller-manager:1/usr/bin/kube-controller-manager --logtostderr=true --v=0 --master=http://127.0.0.1:8080 kube-scheduler:1/usr/bin/kube-scheduler --logtostderr=true --v=0 --master=http://192.168.10.6:8080 kube-dns:1/usr/bin/kube-dns --dns-port=53 --domain=k8s.zxy.com --kube-master-url=http://127.0.0.1:8080 nodekube-proxy:1/usr/bin/kube-proxy --logtostderr=true --v=0 --master=http://192.168.10.6:8080 kubelet:12345/usr/bin/kubelet --logtostderr=true --v=0 \\ --api-servers=http://192.168.10.6:8080 \\ --address=192.168.10.7 \\ --hostname-override=k8s-node1 \\ --allow-privileged=false --cluster-domain=k8s.zxy.com --cluster-dns=192.168.10.6 kube-dashboard创建对应的yaml文件可以参考dashborad 如果apiserver为非加密方式，需要添加args参数(与ports平行)：12args: - --apiserver-host=http://127.0.0.1:8080 开始创建：123456789101112131415161718git clone https://github.com/kubernetes/kubernetes.gitcd kubernetes/cluster/addons/dashboard/#docker load -i /docker/works/images/Kubernetes集群搭建/k8s/tar/kubernetes-dashboard-amd64.tar#docker load -i /docker/works/images/Kubernetes集群搭建/k8s/tar/pause-amd64.tar#create service[root@k8s-master ~]$ kubectl create -f dashboard-service.yaml service \"kubernetes-dashboard\" created# 查看service[root@k8s-master ~]$ kubectl get svc -o wide -n kube-systemNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes-dashboard 10.254.142.79 &lt;none&gt; 80/TCP 12s k8s-app=kubernetes-dashboard# create rc[root@k8s-master ~]$ kubectl create -f dashboard-controller.yaml replicationcontroller \"kubernetes-dashboard-v1.5.0\" created 如出现以下的日志表示创建成功：12345[root@k8s-master dashboard]# kubectl logs kubernetes-dashboard-v1.5.0-fnlhb -n kube-systemUsing HTTP port: 9090Creating API server client for https://10.254.0.1:443Successful initial request to the apiserver, version: 1.5.1Creating in-cluster Heapster client 异常解决ContainerCreating查看pod:123[root@k8s-master ~]$ kubectl get po -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODEkubernetes-dashboard-v1.5.0-7tjjx 0/1 ContainerCreating 0 1m &lt;none&gt; k8s-node2 查看pod详情:123456789[root@k8s-master dashboard]# kubectl describe pod kubernetes-dashboard-v1.5.0-7tjjx -n kube-system ...Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 2m 2m 1 &#123;default-scheduler &#125; Normal Scheduled Successfully assigned kubernetes-dashboard-v1.5.0-7tjjx to k8s-node2 1m 27s 2 &#123;kubelet k8s-node2&#125; Warning FailedSync Error syncing pod, skipping: failed to \"StartContainer\" for \"POD\" with ErrImagePull: \"image pull failed for gcr.io/google_containers/pause-amd64:3.0, this may be because there are no credentials on this request. details: (Error response from daemon: &#123;\\\"message\\\":\\\"Get https://gcr.io/v1/_ping: dial tcp 74.125.204.82:443: i/o timeout\\\"&#125;)\" 12s 12s 1 &#123;kubelet k8s-node2&#125; Warning FailedSync Error syncing pod, skipping: failed to \"StartContainer\" for \"POD\" with ImagePullBackOff: \"Back-off pulling image \\\"gcr.io/google_containers/pause-amd64:3.0\\\"\" 通过日志可以看到连不到gcr.io，需要事先下载： gcr.io/google_containers/pause-amd64:3.0gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.0注意：需要下载并导入到node节点，而不是master节点。具体请参考镜像下载 CrashLoopBackOff 参考：https://github.com/kubernetes/dashboard/issues/374查看pod日志：12345kubectl logs kubernetes-dashboard-v1.5.0-sp8qv -n kube-system Using HTTP port: 9090Creating API server client for https://10.254.0.1:443Error while initializing connection to Kubernetes apiserver. This most likely means that the cluster is misconfigured (e.g., it has invalid apiserver certificates or service accounts configuration) or the --apiserver-host param points to a server that does not exist. Reason: the server has asked for the client to provide credentialsRefer to the troubleshooting guide for more information: https://github.com/kubernetes/dashboard/blob/master/docs/user-guide/troubleshooting.md 请安装以下方式操作：123456789101112131415161718[root@k8s-master ~]$ kubectl get secrets --namespace=kube-systemNAME TYPE DATA AGEdefault-token-fwvl9 kubernetes.io/service-account-token 3 1h#kubectl delete secret `kubectl get secrets --namespace=kube-system |awk '&#123;print $1&#125;' | sed -e '1d'` --namespace=kube-system[root@k8s-master ~]$ kubectl delete secret default-token-fwvl9 --namespace=kube-systemsecret \"default-token-fwvl9\" deleted[root@k8s-master ~]$ kubectl get rc -n kube-systemNAME DESIRED CURRENT READY AGEkubernetes-dashboard-v1.5.0 1 1 0 6m#kubectl delete rc `kubectl get rc -n kube-system |awk '&#123;print $1&#125;' | sed -e '1d'` --namespace=kube-system [root@k8s-master ~]$ kubectl delete rc kubernetes-dashboard-v1.5.0 --namespace=kube-system replicationcontroller \"kubernetes-dashboard-v1.5.0\" deleted[root@k8s-master ~]$ kubectl create -f dashboard-controller.yaml replicationcontroller \"kubernetes-dashboard-v1.5.0\" created 访问：https://192.168.10.6:6443/ui如果提示Unauthorized的话，需要在/etc/kubernetes/apiserver中KUBE_API_ARGS参数后添加：1KUBE_API_ARGS=\"--basic-auth-file=/etc/kubernetes/basic_auth.csv\" basic_auth.csv格式为：1password,username,uid 重启服务：12systemctl daemon-reloadsystemctl restart kube-apiserver 具体node请用get pod命令查看:123kubectl get pod -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODEkubernetes-dashboard-v1.5.0-8hsb9 1/1 Running 0 13m 10.1.20.2 k8s-node1 创建服务 参考：http://running.iteye.com/blog/2322959本例以kubernetes源码中的guestbook为例讲解如何创建服务 先下载源码：12345#docker load -i /docker/works/images/Kubernetes集群搭建/others/redis-master.tar#docker load -i /docker/works/images/Kubernetes集群搭建/others/guestbook-redis-slave.tar#docker load -i /docker/works/images/Kubernetes集群搭建/others/guestbook-php-frontend.targit clone https://github.com/kubernetes/kubernetes.gitcd examples/guestbook/ 由于不能访问gcr.io，可以修改相应的yaml文件：修改legacy/redis-master-controller.yaml中的images，替换:1image: gcr.io/google_containers/redis:e2e # or just image: redis 为12image: kubeguide/redis-masterimagePullPolicy: IfNotPresent 修改legacy/redis-slave-controller.yaml中的images，替换：1image: gcr.io/google_samples/gb-redisslave:v1 为12image: kubeguide/guestbook-redis-slaveimagePullPolicy: IfNotPresent 并添加：123env:- name: GET_HOSTS_FROM value: env 修改frontend-service.yaml，添加：1234type: NodePort ports: - port: 80 nodePort: 30001 添加type: NodePort与nodePort: 30001参数，对外暴露30001 暴露对外端口方式： 在service中通过nodePort定义： 1234type: NodePort ports: - port: 80 nodePort: 30001 其中端口号必须在：30000-32767之间 通过rc定义： 123ports: - containerPort: 80 hostPort: 80 创建redis-master的service与rc:12kubectl create -f redis-slave-service.yamlkubectl create -f legacy/redis-master-controller.yaml 创建redis-slave的service与rc:12kubectl create -f redis-slave-service.yamlkubectl create -f legacy/redis-slave-controller.yaml 创建frontend的service与rc:12kubectl create -f frontend-service.yamlkubectl create -f legacy/frontend-controller.yaml 或者直接下载已经修改后的guestbook 查看pod：123456[root@k8s-master ~]$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODEfrontend-5l5kp 1/1 Running 0 2s 10.1.20.3 k8s-node1redis-master-2r7p7 1/1 Running 0 4m 10.1.30.2 k8s-node2redis-slave-n6nz6 1/1 Running 0 2m 10.1.20.5 k8s-node1redis-slave-rrl87 1/1 Running 0 2m 10.1.30.3 k8s-node2 异常问题：该demo是基于环境变量，所以必须先创建service，再创建rc，否则会出现STATUS为Running，但功能会报错。 发现有一台始终没有iptables规则：通过tail -n100 -f /var/log/message查看，发现有一台node，没有修改/etc/kubernetes/config中的apiserver的地址。 docker registry此章节为通过kubernetes方式部署。 12git clone https://github.com/kubernetes/kubernetes.gitcd kubernetes/cluster/addons/registry vim registry-pv.yaml1234567891011121314151617apiVersion: v1kind: PersistentVolumemetadata: name: kube-system-kube-registry-pv labels: kubernetes.io/cluster-service: \"true\"spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: /data #nfs: # path: /data/k8s # server: 192.168.12.171 persistentVolumeReclaimPolicy: Recycle vim registry-pvc.yaml12345678910111213apiVersion: v1kind: PersistentVolumeClaimmetadata: name: kube-registry-pvc namespace: kube-system labels: kubernetes.io/cluster-service: \"true\"spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 创建：1234567kubectl create -f registry-pv.yaml kubectl create -f registry-pvc.yaml``` 查看pv：```bashkubectl get pv 新建registry svc和rc:vim registry-rc.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: v1kind: ReplicationControllermetadata: name: kube-registry-v0 namespace: kube-system labels: k8s-app: kube-registry version: v0 kubernetes.io/cluster-service: \"true\"spec: replicas: 1 selector: k8s-app: kube-registry version: v0 template: metadata: labels: k8s-app: kube-registry version: v0 kubernetes.io/cluster-service: \"true\" spec: containers: - name: registry image: registry:2 resources: # keep request = limit to keep this container in guaranteed class limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi env: - name: REGISTRY_HTTP_ADDR value: :5000 - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY value: /var/lib/registry volumeMounts: - name: image-store mountPath: /var/lib/registry ports: - containerPort: 5000 name: registry protocol: TCP volumes: - name: image-store persistentVolumeClaim: claimName: kube-registry-pvc vim registry-svc.yaml123456789101112131415161718apiVersion: v1kind: Servicemetadata: name: kube-registry namespace: kube-system labels: k8s-app: kube-registry kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"KubeRegistry\"spec: selector: k8s-app: kube-registry type: NodePort ports: - name: registry port: 5000 nodePort: 30009 protocol: TCP 创建rc、svc:12kubectl create -f registry-rc.yaml kubectl create -f registry-svc.yaml 查看状态:1kubectl get svc --namespace=kube-system 每台修改docker的配置文件:注意：insecure-registry不能加上http://1234sed -i \"s;^ExecStart=/usr/bin/dockerd.*;ExecStart=/usr/bin/dockerd --bip=10.1.20.1/24 \\ --insecure-registry=192.168.10.8:30009 \\ --registry-mirror=http://3fecfd09.m.daocloud.io;\" \\ /usr/lib/systemd/system/docker.service 每台重启docker:123systemctl daemon-reloadsystemctl restart dockersystemctl status docker 测试：12docker tag registry:2 192.168.10.8:30009/registry:2docker push 192.168.10.8:30009/registry:2 参考 http://blog.csdn.net/air_penguin/article/details/51350910https://mritd.me/2016/09/07/Kubernetes-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/https://mritd.me/2016/09/11/kubernetes-%E5%8F%8C%E5%90%91-TLS-%E9%85%8D%E7%BD%AE/http://www.pangxie.space/docker/1055","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.gcalls.cn/tags/kubernetes/"}]},{"title":"hexo创建博客","slug":"hexo创建博客","date":"2017-01-02T08:06:28.000Z","updated":"2024-08-02T05:39:00.959Z","comments":true,"path":"/2017/01/hexo创建博客.html","link":"","permalink":"http://blog.gcalls.cn/2017/01/hexo创建博客.html","excerpt":"Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Heroku上，是搭建博客的首选框架。由于hexo相关的文章网上很多，我在这里就不再重复了。hexo的配置有些繁锁，这里主要介绍通过我提供的模板快速生成。","text":"Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Heroku上，是搭建博客的首选框架。由于hexo相关的文章网上很多，我在这里就不再重复了。hexo的配置有些繁锁，这里主要介绍通过我提供的模板快速生成。 nodejshexo需要安装nodejs环境，以Linux为例，其他操作系统安装类似： 123456curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.34.0/install.sh | bash. ~/.bashrc#显示有远端的版本nvm ls-remote#安装对应的版本nvm install 对应的版本 安装常用工具： 1234567891011npm install hexo-cli -gnpm install hexo-server -gnpm install hexo-deployer-git -gnpm install yarn -gnpm install http-server -gyarn global add servenpm config set registry https://registry.npmmirror.com --globalnpm config set disturl https://npmmirror.com/dist --globalyarn config set registry https://registry.npmmirror.com --globalyarn config set disturl https://npmmirror.com/dist --global 安装git123456789sudo apt-get install gitgit config --global user.name \"aa\"git config --global user.email aa@aa.comgit config --global core.autocrlf falsegit config --global core.safecrlf warngit config --global core.filemode falsegit config --global core.whitespace cr-at-eolgit config --global credential.helper storegit config http.postBuffer 524288000 配置博客下载hexo模板，并解压。打开终端并输入命令： 12cd 模板根目录yarn install 修改_config.yml文件，修改所有Please edit的内容为自己的内容。git仓库需要自己创建。 模板提供了3个theme：BlueLake、Anisina、next，可以修改_config.yml下的theme参数。 使用12345678#创建新的文章hexo n \"新文章title\"#debughexo s#生成部署文件hexo g#部署hexo d 可以直接运行模板根目录下的deploy.sh或者deploy.bat文件快速部署。","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://blog.gcalls.cn/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://blog.gcalls.cn/tags/Hexo/"}]}]}